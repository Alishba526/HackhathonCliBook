{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Physical AI &amp; Humanoid Robotics","text":""},{"location":"#master-the-future-of-ai-in-the-physical-world","title":"Master the Future of AI in the Physical World","text":"<p>Welcome to the comprehensive textbook on Physical AI &amp; Humanoid Robotics. This course bridges the gap between artificial intelligence and physical embodiment\u2014where digital brains meet robotic bodies.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>By completing this course, you will:</p> <p>\u2705 Understand Physical AI principles and embodied intelligence \u2705 Master ROS 2 - the robotic nervous system \u2705 Simulate robots with Gazebo physics engine \u2705 Create high-fidelity environments with Unity \u2705 Develop with NVIDIA Isaac - advanced perception \u2705 Build Vision-Language-Action systems - voice-to-action control \u2705 Deploy the capstone - autonomous humanoid with voice commands  </p>"},{"location":"#course-modules","title":"Course Modules","text":"<p>Module 1: The Robotic Nervous System (ROS 2) - ROS 2 middleware architecture - Nodes, Topics, and Services - URDF robot description - Real-time control systems</p> <p>Module 2: The Digital Twin (Gazebo &amp; Unity) - Physics simulation with Gazebo - High-fidelity rendering in Unity - Sensor simulation and testing - Environment building</p> <p>Module 3: The AI-Robot Brain (NVIDIA Isaac) - Photorealistic simulation - Synthetic data generation - Hardware-accelerated perception - Nav2 path planning</p> <p>Module 4: Vision-Language-Action (VLA) - Voice-to-text with Whisper - Vision understanding with CLIP - LLM planning with GPT-4 - Robot action execution</p>"},{"location":"#capstone-project","title":"Capstone Project","text":"<p>Build an autonomous humanoid that: 1. Listens to voice commands 2. Understands natural language 3. Plans action sequences 4. Executes tasks in simulation 5. Verifies completion</p> <p>Pipeline: <pre><code>\"Pick up the red ball\" \u2192\nWhisper transcription \u2192\nCLIP vision understanding \u2192\nGPT-4 planning \u2192\nROS 2 execution \u2192\nTask completion \u2713\n</code></pre></p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Strong Python fundamentals</li> <li>Mathematics: Linear algebra, calculus, probability</li> <li>Robotics: Basic kinematics and dynamics knowledge</li> </ul>"},{"location":"#hardware-requirements","title":"Hardware Requirements","text":"<p>Minimum: - GPU: NVIDIA RTX 4070 Ti (12GB VRAM) - CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9 - RAM: 32GB DDR5 - OS: Ubuntu 22.04 LTS</p> <p>Recommended: - GPU: NVIDIA RTX 4090 (24GB VRAM) - CPU: Intel Core i9 or AMD Ryzen 9 9950X - RAM: 64GB DDR5</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Start with Module 1 to understand ROS 2</li> <li>Progress through the modules in order</li> <li>Complete hands-on exercises in each module</li> <li>Build the capstone project at the end</li> </ol> <p>Created for hackathon | Physical AI &amp; Humanoid Robotics Textbook | 2025</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"capstone/","title":"Capstone Project: Autonomous Humanoid with Voice Commands","text":""},{"location":"capstone/#project-overview","title":"Project Overview","text":"<p>Build an autonomous humanoid robot that can: 1. Listen to voice commands 2. Understand natural language instructions 3. Plan a sequence of actions 4. Execute those actions in simulation 5. Verify success using computer vision</p>"},{"location":"capstone/#final-deliverable","title":"Final Deliverable","text":"<p>A simulated humanoid robot that performs complex tasks based on voice commands, demonstrating the complete Physical AI stack.</p>"},{"location":"capstone/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input (Whisper)             \u2502 (speech \u2192 text)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Language Understanding (GPT-4)    \u2502 (text \u2192 action plan)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Vision-based Verification (CLIP)  \u2502 (verify feasibility)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Motor Control (ROS 2)             \u2502 (action plan \u2192 motor commands)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Gazebo Simulation                 \u2502 (execute in physics)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Vision Feedback                   \u2502 (verify task completion)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"capstone/#project-phases","title":"Project Phases","text":""},{"location":"capstone/#phase-1-setup-and-simulation-week-8-9","title":"Phase 1: Setup and Simulation (Week 8-9)","text":""},{"location":"capstone/#11-create-the-simulated-humanoid","title":"1.1: Create the Simulated Humanoid","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/humanoid_sim.py\n\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_folder\nimport numpy as np\n\nclass HumanoidSimulator:\n    def __init__(self):\n        self.simulation_app = SimulationApp({\"headless\": False})\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Load humanoid robot from NVIDIA's library\n        assets_root_path = get_assets_root_folder()\n        humanoid_usd = assets_root_path + \"/Isaac/2023.1.1/Isaac3rdParty/NVIDIA/Humanoid_Pro/H1.usd\"\n\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path=\"/World/Humanoid\",\n                usd_path=humanoid_usd,\n                position=np.array([0, 0, 0.8])\n            )\n        )\n\n    def step(self):\n        self.world.step(render=True)\n        return self.world.is_playing()\n\n    def close(self):\n        self.simulation_app.close()\n\ndef main():\n    simulator = HumanoidSimulator()\n\n    # Run simulation\n    for i in range(1000):\n        if not simulator.step():\n            break\n\n    simulator.close()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#12-connect-to-ros-2","title":"1.2: Connect to ROS 2","text":"<pre><code># Create bridge between Isaac Sim and ROS 2\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass HumanoidBridge(Node):\n    def __init__(self, simulator):\n        super().__init__('humanoid_bridge')\n        self.simulator = simulator\n\n        # Subscribe to control commands\n        self.cmd_vel_sub = self.create_subscription(\n            Twist,\n            '/cmd_vel',\n            self.velocity_callback,\n            10\n        )\n\n        self.joint_cmd_sub = self.create_subscription(\n            Float64MultiArray,\n            '/joint_commands',\n            self.joint_callback,\n            10\n        )\n\n    def velocity_callback(self, msg):\n        # Convert Twist to humanoid motion\n        linear_x = msg.linear.x\n        angular_z = msg.angular.z\n\n        # Compute walking pattern\n        joint_targets = self.compute_gait(linear_x, angular_z)\n        self.apply_joint_targets(joint_targets)\n\n    def joint_callback(self, msg):\n        self.apply_joint_targets(msg.data)\n\n    def compute_gait(self, forward_speed, turning_speed):\n        \"\"\"Compute bipedal walking gait\"\"\"\n        # Simplified gait computation\n        t = self.simulator.world.current_time\n\n        hip_angles = forward_speed * np.sin(2 * np.pi * t)\n        knee_angles = forward_speed * (np.cos(2 * np.pi * t) - 1)\n\n        return np.array([hip_angles, knee_angles, 0, 0, hip_angles, knee_angles])\n\n    def apply_joint_targets(self, targets):\n        # Apply to simulated robot\n        pass\n</code></pre>"},{"location":"capstone/#phase-2-voice-to-action-pipeline-week-10","title":"Phase 2: Voice to Action Pipeline (Week 10)","text":""},{"location":"capstone/#21-voice-capture","title":"2.1: Voice Capture","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/voice_interface.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport sounddevice as sd\nimport scipy.io.wavfile as wavfile\nimport numpy as np\nimport threading\n\nclass VoiceInterfaceNode(Node):\n    def __init__(self):\n        super().__init__('voice_interface')\n\n        openai.api_key = \"your-api-key-here\"\n\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.listening = True\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.listen_loop)\n        self.listen_thread.daemon = True\n        self.listen_thread.start()\n\n    def listen_loop(self):\n        while self.listening:\n            try:\n                # Record audio\n                self.get_logger().info('Listening...')\n                audio = sd.rec(\n                    int(16000 * 5),  # 5 seconds\n                    samplerate=16000,\n                    channels=1,\n                    dtype=np.int16\n                )\n                sd.wait()\n\n                # Save audio\n                wavfile.write('command.wav', 16000, audio)\n\n                # Transcribe with Whisper\n                with open('command.wav', 'rb') as f:\n                    transcript = openai.Audio.transcribe(\"whisper-1\", f)\n\n                command = transcript['text']\n                if command.strip():\n                    self.get_logger().info(f'Heard: {command}')\n\n                    # Publish command\n                    msg = String()\n                    msg.data = command\n                    self.command_pub.publish(msg)\n\n            except Exception as e:\n                self.get_logger().error(f'Error: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceInterfaceNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#22-language-understanding","title":"2.2: Language Understanding","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/language_planner.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport json\n\nclass LanguagePlannerNode(Node):\n    def __init__(self):\n        super().__init__('language_planner')\n\n        openai.api_key = \"your-api-key-here\"\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice/command',\n            self.command_callback,\n            10\n        )\n\n        # Publish action plan\n        self.plan_pub = self.create_publisher(String, '/action_plan', 10)\n\n    def command_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'Processing: {command}')\n\n        # Generate action plan\n        plan = self.generate_plan(command)\n\n        # Publish plan\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan)\n        self.plan_pub.publish(plan_msg)\n\n    def generate_plan(self, command):\n        \"\"\"Use GPT-4 to generate robot action plan\"\"\"\n\n        system_prompt = \"\"\"You are a robot planning system. Convert natural language commands into \nstructured robot action sequences.\n\nAvailable Actions:\n- move_to(x, y, z): Move arm to position\n- grasp(): Close gripper\n- release(): Open gripper  \n- navigate_to(x, y): Move robot base\n- look_at(x, y, z): Turn camera toward point\n- wait(seconds): Pause\n\nYou MUST respond with valid JSON array of actions only, no other text.\nExample response: [{\"action\": \"navigate_to\", \"params\": {\"x\": 5, \"y\": 0}}, {\"action\": \"grasp\", \"params\": {}}]\n\"\"\"\n\n        user_prompt = f'Command: \"{command}\"'\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.3\n        )\n\n        response_text = response['choices'][0]['message']['content'].strip()\n\n        try:\n            # Extract JSON if there's extra text\n            import re\n            json_match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n            if json_match:\n                plan = json.loads(json_match.group())\n            else:\n                plan = json.loads(response_text)\n\n            return plan\n        except:\n            self.get_logger().error(f'Failed to parse plan: {response_text}')\n            return []\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LanguagePlannerNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#phase-3-execution-and-feedback-week-11","title":"Phase 3: Execution and Feedback (Week 11)","text":""},{"location":"capstone/#31-action-executor","title":"3.1: Action Executor","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/action_executor.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float64MultiArray\nfrom geometry_msgs.msg import Twist\nimport json\nimport time\n\nclass ActionExecutorNode(Node):\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Subscribe to action plans\n        self.plan_sub = self.create_subscription(\n            String,\n            '/action_plan',\n            self.plan_callback,\n            10\n        )\n\n        # Control publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.arm_cmd_pub = self.create_publisher(Float64MultiArray, '/joint_commands', 10)\n\n        # Feedback publisher\n        self.feedback_pub = self.create_publisher(String, '/execution_feedback', 10)\n\n    def plan_callback(self, msg):\n        plan = json.loads(msg.data)\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        for i, action in enumerate(plan):\n            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action[\"action\"]}')\n\n            action_type = action.get('action')\n            params = action.get('params', {})\n\n            try:\n                if action_type == 'move_to':\n                    self.move_to_position(params['x'], params['y'], params['z'])\n                elif action_type == 'grasp':\n                    self.grasp()\n                elif action_type == 'release':\n                    self.release()\n                elif action_type == 'navigate_to':\n                    self.navigate_to(params['x'], params['y'])\n                elif action_type == 'look_at':\n                    self.look_at(params['x'], params['y'], params['z'])\n                elif action_type == 'wait':\n                    time.sleep(params.get('seconds', 1))\n\n                # Send feedback\n                feedback = String()\n                feedback.data = f\"\u2713 Completed: {action_type}\"\n                self.feedback_pub.publish(feedback)\n\n            except Exception as e:\n                self.get_logger().error(f'Error executing {action_type}: {e}')\n\n    def move_to_position(self, x, y, z):\n        \"\"\"Move arm to target position\"\"\"\n        # Compute IK and send joint commands\n        joint_angles = self.compute_inverse_kinematics(x, y, z)\n\n        msg = Float64MultiArray()\n        msg.data = joint_angles\n        self.arm_cmd_pub.publish(msg)\n\n        time.sleep(1)  # Wait for arm to reach\n\n    def grasp(self):\n        \"\"\"Close gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [0.5]  # Close\n        self.arm_cmd_pub.publish(msg)\n        time.sleep(0.5)\n\n    def release(self):\n        \"\"\"Open gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [0.0]  # Open\n        self.arm_cmd_pub.publish(msg)\n        time.sleep(0.5)\n\n    def navigate_to(self, x, y):\n        \"\"\"Navigate robot base to position\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.3  # Forward\n        msg.angular.z = 0.1  # Turning\n        self.cmd_vel_pub.publish(msg)\n\n        time.sleep(3)\n\n    def look_at(self, x, y, z):\n        \"\"\"Turn camera toward point\"\"\"\n        pass\n\n    def compute_inverse_kinematics(self, x, y, z):\n        \"\"\"Compute joint angles for target position\"\"\"\n        # Simplified IK\n        import numpy as np\n        r = np.sqrt(x**2 + y**2)\n\n        angles = [\n            np.arctan2(y, x),\n            np.arctan2(z, r),\n            np.arctan2(z, r) * 0.5,\n            0.0\n        ]\n\n        return angles\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionExecutorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#32-vision-feedback","title":"3.2: Vision Feedback","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/vision_feedback.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nimport json\n\nclass VisionFeedbackNode(Node):\n    def __init__(self):\n        super().__init__('vision_feedback')\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.feedback_pub = self.create_publisher(String, '/vision_feedback', 10)\n\n        # Load CLIP for understanding objects\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Analyze image\n        analysis = self.analyze_image(cv_image)\n\n        # Publish feedback\n        feedback = String()\n        feedback.data = json.dumps(analysis)\n        self.feedback_pub.publish(feedback)\n\n    def analyze_image(self, image):\n        \"\"\"Analyze image for task completion\"\"\"\n\n        # Prepare inputs\n        image_input = self.processor(\n            images=image,\n            return_tensors=\"pt\"\n        )['pixel_values'].to(self.device)\n\n        # Check for common objects\n        objects_to_check = [\"red cube\", \"box\", \"table\", \"gripper\", \"hand\", \"object\"]\n\n        with torch.no_grad():\n            image_features = self.model.get_image_features(image_input)\n\n            detections = {}\n            for obj in objects_to_check:\n                text_input = self.processor(\n                    text=[f\"a {obj}\"],\n                    return_tensors=\"pt\",\n                    padding=True\n                ).to(self.device)\n\n                text_features = self.model.get_text_features(**text_input)\n\n                # Normalize\n                image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n                text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n\n                # Compute similarity\n                similarity = (image_features_norm @ text_features_norm.T)[0][0].item()\n\n                if similarity &gt; 0.3:\n                    detections[obj] = float(similarity)\n\n        return {\n            \"timestamp\": float(rclpy.clock.Clock().now().nanoseconds),\n            \"objects_detected\": detections\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionFeedbackNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#phase-4-integration-and-testing-week-12","title":"Phase 4: Integration and Testing (Week 12)","text":""},{"location":"capstone/#41-main-orchestrator","title":"4.1: Main Orchestrator","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/main_orchestrator.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport time\n\nclass OrchestratorNode(Node):\n    def __init__(self):\n        super().__init__('orchestrator')\n\n        self.state = \"idle\"\n        self.current_task = None\n\n        # Publisher for starting execution\n        self.execute_pub = self.create_publisher(String, '/action_plan', 10)\n\n        # Subscriber for feedback\n        self.feedback_sub = self.create_subscription(\n            String,\n            '/execution_feedback',\n            self.feedback_callback,\n            10\n        )\n\n        self.voice_sub = self.create_subscription(\n            String,\n            '/voice/command',\n            self.voice_callback,\n            10\n        )\n\n    def voice_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'\\n\ud83c\udfa4 Command: \"{command}\"\\n')\n        self.current_task = command\n        self.state = \"processing\"\n\n    def feedback_callback(self, msg):\n        feedback = msg.data\n        if \"\u2713\" in feedback:\n            self.get_logger().info(f'  {feedback}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OrchestratorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"capstone/#42-launch-file","title":"4.2: Launch File","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/launch/full_system.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Humanoid simulator\n        Node(\n            package='autonomous_humanoid',\n            executable='humanoid_sim',\n            name='humanoid_sim'\n        ),\n\n        # Voice interface\n        Node(\n            package='autonomous_humanoid',\n            executable='voice_interface',\n            name='voice_interface'\n        ),\n\n        # Language planner\n        Node(\n            package='autonomous_humanoid',\n            executable='language_planner',\n            name='language_planner'\n        ),\n\n        # Action executor\n        Node(\n            package='autonomous_humanoid',\n            executable='action_executor',\n            name='action_executor'\n        ),\n\n        # Vision feedback\n        Node(\n            package='autonomous_humanoid',\n            executable='vision_feedback',\n            name='vision_feedback'\n        ),\n\n        # Orchestrator\n        Node(\n            package='autonomous_humanoid',\n            executable='orchestrator',\n            name='orchestrator'\n        ),\n    ])\n</code></pre>"},{"location":"capstone/#running-the-capstone","title":"Running the Capstone","text":"<pre><code>cd ~/ros2_ws\ncolcon build --packages-select autonomous_humanoid\nsource install/setup.bash\n\n# Start the entire system\nros2 launch autonomous_humanoid full_system.launch.py\n</code></pre>"},{"location":"capstone/#example-commands-to-try","title":"Example Commands to Try","text":"<pre><code>\"Move your arm to the right\"\n\"Pick up the red cube\"\n\"Walk to the table\"\n\"Look at the camera\"\n\"Open your hand\"\n\"Pick up the blue sphere and place it on the table\"\n</code></pre>"},{"location":"capstone/#evaluation-criteria","title":"Evaluation Criteria","text":"<ol> <li>Voice Recognition (20 points)</li> <li>Accurately captures voice commands</li> <li> <p>Robust to background noise</p> </li> <li> <p>Language Understanding (20 points)</p> </li> <li>Correctly parses natural language</li> <li> <p>Handles complex multi-step tasks</p> </li> <li> <p>Execution (30 points)</p> </li> <li>Actions execute smoothly in simulation</li> <li>Proper sequencing of tasks</li> <li> <p>Error handling</p> </li> <li> <p>Vision Integration (20 points)</p> </li> <li>Provides real-time feedback</li> <li> <p>Verifies task completion</p> </li> <li> <p>Code Quality (10 points)</p> </li> <li>Well-documented</li> <li>Modular design</li> <li>Error handling</li> </ol>"},{"location":"capstone/#bonus-features","title":"Bonus Features","text":"<ul> <li>Advanced Gait: Implement realistic bipedal walking</li> <li>Obstacle Avoidance: Navigate around objects</li> <li>Multi-Robot Coordination: Control multiple robots</li> <li>Sim-to-Real Transfer: Deploy to real robot</li> <li>Complex Tasks: Multi-stage tasks with dependencies</li> </ul> <p>Congratulations! You've completed the Physical AI &amp; Humanoid Robotics textbook. \ud83d\ude80</p> <p>Next step: Deploy your project to GitHub and submit to the hackathon!</p>"},{"location":"module-1-robotic-nervous-system/","title":"Module 1: The Robotic Nervous System","text":""},{"location":"module-1-robotic-nervous-system/#ros-2-fundamentals","title":"ROS 2 Fundamentals","text":"<p>Welcome to Module 1: The Robotic Nervous System. In this module, you'll learn ROS 2 (Robot Operating System 2), the middleware that enables all robot components to communicate and work together.</p>"},{"location":"module-1-robotic-nervous-system/#what-is-ros-2","title":"What is ROS 2?","text":"<p>ROS 2 is the \"nervous system\" of your humanoid robot. Just like your biological nervous system coordinates your brain with your body, ROS 2 coordinates: - AI algorithms (the brain) - Sensor inputs (eyes, ears, touch) - Motor controllers (muscles) - Real-time communication</p>"},{"location":"module-1-robotic-nervous-system/#module-topics","title":"Module Topics","text":"<p>This module covers:</p> <ol> <li>ROS 2 Architecture - Nodes, Topics, Services, and Messages</li> <li>Your First ROS 2 Node - Publish velocity commands and read sensor data</li> <li>URDF Robot Description - XML format for describing humanoid structure</li> </ol>"},{"location":"module-1-robotic-nervous-system/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this module, you will:</p> <p>\u2705 Understand the ROS 2 distributed computing model \u2705 Create Publisher/Subscriber nodes \u2705 Implement Services for request-response patterns \u2705 Write a URDF file describing a humanoid robot \u2705 Simulate robot movement with ROS 2 commands  </p>"},{"location":"module-1-robotic-nervous-system/#key-concepts","title":"Key Concepts","text":"<p>Nodes: Independent processes that perform specific tasks Topics: Data streams where nodes publish and subscribe Services: Request-response communication patterns Messages: Data structures that nodes exchange URDF: XML format for robot descriptions</p>"},{"location":"module-1-robotic-nervous-system/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ programming experience</li> <li>Basic understanding of object-oriented programming</li> <li>Linux/Ubuntu familiarity</li> </ul>"},{"location":"module-1-robotic-nervous-system/#next-steps","title":"Next Steps","text":"<ol> <li>Start with ROS 2 Fundamentals</li> <li>Progress to Your First ROS 2 Node</li> <li>Learn URDF Robot Description</li> <li>Then move to Module 2: The Digital Twin</li> </ol> <p>Module 1 of 4 | The Robotic Nervous System | ROS 2</p>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/","title":"Chapter 1: ROS 2 Architecture &amp; Core Concepts","text":""},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#what-is-ros-2","title":"What is ROS 2?","text":"<p>ROS 2 (Robot Operating System 2) is the middleware that serves as the \"nervous system\" of your humanoid robot. It manages communication between all robot components: sensors, processors, and actuators.</p>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#the-nervous-system-analogy","title":"The Nervous System Analogy","text":"<p>Just like your biological nervous system coordinates your brain with your body: - Brain = AI algorithms running on edge computers - Spinal cord = ROS 2 (message passing, coordination) - Sensors = Cameras, LiDAR, IMUs, depth cameras - Muscles = Motors, actuators, grippers</p> <p>Without ROS 2, these components can't communicate. With ROS 2, they work as one integrated system.</p>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#ros-2-core-concepts","title":"ROS 2 Core Concepts","text":""},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#1-nodes","title":"1. Nodes","text":"<p>A node is an independent process that performs a specific task.</p> <p>Examples in your humanoid robot: - Vision processing node (detects objects) - Motor control node (moves joints) - Navigation node (plans paths) - AI inference node (runs LLM)</p> <p>Key principle: Each node is independent. If one crashes, others keep running.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\n\nclass MyRobotNode(Node):\n    def __init__(self):\n        super().__init__('my_robot_node')\n        self.get_logger().info('Robot node started!')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MyRobotNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#2-topics","title":"2. Topics","text":"<p>Topics are data streams. One node publishes data, others subscribe.</p> <p>Publisher example (motor sends joint angles): <pre><code>from geometry_msgs.msg import Twist\n\nself.publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n\nmsg = Twist()\nmsg.linear.x = 0.5  # Move forward\nself.publisher.publish(msg)\n</code></pre></p> <p>Subscriber example (sensor reads data): <pre><code>from sensor_msgs.msg import Image\n\nself.subscription = self.create_subscription(\n    Image,\n    'camera/image_raw',\n    self.image_callback,\n    10\n)\n\ndef image_callback(self, msg):\n    # Process camera image\n    pass\n</code></pre></p>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#3-services","title":"3. Services","text":"<p>Services are request-response patterns. One node asks, another replies.</p> <p>Service definition: <pre><code># GetRobotState.srv\n---\nbool is_moving\nfloat32 battery_level\n</code></pre></p> <p>Service server: <pre><code>srv = self.create_service(GetRobotState, 'get_state', self.handle_get_state)\n\ndef handle_get_state(self, request, response):\n    response.is_moving = True\n    response.battery_level = 0.85\n    return response\n</code></pre></p> <p>Service client: <pre><code>client = self.create_client(GetRobotState, 'get_state')\nfuture = client.call_async(GetRobotState.Request())\nresponse = future.result()\nprint(f\"Battery: {response.battery_level}\")\n</code></pre></p>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#ros-2-executor-pattern","title":"ROS 2 Executor Pattern","text":"<p>The <code>rclpy.spin()</code> function creates an event loop that: 1. Listens for messages on subscribed topics 2. Calls callbacks when data arrives 3. Processes service requests 4. Allows your robot to react in real-time</p> <pre><code>def main():\n    rclpy.init()\n    node = MyRobotNode()\n\n    # This loop runs forever, processing callbacks\n    rclpy.spin(node)\n\n    rclpy.shutdown()\n</code></pre>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<p>ROS 2 allows tuning message delivery guarantees:</p> <pre><code>from rclpy.qos import QoSProfile, ReliabilityPolicy\n\n# Reliable delivery (for critical commands)\nqos = QoSProfile(reliability=ReliabilityPolicy.RELIABLE)\nself.create_subscription(Twist, 'cmd_vel', callback, qos)\n\n# Best-effort (for sensor data that's always updating)\nqos = QoSProfile(reliability=ReliabilityPolicy.BEST_EFFORT)\nself.create_subscription(Image, 'camera/image', callback, qos)\n</code></pre>"},{"location":"module-1-robotic-nervous-system/1-ros2-fundamentals/#summary","title":"Summary","text":"<p>ROS 2 gives your humanoid robot: - \u2705 Distributed computing - Many processes working together - \u2705 Real-time communication - Sensors \u2192 decisions \u2192 actions in milliseconds - \u2705 Fault isolation - One failed component doesn't crash everything - \u2705 Scalability - Add new nodes without modifying existing ones - \u2705 Language agnostic - Mix Python, C++, Rust nodes seamlessly</p> <p>In the next chapters, you'll build actual ROS 2 applications and create URDF descriptions for humanoid robots.</p>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/","title":"Chapter 2: Your First ROS 2 Node","text":""},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#building-your-first-node","title":"Building Your First Node","text":"<p>Let's create a simple robot controller that publishes velocity commands to move a humanoid robot forward.</p>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#setup","title":"Setup","text":"<pre><code># Install ROS 2 (if not already installed)\nsudo apt install ros-humble-desktop\n\n# Create a workspace\nmkdir -p ~/robot_ws/src\ncd ~/robot_ws\n\n# Create a package\nros2 pkg create --build-type ament_python robot_controller\ncd robot_controller\n</code></pre>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#create-your-first-node","title":"Create Your First Node","text":"<p>Create <code>robot_controller/robot_controller/motor_controller.py</code>:</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass MotorController(Node):\n    \"\"\"Controls the humanoid robot's movement\"\"\"\n\n    def __init__(self):\n        super().__init__('motor_controller')\n\n        # Create a publisher for velocity commands\n        self.publisher = self.create_publisher(\n            Twist,\n            'cmd_vel',\n            10\n        )\n\n        self.get_logger().info('Motor Controller initialized')\n        self.get_logger().info('Publishing to /cmd_vel topic')\n\n        # Timer to send commands every 0.5 seconds\n        self.timer = self.create_timer(0.5, self.publish_velocity)\n        self.counter = 0\n\n    def publish_velocity(self):\n        \"\"\"Send movement commands to the robot\"\"\"\n\n        # Create a Twist message (standard ROS 2 velocity command)\n        msg = Twist()\n\n        # Linear velocity (move forward 0.5 m/s)\n        msg.linear.x = 0.5\n        msg.linear.y = 0.0\n        msg.linear.z = 0.0\n\n        # Angular velocity (rotate slowly)\n        msg.angular.x = 0.0\n        msg.angular.y = 0.0\n        msg.angular.z = 0.1  # Rotate 0.1 rad/s\n\n        # Publish the message\n        self.publisher.publish(msg)\n\n        self.counter += 1\n        self.get_logger().info(\n            f'Published velocity command #{self.counter}: '\n            f'forward={msg.linear.x} m/s, rotate={msg.angular.z} rad/s'\n        )\n\ndef main(args=None):\n    # Initialize ROS 2\n    rclpy.init(args=args)\n\n    # Create the node\n    node = MotorController()\n\n    # Keep the node running\n    rclpy.spin(node)\n\n    # Shutdown when done\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#update-setuppy","title":"Update setup.py","text":"<p>Edit <code>setup.py</code> to include your node:</p> <pre><code>entry_points={\n    'console_scripts': [\n        'motor_controller = robot_controller.motor_controller:main',\n    ],\n},\n</code></pre>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#build-and-run","title":"Build and Run","text":"<pre><code># Go to workspace root\ncd ~/robot_ws\n\n# Build the package\ncolcon build\n\n# Source the setup script\nsource install/setup.bash\n\n# Run your node\nros2 run robot_controller motor_controller\n</code></pre> <p>Output: <pre><code>[INFO] Motor Controller initialized\n[INFO] Publishing to /cmd_vel topic\n[INFO] Published velocity command #1: forward=0.5 m/s, rotate=0.1 rad/s\n[INFO] Published velocity command #2: forward=0.5 m/s, rotate=0.1 rad/s\n</code></pre></p>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#monitoring-topics-with-ros2-cli","title":"Monitoring Topics with ros2 CLI","text":"<p>In another terminal, watch the published messages:</p> <pre><code># See all topics\nros2 topic list\n\n# See message frequency and data\nros2 topic hz /cmd_vel\nros2 topic echo /cmd_vel\n</code></pre>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#advanced-subscribing-to-sensor-data","title":"Advanced: Subscribing to Sensor Data","text":"<p>Create a sensor subscriber node:</p> <pre><code>from sensor_msgs.msg import Image, Imu\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\n\nclass SensorMonitor(Node):\n    def __init__(self):\n        super().__init__('sensor_monitor')\n\n        # Subscribe to camera images\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            QoSProfile(reliability=ReliabilityPolicy.BEST_EFFORT)\n        )\n\n        # Subscribe to IMU data (Inertial Measurement Unit)\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n    def image_callback(self, msg):\n        self.get_logger().info(\n            f'Received image: {msg.width}x{msg.height}'\n        )\n\n    def imu_callback(self, msg):\n        self.get_logger().info(\n            f'Accel: x={msg.linear_acceleration.x:.2f} m/s\u00b2'\n        )\n</code></pre>"},{"location":"module-1-robotic-nervous-system/2-first-ros2-node/#summary","title":"Summary","text":"<p>You now understand: - \u2705 How to create a ROS 2 Node - \u2705 How to publish messages to topics - \u2705 How to subscribe to topics - \u2705 The ROS 2 execution model with <code>rclpy.spin()</code> - \u2705 Message types (Twist for velocities, Image for cameras, Imu for sensors)</p> <p>Next: Learn URDF to describe humanoid robot structure!</p>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/","title":"Chapter 3: URDF - Describing Your Humanoid Robot","text":""},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#what-is-urdf","title":"What is URDF?","text":"<p>URDF (Unified Robot Description Format) is an XML standard that describes: - Robot structure (links and joints) - Physical properties (mass, size, materials) - Sensor locations (cameras, LiDAR, IMU) - Actuator constraints (joint limits, friction)</p> <p>Think of URDF as a blueprint that tells Gazebo and ROS 2 how your robot is built.</p>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#urdf-structure","title":"URDF Structure","text":""},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#basic-components","title":"Basic Components","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot name=\"humanoid_v1\"&gt;\n\n  &lt;!-- Links: Physical bodies --&gt;\n  &lt;link name=\"base_link\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"10.0\"/&gt;\n      &lt;inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.1\" iyz=\"0.0\" izz=\"0.1\"/&gt;\n    &lt;/inertial&gt;\n\n    &lt;!-- Visual appearance --&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.3 0.8\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"white\"/&gt;\n    &lt;/visual&gt;\n\n    &lt;!-- Collision geometry --&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.3 0.8\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Joints: Connections between links --&gt;\n  &lt;joint name=\"torso_to_left_shoulder\" type=\"revolute\"&gt;\n    &lt;parent link=\"base_link\"/&gt;\n    &lt;child link=\"left_shoulder\"/&gt;\n    &lt;origin xyz=\"0.15 0.2 0.0\" rpy=\"0 0 0\"/&gt;\n\n    &lt;!-- Range of motion --&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/&gt;\n\n    &lt;!-- Friction properties --&gt;\n    &lt;dynamics damping=\"0.1\" friction=\"0.0\"/&gt;\n  &lt;/joint&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#a-complete-humanoid-example","title":"A Complete Humanoid Example","text":"<p>Here's a simplified humanoid with torso, arms, and head:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot name=\"humanoid_simple\"&gt;\n\n  &lt;!-- Torso (main body) --&gt;\n  &lt;link name=\"torso\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"15.0\"/&gt;\n      &lt;inertia ixx=\"0.5\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.5\" iyz=\"0.0\" izz=\"0.3\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.4 1.2\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.4 1.2\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Head --&gt;\n  &lt;link name=\"head\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.02\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.15\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"skin\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.15\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Neck joint connecting head to torso --&gt;\n  &lt;joint name=\"neck_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"head\"/&gt;\n    &lt;origin xyz=\"0 0 0.7\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 0 1\"/&gt;  &lt;!-- Rotate around Z axis --&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"5\" velocity=\"2.0\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Left Arm --&gt;\n  &lt;link name=\"left_shoulder\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.01\" iyz=\"0.0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;link name=\"left_upper_arm\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.005\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Shoulder joint --&gt;\n  &lt;joint name=\"left_shoulder_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"left_shoulder\"/&gt;\n    &lt;origin xyz=\"0.2 0.25 0.4\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"1 0 0\"/&gt;  &lt;!-- Rotate around X axis --&gt;\n    &lt;limit lower=\"-3.14\" upper=\"3.14\" effort=\"20\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.1\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Upper arm joint --&gt;\n  &lt;joint name=\"left_shoulder_to_upper_arm\" type=\"revolute\"&gt;\n    &lt;parent link=\"left_shoulder\"/&gt;\n    &lt;child link=\"left_upper_arm\"/&gt;\n    &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;  &lt;!-- Rotate around Y axis --&gt;\n    &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"15\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Right Arm (mirror of left) --&gt;\n  &lt;link name=\"right_shoulder\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.01\" iyz=\"0.0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;link name=\"right_upper_arm\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.005\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;joint name=\"right_shoulder_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"right_shoulder\"/&gt;\n    &lt;origin xyz=\"-0.2 0.25 0.4\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"1 0 0\"/&gt;\n    &lt;limit lower=\"-3.14\" upper=\"3.14\" effort=\"20\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.1\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;joint name=\"right_shoulder_to_upper_arm\" type=\"revolute\"&gt;\n    &lt;parent link=\"right_shoulder\"/&gt;\n    &lt;child link=\"right_upper_arm\"/&gt;\n    &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;\n    &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"15\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Materials definition --&gt;\n  &lt;material name=\"blue\"&gt;\n    &lt;color rgba=\"0.0 0.5 1.0 1.0\"/&gt;\n  &lt;/material&gt;\n\n  &lt;material name=\"skin\"&gt;\n    &lt;color rgba=\"0.9 0.8 0.7 1.0\"/&gt;\n  &lt;/material&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#understanding-urdf-concepts","title":"Understanding URDF Concepts","text":""},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#links","title":"Links","text":"<pre><code>&lt;link name=\"left_hand\"&gt;\n  &lt;!-- Inertial: mass and moment of inertia --&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.5\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n\n  &lt;!-- Visual: how it looks in simulation --&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;mesh filename=\"package://robot_description/meshes/hand.stl\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n\n  &lt;!-- Collision: how it interacts physically --&gt;\n  &lt;collision&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.1 0.05 0.2\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/collision&gt;\n&lt;/link&gt;\n</code></pre>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#joint-types","title":"Joint Types","text":"<pre><code>&lt;!-- Revolute (hinge joint) - rotates around one axis --&gt;\n&lt;joint name=\"elbow\" type=\"revolute\"&gt;\n  &lt;axis xyz=\"0 1 0\"/&gt;\n  &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"10\" velocity=\"1.0\"/&gt;\n&lt;/joint&gt;\n\n&lt;!-- Prismatic (sliding joint) - moves along one axis --&gt;\n&lt;joint name=\"gripper\" type=\"prismatic\"&gt;\n  &lt;axis xyz=\"1 0 0\"/&gt;\n  &lt;limit lower=\"0\" upper=\"0.1\" effort=\"50\" velocity=\"0.5\"/&gt;\n&lt;/joint&gt;\n\n&lt;!-- Fixed (no movement) --&gt;\n&lt;joint name=\"camera_mount\" type=\"fixed\"&gt;\n  &lt;parent link=\"head\"/&gt;\n  &lt;child link=\"camera\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#using-urdf-in-ros-2","title":"Using URDF in ROS 2","text":""},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#load-urdf-in-a-launch-file","title":"Load URDF in a Launch File","text":"<pre><code># launch/display.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    urdf_file = get_package_share_directory('robot_description')\n    urdf_file += '/urdf/humanoid.urdf'\n\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{'robot_description': robot_desc}]\n        ),\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            arguments=['-d', get_package_share_directory('robot_description') + '/rviz/config.rviz']\n        )\n    ])\n</code></pre>"},{"location":"module-1-robotic-nervous-system/3-urdf-robot-description/#summary","title":"Summary","text":"<p>URDF enables: - \u2705 Simulation - Test robots in Gazebo before building - \u2705 Visualization - See robot structure in RViz - \u2705 Physics - Realistic mass, inertia, collisions - \u2705 Joint Control - Define what can move - \u2705 Sensor Mounting - Place cameras, LiDAR, IMU on robot</p> <p>Next: Simulate your humanoid in Gazebo!</p>"},{"location":"module-2-digital-twin/","title":"Module 2: The Digital Twin","text":""},{"location":"module-2-digital-twin/#physics-simulation-high-fidelity-rendering","title":"Physics Simulation &amp; High-Fidelity Rendering","text":"<p>Welcome to Module 2: The Digital Twin. In this module, you'll learn to simulate robots in virtual environments before deploying to real hardware.</p>"},{"location":"module-2-digital-twin/#what-is-a-digital-twin","title":"What is a Digital Twin?","text":"<p>A digital twin is a virtual replica of your robot that: - Accurately simulates physics (gravity, friction, collisions) - Reproduces sensor data (cameras, lidar, IMU) - Allows safe testing of algorithms - Eliminates expensive hardware failures</p>"},{"location":"module-2-digital-twin/#why-two-simulators","title":"Why Two Simulators?","text":"<p>Gazebo - Physics accuracy - Precise physics engine (bullet, ODE) - Lightweight and fast - Perfect for algorithm development - ROS 2 native integration</p> <p>Unity - Visual realism - Photorealistic rendering - Human-robot interaction scenarios - Semantic scene understanding - Real-time performance</p>"},{"location":"module-2-digital-twin/#module-topics","title":"Module Topics","text":"<p>This module covers:</p> <ol> <li>Gazebo Simulation - Physics engine, world setup, sensor simulation</li> <li>Unity Simulation - 3D modeling, humanoid in realistic environments</li> </ol>"},{"location":"module-2-digital-twin/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this module, you will:</p> <p>\u2705 Create Gazebo simulation worlds \u2705 Simulate cameras, lidar, and IMUs \u2705 Model humanoid robots with URDF \u2705 Connect ROS 2 to Gazebo \u2705 Build humanoid in Unity with physics \u2705 Integrate ROS 2 with Unity \u2705 Test algorithms in simulated environments  </p>"},{"location":"module-2-digital-twin/#key-technologies","title":"Key Technologies","text":"<ul> <li>Gazebo Harmonic - Physics simulation</li> <li>ODE/Bullet - Physics engines</li> <li>SDF Format - Simulation description</li> <li>Unity 2022+ LTS - 3D graphics</li> <li>ROS-TCP-Connector - ROS integration with Unity</li> </ul>"},{"location":"module-2-digital-twin/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Module 1 (ROS 2 fundamentals)</li> <li>Linux with ROS 2 installed</li> <li>Basic 3D graphics understanding</li> </ul>"},{"location":"module-2-digital-twin/#hardware-recommendations","title":"Hardware Recommendations","text":"<ul> <li>GPU: RTX 4070 Ti or better (for real-time rendering)</li> <li>RAM: 32GB minimum</li> <li>Storage: 50GB free space</li> </ul>"},{"location":"module-2-digital-twin/#next-steps","title":"Next Steps","text":"<ol> <li>Start with Gazebo Simulation</li> <li>Progress to Unity Simulation</li> <li>Then move to Module 3: The AI-Robot Brain</li> </ol> <p>Module 2 of 4 | The Digital Twin | Gazebo &amp; Unity</p>"},{"location":"module-2-digital-twin/1-gazebo-simulation/","title":"Chapter 4: Physics Simulation with Gazebo","text":""},{"location":"module-2-digital-twin/1-gazebo-simulation/#what-is-gazebo","title":"What is Gazebo?","text":"<p>Gazebo is a powerful physics simulator that: - Simulates gravity, friction, and collisions - Renders 3D environments in real-time - Integrates with ROS 2 for robot control - Lets you test algorithms before using real robots</p> <p>Think of Gazebo as a virtual test lab where you can crash test your code without breaking expensive hardware.</p>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#gazebo-fundamentals","title":"Gazebo Fundamentals","text":""},{"location":"module-2-digital-twin/1-gazebo-simulation/#starting-gazebo","title":"Starting Gazebo","text":"<pre><code># Install Gazebo Harmonic\nsudo apt-get install ignition-gazebo\n\n# Launch Gazebo\ngazebo\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#creating-a-gazebo-world","title":"Creating a Gazebo World","text":"<p>Gazebo uses SDF (Simulation Description Format) files. Here's a simple world:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;sdf version=\"1.10\"&gt;\n  &lt;world name=\"robot_world\"&gt;\n\n    &lt;!-- Physics engine --&gt;\n    &lt;physics name=\"default_physics\" type=\"bullet\"&gt;\n      &lt;max_step_size&gt;0.001&lt;/max_step_size&gt;\n      &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt;\n    &lt;/physics&gt;\n\n    &lt;!-- Ground plane --&gt;\n    &lt;model name=\"ground_plane\"&gt;\n      &lt;static&gt;true&lt;/static&gt;\n      &lt;link name=\"link\"&gt;\n        &lt;collision name=\"collision\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n        &lt;/collision&gt;\n        &lt;visual name=\"visual\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n          &lt;material&gt;\n            &lt;ambient&gt;0.8 0.8 0.8 1&lt;/ambient&gt;\n            &lt;diffuse&gt;0.8 0.8 0.8 1&lt;/diffuse&gt;\n          &lt;/material&gt;\n        &lt;/visual&gt;\n      &lt;/link&gt;\n    &lt;/model&gt;\n\n    &lt;!-- Lighting --&gt;\n    &lt;light type=\"directional\" name=\"sun\"&gt;\n      &lt;cast_shadows&gt;true&lt;/cast_shadows&gt;\n      &lt;pose&gt;0 0 10 0 0 0&lt;/pose&gt;\n      &lt;diffuse&gt;1 1 1 1&lt;/diffuse&gt;\n      &lt;specular&gt;0.5 0.5 0.5 1&lt;/specular&gt;\n      &lt;direction&gt;-1 -1 -1&lt;/direction&gt;\n    &lt;/light&gt;\n\n    &lt;!-- Your robot will be inserted here --&gt;\n    &lt;include&gt;\n      &lt;uri&gt;model://humanoid&lt;/uri&gt;\n      &lt;pose&gt;0 0 1 0 0 0&lt;/pose&gt;\n    &lt;/include&gt;\n\n  &lt;/world&gt;\n&lt;/sdf&gt;\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#simulating-your-humanoid-robot","title":"Simulating Your Humanoid Robot","text":""},{"location":"module-2-digital-twin/1-gazebo-simulation/#launch-with-ros-2","title":"Launch with ROS 2","text":"<p>Create <code>launch/gazebo_humanoid.launch.py</code>:</p> <pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import ExecuteProcess\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Get paths\n    robot_desc_dir = get_package_share_directory('robot_description')\n    world_file = robot_desc_dir + '/worlds/robot_world.sdf'\n    urdf_file = robot_desc_dir + '/urdf/humanoid.urdf'\n\n    # Read URDF\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        # Launch Gazebo\n        ExecuteProcess(\n            cmd=['gazebo', '--verbose', world_file],\n            output='screen'\n        ),\n\n        # Publish robot description\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{'robot_description': robot_desc}]\n        ),\n\n        # Joint state broadcaster\n        Node(\n            package='controller_manager',\n            executable='spawner',\n            arguments=['joint_state_broadcaster'],\n        ),\n\n        # Diff drive controller for movement\n        Node(\n            package='controller_manager',\n            executable='spawner',\n            arguments=['diff_drive_controller'],\n        ),\n\n        # Your custom controller node\n        Node(\n            package='robot_controller',\n            executable='motor_controller',\n            output='screen'\n        ),\n    ])\n</code></pre> <p>Run it: <pre><code>ros2 launch robot_description gazebo_humanoid.launch.py\n</code></pre></p>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#sensor-simulation","title":"Sensor Simulation","text":""},{"location":"module-2-digital-twin/1-gazebo-simulation/#simulating-a-camera","title":"Simulating a Camera","text":"<p>Add to your URDF:</p> <pre><code>&lt;!-- Camera link --&gt;\n&lt;link name=\"camera\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.1\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.05 0.05 0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n  &lt;collision&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.05 0.05 0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/collision&gt;\n&lt;/link&gt;\n\n&lt;!-- Mount camera on head --&gt;\n&lt;joint name=\"head_to_camera\" type=\"fixed\"&gt;\n  &lt;parent link=\"head\"/&gt;\n  &lt;child link=\"camera\"/&gt;\n  &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#gazebo-camera-plugin","title":"Gazebo Camera Plugin","text":"<p>Add to SDF world file:</p> <pre><code>&lt;plugin\n    filename=\"gz-sim-camera-system\"\n    name=\"gz::sim::systems::Camera\"&gt;\n&lt;/plugin&gt;\n\n&lt;sensor name=\"camera\" type=\"camera\"&gt;\n  &lt;camera&gt;\n    &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt;\n    &lt;image&gt;\n      &lt;width&gt;640&lt;/width&gt;\n      &lt;height&gt;480&lt;/height&gt;\n    &lt;/image&gt;\n    &lt;clip&gt;\n      &lt;near&gt;0.1&lt;/near&gt;\n      &lt;far&gt;100&lt;/far&gt;\n    &lt;/clip&gt;\n  &lt;/camera&gt;\n  &lt;always_on&gt;1&lt;/always_on&gt;\n  &lt;update_rate&gt;30&lt;/update_rate&gt;\n  &lt;visualize&gt;true&lt;/visualize&gt;\n  &lt;topic&gt;camera&lt;/topic&gt;\n&lt;/sensor&gt;\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#subscribe-to-camera-in-ros-2","title":"Subscribe to Camera in ROS 2","text":"<pre><code>from sensor_msgs.msg import Image\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Process image (detect objects, etc.)\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        self.get_logger().info(f'Processed frame: {gray.shape}')\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#simulating-lidar","title":"Simulating LiDAR","text":"<pre><code>&lt;!-- LiDAR link --&gt;\n&lt;link name=\"lidar\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.2\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;cylinder length=\"0.08\" radius=\"0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n&lt;/link&gt;\n\n&lt;joint name=\"torso_to_lidar\" type=\"fixed\"&gt;\n  &lt;parent link=\"torso\"/&gt;\n  &lt;child link=\"lidar\"/&gt;\n  &lt;origin xyz=\"0 0 0.6\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre> <p>LiDAR plugin in Gazebo:</p> <pre><code>&lt;sensor name=\"gpu_lidar\" type=\"gpu_lidar\"&gt;\n  &lt;pose&gt;0 0 0.6 0 0 0&lt;/pose&gt;\n  &lt;topic&gt;lidar&lt;/topic&gt;\n  &lt;update_rate&gt;10&lt;/update_rate&gt;\n  &lt;ray&gt;\n    &lt;scan&gt;\n      &lt;horizontal&gt;\n        &lt;samples&gt;360&lt;/samples&gt;\n        &lt;resolution&gt;1&lt;/resolution&gt;\n        &lt;min_angle&gt;0&lt;/min_angle&gt;\n        &lt;max_angle&gt;6.28&lt;/max_angle&gt;\n      &lt;/horizontal&gt;\n      &lt;vertical&gt;\n        &lt;samples&gt;128&lt;/samples&gt;\n        &lt;resolution&gt;1&lt;/resolution&gt;\n        &lt;min_angle&gt;-1.047&lt;/min_angle&gt;\n        &lt;max_angle&gt;1.047&lt;/max_angle&gt;\n      &lt;/vertical&gt;\n    &lt;/scan&gt;\n    &lt;range&gt;\n      &lt;min&gt;0.1&lt;/min&gt;\n      &lt;max&gt;30.0&lt;/max&gt;\n      &lt;resolution&gt;0.02&lt;/resolution&gt;\n    &lt;/range&gt;\n  &lt;/ray&gt;\n&lt;/sensor&gt;\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#simulating-imu-inertial-measurement-unit","title":"Simulating IMU (Inertial Measurement Unit)","text":"<pre><code>&lt;!-- IMU link --&gt;\n&lt;link name=\"imu\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.05\"/&gt;\n    &lt;inertia ixx=\"0.0001\" ixy=\"0\" ixz=\"0\" iyy=\"0.0001\" iyz=\"0\" izz=\"0.0001\"/&gt;\n  &lt;/inertial&gt;\n&lt;/link&gt;\n\n&lt;joint name=\"torso_to_imu\" type=\"fixed\"&gt;\n  &lt;parent link=\"torso\"/&gt;\n  &lt;child link=\"imu\"/&gt;\n  &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre> <p>IMU plugin:</p> <pre><code>&lt;sensor name=\"imu_sensor\" type=\"imu\"&gt;\n  &lt;always_on&gt;true&lt;/always_on&gt;\n  &lt;update_rate&gt;100&lt;/update_rate&gt;\n  &lt;visualize&gt;false&lt;/visualize&gt;\n  &lt;topic&gt;imu&lt;/topic&gt;\n&lt;/sensor&gt;\n</code></pre> <p>Subscribe in ROS 2:</p> <pre><code>from sensor_msgs.msg import Imu\n\ndef __init__(self):\n    self.imu_sub = self.create_subscription(\n        Imu,\n        '/imu',\n        self.imu_callback,\n        10\n    )\n\ndef imu_callback(self, msg):\n    accel = msg.linear_acceleration\n    angular_vel = msg.angular_velocity\n\n    self.get_logger().info(\n        f'Accel: {accel.x:.2f}, {accel.y:.2f}, {accel.z:.2f}'\n    )\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#collision-detection","title":"Collision Detection","text":"<p>Gazebo automatically detects collisions. Access them via ROS 2:</p> <pre><code>from gazebo_msgs.srv import GetContactsInfo\n\nclass CollisionDetector(Node):\n    def __init__(self):\n        super().__init__('collision_detector')\n        self.client = self.create_client(\n            GetContactsInfo,\n            '/gazebo/get_contacts'\n        )\n\n        self.timer = self.create_timer(0.1, self.check_collisions)\n\n    def check_collisions(self):\n        if not self.client.service_is_ready():\n            return\n\n        future = self.client.call_async(GetContactsInfo.Request())\n\n        def callback(future):\n            contacts = future.result().contacts\n            if contacts:\n                self.get_logger().warn(f'Collision detected! {len(contacts)} contacts')\n\n        future.add_done_callback(callback)\n</code></pre>"},{"location":"module-2-digital-twin/1-gazebo-simulation/#summary","title":"Summary","text":"<p>Gazebo provides: - \u2705 Realistic Physics - Gravity, friction, inertia - \u2705 Sensor Simulation - Cameras, LiDAR, IMU - \u2705 ROS 2 Integration - Direct topic communication - \u2705 Safe Testing - Test algorithms without hardware risk - \u2705 Reproducibility - Deterministic simulations for debugging</p> <p>Next: Create high-fidelity 3D environments with Unity!</p>"},{"location":"module-2-digital-twin/2-unity-simulation/","title":"Chapter 5: High-Fidelity Simulation with Unity","text":""},{"location":"module-2-digital-twin/2-unity-simulation/#why-unity-for-robotics","title":"Why Unity for Robotics?","text":"<p>While Gazebo excels at physics accuracy, Unity provides: - Photorealistic graphics - Detailed 3D environments - Human-robot interaction - Test social navigation - Semantic understanding - AI can \"reason about\" scenes - Fast iteration - Visual feedback for development</p>"},{"location":"module-2-digital-twin/2-unity-simulation/#setup-unity-robotics","title":"Setup: Unity Robotics","text":""},{"location":"module-2-digital-twin/2-unity-simulation/#install-unity-hub","title":"Install Unity Hub","text":"<ol> <li>Download from <code>unity.com/download</code></li> <li>Create a Unity account</li> <li>Install Unity 2022 LTS or newer</li> </ol>"},{"location":"module-2-digital-twin/2-unity-simulation/#install-ros-tcp-connector","title":"Install ROS-TCP Connector","text":"<p>In Unity, use the Package Manager: <pre><code>Window \u2192 TextMesh Pro \u2192 Import TMP Essentials\nWindow \u2192 Package Manager \u2192 Add package from git URL\nhttps://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.ros-tcp-connector\n</code></pre></p>"},{"location":"module-2-digital-twin/2-unity-simulation/#configure-ros-connection","title":"Configure ROS Connection","text":"<ol> <li>Create an empty GameObject called \"RosConnector\"</li> <li>Add component: <code>ROS Connector</code></li> <li>Set:</li> <li>ROS IP Address: <code>127.0.0.1</code> (localhost)</li> <li>ROS Port: <code>5005</code></li> </ol>"},{"location":"module-2-digital-twin/2-unity-simulation/#creating-a-humanoid-in-unity","title":"Creating a Humanoid in Unity","text":""},{"location":"module-2-digital-twin/2-unity-simulation/#step-1-model-the-robot","title":"Step 1: Model the Robot","text":"<p>Create a hierarchy: <pre><code>Humanoid\n\u251c\u2500\u2500 Torso (Cube, scale 0.3 \u00d7 0.4 \u00d7 1.2)\n\u251c\u2500\u2500 Head (Sphere, radius 0.15)\n\u2502   \u2514\u2500\u2500 Camera (Camera component)\n\u251c\u2500\u2500 LeftArm\n\u2502   \u251c\u2500\u2500 LeftShoulder (Sphere)\n\u2502   \u2514\u2500\u2500 LeftForearm (Cylinder)\n\u2514\u2500\u2500 RightArm\n    \u251c\u2500\u2500 RightShoulder (Sphere)\n    \u2514\u2500\u2500 RightForearm (Cylinder)\n</code></pre></p>"},{"location":"module-2-digital-twin/2-unity-simulation/#step-2-add-physics","title":"Step 2: Add Physics","text":"<p>For each body part: 1. Add <code>Rigidbody</code> component 2. Set mass (torso: 15kg, arms: 2kg each) 3. Add <code>Capsule Collider</code> for realistic collisions</p>"},{"location":"module-2-digital-twin/2-unity-simulation/#step-3-add-joints","title":"Step 3: Add Joints","text":"<p>Connect body parts with <code>ConfigurableJoint</code>:</p> <pre><code>using UnityEngine;\n\npublic class RobotJointController : MonoBehaviour\n{\n    public ConfigurableJoint joint;\n    public float targetAngle = 0f;\n    public float forceLimit = 100f;\n\n    void FixedUpdate()\n    {\n        // Calculate torque needed to reach target angle\n        float currentAngle = transform.localEulerAngles.x;\n        float error = targetAngle - currentAngle;\n\n        // Apply torque proportional to error\n        Rigidbody rb = joint.GetComponent&lt;Rigidbody&gt;();\n        Vector3 torque = Vector3.right * error * forceLimit;\n        rb.AddTorque(torque);\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#subscribing-to-ros-topics","title":"Subscribing to ROS Topics","text":""},{"location":"module-2-digital-twin/2-unity-simulation/#create-a-ros-subscriber-script","title":"Create a ROS Subscriber Script","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Std;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.ROSGeometry;\n\npublic class TwistSubscriber : MonoBehaviour\n{\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe&lt;TwistMsg&gt;(\"/cmd_vel\", OnTwistReceived);\n    }\n\n    void OnTwistReceived(TwistMsg twist)\n    {\n        // Extract linear and angular velocities\n        float forward = (float)twist.linear.x;\n        float rotate = (float)twist.angular.z;\n\n        // Apply to robot\n        Rigidbody rb = GetComponent&lt;Rigidbody&gt;();\n        rb.velocity = new Vector3(forward, 0, 0);\n        rb.angularVelocity = new Vector3(0, rotate, 0);\n\n        Debug.Log($\"Moving forward: {forward}, rotating: {rotate}\");\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#publishing-robot-state","title":"Publishing Robot State","text":"<pre><code>using RosMessageTypes.Sensor;\nusing RosMessageTypes.Nav;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class OdometryPublisher : MonoBehaviour\n{\n    private ROSConnection ros;\n    private string odometryTopic = \"/odom\";\n    private float publishRate = 30f;\n    private float lastPublishTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher&lt;OdometryMsg&gt;(odometryTopic);\n    }\n\n    void FixedUpdate()\n    {\n        if (Time.time - lastPublishTime &gt; 1f / publishRate)\n        {\n            PublishOdometry();\n            lastPublishTime = Time.time;\n        }\n    }\n\n    void PublishOdometry()\n    {\n        // Get current position and velocity\n        Rigidbody rb = GetComponent&lt;Rigidbody&gt;();\n\n        var odomMsg = new OdometryMsg\n        {\n            header = new HeaderMsg\n            {\n                stamp = new TimeMsg { sec = (uint)Time.time },\n                frame_id = \"odom\"\n            },\n            child_frame_id = \"base_link\",\n            pose = new PoseWithCovarianceMsg\n            {\n                pose = new PoseMsg\n                {\n                    position = new PointMsg\n                    {\n                        x = transform.position.x,\n                        y = transform.position.y,\n                        z = transform.position.z\n                    }\n                }\n            },\n            twist = new TwistWithCovarianceMsg\n            {\n                twist = new TwistMsg\n                {\n                    linear = new Vector3Msg\n                    {\n                        x = rb.velocity.x,\n                        y = rb.velocity.y,\n                        z = rb.velocity.z\n                    }\n                }\n            }\n        };\n\n        ros.Publish(odometryTopic, odomMsg);\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#unity-camera-integration","title":"Unity Camera Integration","text":""},{"location":"module-2-digital-twin/2-unity-simulation/#subscribe-to-ros-image-and-render-in-unity","title":"Subscribe to ROS Image and Render in Unity","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class ImageDisplayer : MonoBehaviour\n{\n    public RawImage displayImage;\n    private Texture2D texture;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe&lt;ImageMsg&gt;(\"/camera/image_raw\", OnImageReceived);\n\n        texture = new Texture2D(640, 480, TextureFormat.RGB24, false);\n        displayImage.texture = texture;\n    }\n\n    void OnImageReceived(ImageMsg imageMsg)\n    {\n        // Convert ROS image to Unity texture\n        texture.LoadRawTextureData(imageMsg.data);\n        texture.Apply();\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#publish-camera-images-from-unity","title":"Publish Camera Images from Unity","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class UnityCamera : MonoBehaviour\n{\n    public Camera captureCamera;\n    private ROSConnection ros;\n    private Texture2D texture;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher&lt;ImageMsg&gt;(\"/unity/camera/image\");\n\n        texture = new Texture2D(640, 480, TextureFormat.RGB24, false);\n    }\n\n    void Update()\n    {\n        // Render camera to texture\n        RenderTexture rt = new RenderTexture(640, 480, 24);\n        captureCamera.targetTexture = rt;\n        captureCamera.Render();\n\n        // Read texture\n        RenderTexture.active = rt;\n        texture.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);\n        texture.Apply();\n\n        // Publish to ROS\n        var imageMsg = new ImageMsg\n        {\n            header = new HeaderMsg\n            {\n                stamp = new TimeMsg { sec = (uint)Time.time },\n                frame_id = \"camera\"\n            },\n            height = 480,\n            width = 640,\n            encoding = \"rgb8\",\n            is_bigendian = false,\n            step = 640 * 3,\n            data = texture.GetRawTextureData()\n        };\n\n        ros.Publish(\"/unity/camera/image\", imageMsg);\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#advanced-physics-based-humanoid-locomotion","title":"Advanced: Physics-Based Humanoid Locomotion","text":"<pre><code>public class HumanoidLocomotion : MonoBehaviour\n{\n    [SerializeField] private Rigidbody torsoRb;\n    [SerializeField] private Rigidbody leftFootRb;\n    [SerializeField] private Rigidbody rightFootRb;\n    [SerializeField] private float walkSpeed = 1f;\n    [SerializeField] private float stepHeight = 0.3f;\n\n    private float stepCycle = 0f;\n    private const float StepDuration = 0.5f;\n\n    void FixedUpdate()\n    {\n        stepCycle += Time.fixedDeltaTime / StepDuration;\n        if (stepCycle &gt; 1f) stepCycle -= 1f;\n\n        // Alternating leg movement (bipedal gait)\n        float leftLift = Mathf.Sin(stepCycle * Mathf.PI) * stepHeight;\n        float rightLift = Mathf.Sin((stepCycle + 0.5f) * Mathf.PI) * stepHeight;\n\n        // Move torso forward\n        torsoRb.velocity = new Vector3(walkSpeed, torsoRb.velocity.y, 0);\n\n        // Lift legs in walking pattern\n        Vector3 leftFootPos = leftFootRb.position;\n        leftFootPos.y = Mathf.Max(leftFootPos.y + leftLift, 0);\n        leftFootRb.MovePosition(leftFootPos);\n\n        Vector3 rightFootPos = rightFootRb.position;\n        rightFootPos.y = Mathf.Max(rightFootPos.y + rightLift, 0);\n        rightFootRb.MovePosition(rightFootPos);\n    }\n}\n</code></pre>"},{"location":"module-2-digital-twin/2-unity-simulation/#summary","title":"Summary","text":"<p>Unity robotics provides: - \u2705 Photorealistic simulation - Better for visualizing human-robot interaction - \u2705 Advanced graphics - Test perception in realistic scenarios - \u2705 ROS integration - Full bidirectional communication - \u2705 Game engine power - Animators, shaders, particles - \u2705 Fast development - Visual editor for scene design</p> <p>Next: Deploy advanced AI perception with NVIDIA Isaac!</p>"},{"location":"module-3-ai-robot-brain/","title":"Module 3: The AI-Robot Brain","text":""},{"location":"module-3-ai-robot-brain/#nvidia-isaac-for-advanced-perception","title":"NVIDIA Isaac for Advanced Perception","text":"<p>Welcome to Module 3: The AI-Robot Brain. In this module, you'll use NVIDIA Isaac to add AI capabilities to your robot, including photorealistic simulation and hardware-accelerated perception.</p>"},{"location":"module-3-ai-robot-brain/#what-is-nvidia-isaac","title":"What is NVIDIA Isaac?","text":"<p>NVIDIA Isaac is an end-to-end robotics platform providing: - Isaac Sim - Photorealistic synthetic data generation - Isaac ROS - GPU-accelerated perception algorithms - Isaac Manipulator - Pre-trained models for object manipulation - Nav2 Integration - Autonomous navigation</p>"},{"location":"module-3-ai-robot-brain/#why-isaac-over-standard-simulators","title":"Why Isaac Over Standard Simulators?","text":"<p>Advanced Capabilities: - Photorealistic graphics (RTX ray-tracing) - Synthetic data generation for AI training - Hardware-accelerated vision algorithms - Domain randomization for real-world transfer - Direct NVIDIA hardware optimization</p>"},{"location":"module-3-ai-robot-brain/#module-topics","title":"Module Topics","text":"<p>This module covers:</p> <ol> <li>Isaac Sim - Photorealistic simulation environment</li> <li>Synthetic Data Generation - Create training datasets</li> <li>Isaac ROS - GPU-accelerated perception</li> <li>Visual SLAM - Real-time localization and mapping</li> <li>Nav2 Integration - Autonomous navigation for humanoids</li> </ol>"},{"location":"module-3-ai-robot-brain/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this module, you will:</p> <p>\u2705 Set up Isaac Sim environments \u2705 Generate synthetic training data \u2705 Implement GPU-accelerated vision pipelines \u2705 Deploy VSLAM for robot localization \u2705 Integrate Nav2 for autonomous navigation \u2705 Train AI models on synthetic data \u2705 Transfer learned behaviors to real robots  </p>"},{"location":"module-3-ai-robot-brain/#key-technologies","title":"Key Technologies","text":"<ul> <li>Isaac Sim 2024.1+ - Photorealistic simulation</li> <li>USD Format - Scene descriptions</li> <li>NVIDIA CUDA - GPU acceleration</li> <li>Isaac ROS - Perception stack</li> <li>Nav2 - Navigation framework</li> <li>TensorRT - Model optimization</li> </ul>"},{"location":"module-3-ai-robot-brain/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Module 1 &amp; 2</li> <li>NVIDIA GPU (RTX 4070 Ti+)</li> <li>CUDA toolkit installed</li> <li>ROS 2 knowledge from Module 1</li> </ul>"},{"location":"module-3-ai-robot-brain/#hardware-requirements","title":"Hardware Requirements","text":"<p>Minimum: - GPU: RTX 4070 Ti (12GB VRAM) - RAM: 32GB DDR5 - Storage: 100GB SSD</p> <p>Recommended: - GPU: RTX 4090 (24GB VRAM) - RAM: 64GB DDR5 - Storage: 500GB NVMe SSD</p>"},{"location":"module-3-ai-robot-brain/#installation","title":"Installation","text":"<pre><code># Download Isaac Sim from NVIDIA\n# https://www.nvidia.com/en-us/isaac/\n\n# Or use Docker\ndocker pull nvcr.io/nvidia/isaac-sim:2024.1\ndocker run --gpus all -it nvcr.io/nvidia/isaac-sim:2024.1\n</code></pre>"},{"location":"module-3-ai-robot-brain/#next-steps","title":"Next Steps","text":"<ol> <li>Start with Isaac Sim</li> <li>Learn synthetic data generation</li> <li>Implement perception pipelines</li> <li>Then move to Module 4: Vision-Language-Action</li> </ol> <p>Module 3 of 4 | The AI-Robot Brain | NVIDIA Isaac</p>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/","title":"Chapter 6: NVIDIA Isaac - The AI Robot Brain","text":""},{"location":"module-3-ai-robot-brain/1-isaac-sim/#what-is-nvidia-isaac","title":"What is NVIDIA Isaac?","text":"<p>NVIDIA Isaac is a comprehensive robotics platform that provides: - Isaac Sim - Photorealistic simulation with synthetic data generation - Isaac ROS - Hardware-accelerated vision and perception algorithms - Isaac Manipulator - Pre-trained models for object manipulation - Nav2 Integration - Path planning for humanoid navigation</p>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#isaac-sim-photorealistic-simulation","title":"Isaac Sim: Photorealistic Simulation","text":""},{"location":"module-3-ai-robot-brain/1-isaac-sim/#installation","title":"Installation","text":"<pre><code># Download from NVIDIA (free with registration)\n# https://www.nvidia.com/en-us/isaac/\n\n# Or use Docker\ndocker pull nvcr.io/nvidia/isaac-sim:2024.1\n\n# Run Isaac Sim\ndocker run --gpus all -it --rm \\\n  -v ~/isaac_data:/home/user/isaac_data \\\n  nvcr.io/nvidia/isaac-sim:2024.1\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#creating-a-humanoid-world-in-isaac-sim","title":"Creating a Humanoid World in Isaac Sim","text":"<p>Isaac Sim uses USD (Universal Scene Description) format:</p> <pre><code># Create file: create_humanoid_world.py\nimport carb\nfrom isaacsim import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": False})\n\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.prims import XFormPrim\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_ground_plane()\n\n# Add robot (pre-built humanoid model)\nrobot_prim_path = \"/World/humanoid\"\nadd_reference_to_stage(\n    usd_path=\"omniverse://localhost/NVIDIA/Assets/Isaac/2024.1/Isaac/Robots/Humanoids/H1/h1.usd\",\n    prim_path=robot_prim_path\n)\n\n# Simulate for 1000 frames\nfor i in range(1000):\n    world.step(render=True)\n\nsimulation_app.close()\n</code></pre> <p>Run it: <pre><code>python create_humanoid_world.py\n</code></pre></p>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Isaac Sim generates unlimited labeled training data:</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom isaacsim import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": True})  # Headless for speed\n\nimport omni\nfrom omni.isaac.core.utils.rotations import euler_angles_to_quat\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera\n\nworld = World()\nworld.scene.add_ground_plane()\n\n# Add camera\ncamera = Camera(\n    prim_path=\"/World/camera\",\n    resolution=(640, 480),\n    translation=[1.0, 1.0, 1.0],\n    orientation=euler_angles_to_quat([0, 0.7, 0])\n)\n\n# Add some objects to detect\nobjects = []\nfor i in range(10):\n    # Random cube\n    pos = [i * 0.5, np.random.rand() * 2, 0.5]\n    add_reference_to_stage(\n        usd_path=\"omniverse://localhost/NVIDIA/Assets/Isaac/2024.1/Isaac/Props/Primitives/Cube.usd\",\n        prim_path=f\"/World/object_{i}\",\n        position=pos\n    )\n\n# Capture and save 1000 annotated frames\nfor frame in range(1000):\n    world.step(render=False)\n\n    # Get camera image\n    rgb_data = camera.get_rgb()\n    depth_data = camera.get_depth()\n\n    # Save image\n    Image.fromarray((rgb_data * 255).astype(np.uint8)).save(\n        f\"training_data/rgb_{frame:04d}.png\"\n    )\n\n    # Save depth\n    (depth_data * 1000).astype(np.uint16).tobytes() # 16-bit depth\n\n    # Save annotations (object positions, labels, etc.)\n    # ... annotation code ...\n\nprint(\"Generated 1000 training images with automatic labels!\")\n\nsimulation_app.close()\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#isaac-ros-hardware-accelerated-perception","title":"Isaac ROS: Hardware-Accelerated Perception","text":""},{"location":"module-3-ai-robot-brain/1-isaac-sim/#vslam-visual-simultaneous-localization-and-mapping","title":"VSLAM (Visual Simultaneous Localization and Mapping)","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Twist\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import Path, Odometry\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass IsaacVSLAMNode(Node):\n    \"\"\"\n    Visual SLAM using Isaac ROS accelerated kernels\n    GPU-accelerated visual odometry from camera images\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_vslam')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Subscribe to camera info (intrinsics)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publish odometry and map\n        self.odometry_pub = self.create_publisher(Odometry, '/odom', 10)\n        self.path_pub = self.create_publisher(Path, '/path', 10)\n\n        self.bridge = CvBridge()\n        self.prev_frame = None\n        self.path_poses = []\n\n        self.get_logger().info(\"Isaac VSLAM Node started\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera frames\"\"\"\n\n        # Convert ROS image to OpenCV\n        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')\n\n        if self.prev_frame is None:\n            self.prev_frame = frame\n            return\n\n        # Feature detection (ORB - fast, GPU-friendly)\n        orb = cv2.ORB_create(nfeatures=500)\n        kp1, des1 = orb.detectAndCompute(self.prev_frame, None)\n        kp2, des2 = orb.detectAndCompute(frame, None)\n\n        if des1 is None or des2 is None:\n            return\n\n        # Feature matching\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        matches = bf.match(des1, des2)\n        matches = sorted(matches, key=lambda x: x.distance)\n\n        # Extract matched points\n        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches[:20]])\n        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches[:20]])\n\n        # Compute Essential Matrix (camera motion)\n        E, mask = cv2.findEssentialMat(pts1, pts2)\n\n        _, R, t, mask = cv2.recoverPose(E, pts1, pts2)\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \"odom\"\n        odom_msg.child_frame_id = \"camera\"\n\n        # Position\n        odom_msg.pose.pose.position.x = float(t[0])\n        odom_msg.pose.pose.position.y = float(t[1])\n        odom_msg.pose.pose.position.z = float(t[2])\n\n        self.odometry_pub.publish(odom_msg)\n\n        self.prev_frame = frame\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera intrinsics for VSLAM\"\"\"\n        self.K = np.array(msg.K).reshape(3, 3)\n        self.D = np.array(msg.D)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacVSLAMNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#object-detection-with-isaac-ros-perception","title":"Object Detection with Isaac ROS Perception","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, BoundingBox2D\nfrom cv_bridge import CvBridge\nimport torch\nfrom torchvision import models, transforms\nimport cv2\n\nclass IsaacObjectDetector(Node):\n    \"\"\"\n    GPU-accelerated object detection using YOLOv8\n    Optimized with NVIDIA TensorRT\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_detector')\n\n        # Load YOLOv8 model (optimized for Jetson)\n        self.model = torch.hub.load(\n            'ultralytics/yolov8',\n            'custom',\n            path='yolov8n.pt',  # nano model for speed\n            force_reload=False\n        )\n\n        # Set to GPU\n        self.model.to('cuda')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.detect_callback,\n            10\n        )\n\n        # Publish detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/detections',\n            10\n        )\n\n        self.bridge = CvBridge()\n        self.get_logger().info(\"Object Detector initialized (GPU-accelerated)\")\n\n    def detect_callback(self, msg):\n        \"\"\"Detect objects in image\"\"\"\n\n        # Convert ROS image to OpenCV\n        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Run inference\n        with torch.no_grad():\n            results = self.model(frame)\n\n        # Create detection array\n        detections = Detection2DArray()\n        detections.header.stamp = msg.header\n        detections.header.frame_id = \"camera\"\n\n        # Extract bounding boxes\n        for *box, conf, cls in results.xyxy[0]:\n            det = Detection2D()\n            det.results[0].id = str(int(cls))\n            det.results[0].score = float(conf)\n\n            # Bounding box\n            x1, y1, x2, y2 = box\n            det.bbox.center.x = float((x1 + x2) / 2)\n            det.bbox.center.y = float((y1 + y2) / 2)\n            det.bbox.size_x = float(x2 - x1)\n            det.bbox.size_y = float(y2 - y1)\n\n            detections.detections.append(det)\n\n        # Publish\n        self.detection_pub.publish(detections)\n\n        self.get_logger().info(\n            f\"Detected {len(detections.detections)} objects\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacObjectDetector()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#path-planning-with-nav2","title":"Path Planning with Nav2","text":""},{"location":"module-3-ai-robot-brain/1-isaac-sim/#setup-nav2-for-humanoid-navigation","title":"Setup Nav2 for Humanoid Navigation","text":"<pre><code>sudo apt install ros-humble-nav2 ros-humble-nav2-bringup\n</code></pre> <p>Create <code>nav2_humanoid.launch.py</code>:</p> <pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    nav2_dir = get_package_share_directory('nav2_bringup')\n\n    return LaunchDescription([\n        # Nav2 Costmap Server\n        Node(\n            package='nav2_costmap_2d',\n            executable='costmap_2d_node',\n            name='global_costmap',\n            parameters=[{\n                'use_sim_time': True,\n                'global_frame': 'map',\n                'robot_base_frame': 'base_link',\n                'plugins': ['static_layer', 'obstacle_layer', 'inflation_layer'],\n                'inflation_layer.inflation_radius': 0.5,\n            }]\n        ),\n\n        # Planner Server\n        Node(\n            package='nav2_planner',\n            executable='planner_server',\n            parameters=[{'use_sim_time': True}]\n        ),\n\n        # Controller Server\n        Node(\n            package='nav2_controller',\n            executable='controller_server',\n            parameters=[{'use_sim_time': True}]\n        ),\n    ])\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#use-nav2-to-navigate","title":"Use Nav2 to Navigate","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav2_simple_commander.robot_navigator import BasicNavigator\nfrom nav_msgs.msg import Path\n\nclass HumanoidNavigator(Node):\n    \"\"\"Navigate humanoid robot using Nav2\"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_navigator')\n        self.navigator = BasicNavigator()\n\n    def navigate_to(self, x, y, theta):\n        \"\"\"Send robot to target position\"\"\"\n\n        # Create goal pose\n        goal_pose = PoseStamped()\n        goal_pose.header.frame_id = 'map'\n        goal_pose.pose.position.x = x\n        goal_pose.pose.position.y = y\n        goal_pose.pose.orientation.w = 1.0\n\n        self.navigator.setInitialPose(PoseStamped())\n        self.navigator.goToPose(goal_pose)\n\n        self.get_logger().info(f'Navigating to ({x}, {y})')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidNavigator()\n\n    # Navigate robot\n    node.navigate_to(x=5.0, y=3.0, theta=0.0)\n\n    # Wait for completion\n    while not node.navigator.isNavComplete():\n        rclpy.spin_once(node)\n\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-3-ai-robot-brain/1-isaac-sim/#summary","title":"Summary","text":"<p>NVIDIA Isaac provides: - \u2705 Photorealistic simulation - Isaac Sim for testing - \u2705 Synthetic data - Unlimited labeled training data - \u2705 GPU acceleration - Fast perception algorithms - \u2705 Ready models - Pre-trained detection, SLAM, control - \u2705 Path planning - Nav2 integration for navigation</p> <p>Next: Combine language understanding with robotics using Vision-Language-Action models!</p>"},{"location":"module-4-vision-language-action/","title":"Module 4: Vision-Language-Action","text":""},{"location":"module-4-vision-language-action/#building-robots-that-understand-natural-language","title":"Building Robots That Understand Natural Language","text":"<p>Welcome to Module 4: Vision-Language-Action (VLA). In this final module, you'll create robots that understand natural language commands and execute complex tasks.</p>"},{"location":"module-4-vision-language-action/#what-is-vision-language-action","title":"What is Vision-Language-Action?","text":"<p>VLA systems combine three AI capabilities:</p> <p>Vision - See and understand the world - Object detection and recognition - Scene segmentation and reasoning - 3D spatial understanding</p> <p>Language - Understand human commands - Speech-to-text (Whisper) - Natural language understanding (GPT-4) - Task planning from descriptions</p> <p>Action - Translate understanding into robot movement - ROS 2 motion control - Gripper manipulation - Real-time feedback and adaptation</p>"},{"location":"module-4-vision-language-action/#the-vla-pipeline","title":"The VLA Pipeline","text":"<pre><code>Human: \"Pick up the red cup\"\n    \u2193\n[Whisper: Speech-to-Text]\n    \u2193\nText: \"Pick up the red cup\"\n    \u2193\n[CLIP: Vision Understanding]\n\"Red cup detected at (0.5, 0.3, 0.8)\"\n    \u2193\n[GPT-4: Action Planning]\nPlan: [move_to, lower_gripper, close, lift]\n    \u2193\n[ROS 2: Execution]\n    \u2193\nRobot completes task \u2713\n</code></pre>"},{"location":"module-4-vision-language-action/#module-topics","title":"Module Topics","text":"<p>This module covers:</p> <ol> <li>Voice Commands - Speech-to-text with Whisper</li> <li>Vision Understanding - CLIP and object detection</li> <li>LLM Planning - GPT-4 for task planning</li> <li>Robot Execution - ROS 2 motion control</li> </ol>"},{"location":"module-4-vision-language-action/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this module, you will:</p> <p>\u2705 Implement speech-to-text with Whisper \u2705 Use CLIP for vision understanding \u2705 Use GPT-4 for task planning \u2705 Create action sequence pipelines \u2705 Control humanoid robots via natural language \u2705 Implement real-time feedback loops \u2705 Deploy end-to-end VLA systems  </p>"},{"location":"module-4-vision-language-action/#key-technologies","title":"Key Technologies","text":"<ul> <li>OpenAI Whisper - Speech recognition</li> <li>OpenAI CLIP - Vision-language understanding</li> <li>GPT-4 - Language model planning</li> <li>OpenAI API - LLM access</li> <li>ROS 2 - Robot control</li> <li>Python Async - Real-time coordination</li> </ul>"},{"location":"module-4-vision-language-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Modules 1, 2, and 3</li> <li>OpenAI API key (for GPT-4 and Whisper)</li> <li>Microphone for voice input</li> <li>ROS 2 installation from Module 1</li> </ul>"},{"location":"module-4-vision-language-action/#api-requirements","title":"API Requirements","text":"<pre><code># Install required packages\npip install openai-whisper\npip install clip\npip install openai  # For GPT-4\n\n# Set API key\nexport OPENAI_API_KEY=\"your-key-here\"\n</code></pre>"},{"location":"module-4-vision-language-action/#real-world-applications","title":"Real-World Applications","text":"<p>This VLA pipeline enables: - \u2705 Home service robots - \u2705 Industrial assembly robots - \u2705 Healthcare assistance robots - \u2705 Elder care companions - \u2705 Research platforms</p>"},{"location":"module-4-vision-language-action/#next-steps","title":"Next Steps","text":"<ol> <li>Start with VLA Systems</li> <li>Implement all pipeline stages</li> <li>Test with voice commands</li> <li>Then complete the Capstone Project</li> </ol> <p>Module 4 of 4 | Vision-Language-Action | Natural Language Robot Control</p>"},{"location":"module-4-vision-language-action/1-vla-systems/","title":"Chapter 7: Vision-Language-Action Models","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#what-is-vla","title":"What is VLA?","text":"<p>Vision-Language-Action (VLA) systems combine three capabilities: - Vision - See and understand the world (cameras, object detection) - Language - Understand natural language commands (LLMs) - Action - Translate understanding into robot movements (ROS 2 control)</p> <p>This creates robots that respond to natural language: \"Clean the table\" \u2192 robot understands, plans, and executes.</p>"},{"location":"module-4-vision-language-action/1-vla-systems/#the-vla-pipeline","title":"The VLA Pipeline","text":"<pre><code>Human: \"Pick up the red cup\"\n     \u2193\n[Speech-to-Text: Whisper]\n     \u2193\nText: \"Pick up the red cup\"\n     \u2193\n[Vision: CLIP + Object Detection]\n     \u2193\nScene Understanding: \"Red cup at (0.5, 0.3, 0.8)\"\n     \u2193\n[Language Model: GPT-4]\n     \u2193\nAction Plan: [\"move_to(0.5, 0.3, 1.0)\", \"lower_gripper()\", \"close_gripper()\"]\n     \u2193\n[ROS 2 Controllers]\n     \u2193\nRobot executes plan\n</code></pre>"},{"location":"module-4-vision-language-action/1-vla-systems/#part-1-voice-input-with-whisper","title":"Part 1: Voice Input with Whisper","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#speech-to-text-with-openai-whisper","title":"Speech-to-Text with OpenAI Whisper","text":"<pre><code>pip install openai-whisper sounddevice numpy scipy\n</code></pre> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\n\nclass VoiceCommandNode(Node):\n    \"\"\"Listen to voice commands and transcribe with Whisper\"\"\"\n\n    def __init__(self):\n        super().__init__('voice_command')\n\n        # Load Whisper model\n        self.model = whisper.load_model(\"base\")  # Options: tiny, base, small, medium, large\n\n        # Publisher for transcribed text\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\n\n        # Timer to record audio\n        self.timer = self.create_timer(1.0, self.record_and_transcribe)\n\n        self.get_logger().info(\"Voice command node ready (listening...)\")\n\n    def record_and_transcribe(self):\n        \"\"\"Record 5 seconds of audio and transcribe\"\"\"\n\n        # Record 5 seconds at 16kHz\n        sample_rate = 16000\n        duration = 5\n\n        self.get_logger().info(\"Recording...\")\n        audio = sd.rec(\n            int(sample_rate * duration),\n            samplerate=sample_rate,\n            channels=1,\n            dtype=np.int16\n        )\n        sd.wait()\n\n        # Normalize audio\n        audio_float = audio.astype(np.float32) / 32768.0\n\n        # Transcribe\n        result = self.model.transcribe(audio_float)\n        text = result['text'].strip()\n\n        if text:\n            self.get_logger().info(f\"You said: {text}\")\n\n            # Publish command\n            msg = String()\n            msg.data = text\n            self.command_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-4-vision-language-action/1-vla-systems/#part-2-vision-understanding-with-clip","title":"Part 2: Vision Understanding with CLIP","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#object-detection-and-understanding","title":"Object Detection and Understanding","text":"<pre><code>import torch\nfrom PIL import Image\nimport clip\n\nclass VisionUnderstandingNode(Node):\n    \"\"\"Understand scenes using CLIP vision-language model\"\"\"\n\n    def __init__(self):\n        super().__init__('vision_understanding')\n\n        # Load CLIP (Vision-Language model)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.understand_image,\n            10\n        )\n\n        # Publish scene understanding\n        self.scene_pub = self.create_publisher(String, '/scene_description', 10)\n\n        self.bridge = CvBridge()\n        self.get_logger().info(\"Vision understanding node ready\")\n\n    def understand_image(self, msg):\n        \"\"\"Analyze image and describe objects\"\"\"\n\n        # Convert ROS image to PIL\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\n\n        # Objects to look for\n        objects = [\"red cup\", \"blue ball\", \"person\", \"table\", \"chair\", \"book\"]\n\n        with torch.no_grad():\n            # Prepare image\n            image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n            # Prepare text labels\n            text = clip.tokenize(objects).to(self.device)\n\n            # Get similarity scores\n            logits_per_image, _ = self.model(image, text)\n            probabilities = logits_per_image.softmax(dim=-1)\n\n        # Find highest probability\n        top_idx = probabilities[0].argmax().item()\n        top_object = objects[top_idx]\n        top_prob = probabilities[0][top_idx].item()\n\n        scene_desc = f\"Detected: {top_object} (confidence: {top_prob:.2%})\"\n\n        self.get_logger().info(scene_desc)\n\n        # Publish\n        msg = String()\n        msg.data = scene_desc\n        self.scene_pub.publish(msg)\n</code></pre>"},{"location":"module-4-vision-language-action/1-vla-systems/#part-3-llm-planning-with-gpt-4","title":"Part 3: LLM Planning with GPT-4","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#translate-natural-language-to-robot-actions","title":"Translate Natural Language to Robot Actions","text":"<pre><code>import openai\n\nclass LLMPlannerNode(Node):\n    \"\"\"Use GPT-4 to plan robot actions from natural language\"\"\"\n\n    def __init__(self):\n        super().__init__('llm_planner')\n\n        # Set OpenAI API key\n        openai.api_key = \"your-api-key\"\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.plan_actions,\n            10\n        )\n\n        # Publish action sequence\n        self.action_pub = self.create_publisher(String, '/action_sequence', 10)\n\n        self.system_prompt = \"\"\"\n        You are a robot action planner. Convert natural language commands into \n        a sequence of ROS 2 action calls.\n\n        Available actions:\n        - move_forward(distance_m)\n        - turn(angle_degrees)\n        - pickup_object(object_name)\n        - place_object(location)\n        - open_gripper()\n        - close_gripper()\n\n        Example:\n        Command: \"Pick up the red cup from the table\"\n        Actions:\n        1. move_forward(1.0)\n        2. detect_object(\"red cup\")\n        3. move_to_object(\"red cup\")\n        4. open_gripper()\n        5. pickup_object(\"red cup\")\n\n        Respond with only the action sequence, one per line.\n        \"\"\"\n\n        self.get_logger().info(\"LLM Planner ready\")\n\n    def plan_actions(self, msg):\n        \"\"\"Plan robot actions using GPT-4\"\"\"\n\n        command = msg.data\n        self.get_logger().info(f\"Planning actions for: {command}\")\n\n        try:\n            # Call GPT-4\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": command}\n                ],\n                temperature=0.3,  # More deterministic\n                max_tokens=200\n            )\n\n            action_sequence = response['choices'][0]['message']['content']\n\n            self.get_logger().info(f\"Action sequence:\\n{action_sequence}\")\n\n            # Publish\n            msg = String()\n            msg.data = action_sequence\n            self.action_pub.publish(msg)\n\n        except Exception as e:\n            self.get_logger().error(f\"LLM planning failed: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-4-vision-language-action/1-vla-systems/#part-4-action-execution-with-ros-2","title":"Part 4: Action Execution with ROS 2","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#execute-planned-actions-on-robot","title":"Execute Planned Actions on Robot","text":"<pre><code>import re\nfrom geometry_msgs.msg import Twist\n\nclass ActionExecutor(Node):\n    \"\"\"Execute action sequence on real or simulated robot\"\"\"\n\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Subscribers\n        self.action_sub = self.create_subscription(\n            String,\n            '/action_sequence',\n            self.execute_actions,\n            10\n        )\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.gripper_pub = self.create_publisher(String, '/gripper_command', 10)\n\n        self.get_logger().info(\"Action executor ready\")\n\n    def execute_actions(self, msg):\n        \"\"\"Parse and execute action sequence\"\"\"\n\n        action_sequence = msg.data\n        actions = [line.strip() for line in action_sequence.split('\\n') if line.strip()]\n\n        for action in actions:\n            self.get_logger().info(f\"Executing: {action}\")\n            self.execute_single_action(action)\n\n    def execute_single_action(self, action):\n        \"\"\"Execute one action\"\"\"\n\n        # Parse action format: action_name(arg1, arg2, ...)\n        match = re.match(r'(\\w+)\\((.*)\\)', action)\n        if not match:\n            self.get_logger().warn(f\"Invalid action format: {action}\")\n            return\n\n        action_name = match.group(1)\n        args_str = match.group(2)\n        args = [arg.strip().strip('\"\\'') for arg in args_str.split(',')]\n\n        if action_name == 'move_forward':\n            distance = float(args[0])\n            self.move_forward(distance)\n\n        elif action_name == 'turn':\n            angle = float(args[0])\n            self.turn(angle)\n\n        elif action_name == 'open_gripper':\n            self.send_gripper_command('OPEN')\n\n        elif action_name == 'close_gripper':\n            self.send_gripper_command('CLOSE')\n\n        elif action_name == 'pickup_object':\n            obj = args[0]\n            self.pickup_object(obj)\n\n    def move_forward(self, distance):\n        \"\"\"Move robot forward\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.5  # 0.5 m/s\n\n        # Estimate time needed\n        duration = distance / 0.5  # seconds\n\n        for _ in range(int(duration * 10)):\n            self.cmd_vel_pub.publish(msg)\n\n        # Stop\n        msg.linear.x = 0.0\n        self.cmd_vel_pub.publish(msg)\n\n    def turn(self, angle_degrees):\n        \"\"\"Turn robot\"\"\"\n        msg = Twist()\n        angle_rad = angle_degrees * 3.14159 / 180\n        msg.angular.z = 0.5  # 0.5 rad/s\n\n        duration = angle_rad / 0.5\n\n        for _ in range(int(duration * 10)):\n            self.cmd_vel_pub.publish(msg)\n\n        msg.angular.z = 0.0\n        self.cmd_vel_pub.publish(msg)\n\n    def send_gripper_command(self, command):\n        \"\"\"Open or close gripper\"\"\"\n        msg = String()\n        msg.data = command\n        self.gripper_pub.publish(msg)\n\n    def pickup_object(self, object_name):\n        \"\"\"Sequence to pick up an object\"\"\"\n        self.open_gripper()\n        self.move_forward(0.3)\n        self.send_gripper_command('CLOSE')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionExecutor()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"module-4-vision-language-action/1-vla-systems/#complete-vla-system-integration","title":"Complete VLA System Integration","text":""},{"location":"module-4-vision-language-action/1-vla-systems/#launch-everything-together","title":"Launch Everything Together","text":"<pre><code># launch/vla_system.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Voice input\n        Node(\n            package='robot_vla',\n            executable='voice_command_node',\n            output='screen'\n        ),\n\n        # Vision understanding\n        Node(\n            package='robot_vla',\n            executable='vision_understanding_node',\n            output='screen'\n        ),\n\n        # LLM planning\n        Node(\n            package='robot_vla',\n            executable='llm_planner_node',\n            output='screen'\n        ),\n\n        # Action execution\n        Node(\n            package='robot_vla',\n            executable='action_executor_node',\n            output='screen'\n        ),\n    ])\n</code></pre> <p>Run it: <pre><code>ros2 launch robot_vla vla_system.launch.py\n</code></pre></p>"},{"location":"module-4-vision-language-action/1-vla-systems/#summary","title":"Summary","text":"<p>Vision-Language-Action systems enable: - \u2705 Natural language control - Talk to your robot like a human - \u2705 Scene understanding - Vision-language models \"see\" like humans - \u2705 Intelligent planning - LLMs reason about complex tasks - \u2705 Real robot execution - Commands translate to actual movements - \u2705 Adaptive behavior - Can learn new tasks from examples</p> <p>Next: Build the capstone\u2014an autonomous humanoid robot!</p>"},{"location":"physical-ai/","title":"Physical AI &amp; Humanoid Robotics - Comprehensive Textbook","text":""},{"location":"physical-ai/#course-overview","title":"\ud83d\udcda Course Overview","text":"<p>This comprehensive textbook teaches the convergence of artificial intelligence and physical embodiment\u2014the future of robotics. Students will learn to design, simulate, and deploy autonomous humanoid robots using industry-standard tools.</p>"},{"location":"physical-ai/#what-youll-learn","title":"\ud83c\udfaf What You'll Learn","text":""},{"location":"physical-ai/#module-1-the-robotic-nervous-system-ros-2","title":"Module 1: The Robotic Nervous System (ROS 2)","text":"<ul> <li>ROS 2 architecture and core concepts</li> <li>Building publishers, subscribers, and services</li> <li>Creating and managing ROS 2 packages</li> <li>Robot Description Format (URDF)</li> </ul>"},{"location":"physical-ai/#module-2-the-digital-twin-gazebo-unity","title":"Module 2: The Digital Twin (Gazebo &amp; Unity)","text":"<ul> <li>Physics simulation in Gazebo</li> <li>Sensor simulation (cameras, LiDAR, IMUs)</li> <li>Building complex robotic environments</li> <li>Real-time physics for accurate training</li> </ul>"},{"location":"physical-ai/#module-3-the-ai-robot-brain-nvidia-isaac","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","text":"<ul> <li>NVIDIA Isaac Sim for photorealistic simulation</li> <li>Computer vision with Isaac ROS</li> <li>Visual SLAM for robot localization</li> <li>Path planning with Nav2</li> <li>Reinforcement learning for control</li> </ul>"},{"location":"physical-ai/#module-4-vision-language-action-models","title":"Module 4: Vision-Language-Action Models","text":"<ul> <li>Voice-to-action with OpenAI Whisper</li> <li>Language understanding with GPT-4</li> <li>Vision understanding with CLIP</li> <li>End-to-end autonomous task execution</li> </ul>"},{"location":"physical-ai/#capstone-project","title":"\ud83c\udfc6 Capstone Project","text":"<p>Build an autonomous humanoid robot that: 1. Listens to natural language voice commands 2. Understands complex task instructions 3. Plans sequences of robot actions 4. Executes actions in realistic physics simulation 5. Verifies task completion using computer vision</p> <p>Example: Say \"Pick up the red cube and place it on the table\" \u2192 Robot understands \u2192 Plans movements \u2192 Grasps object \u2192 Places it \u2192 Verifies success</p>"},{"location":"physical-ai/#system-requirements","title":"\ud83d\udcbb System Requirements","text":""},{"location":"physical-ai/#development-workstation","title":"Development Workstation","text":"<ul> <li>GPU: NVIDIA RTX 4070 Ti (12GB VRAM) or higher</li> <li>CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9</li> <li>RAM: 64GB DDR5 (minimum 32GB)</li> <li>OS: Ubuntu 22.04 LTS</li> </ul>"},{"location":"physical-ai/#optional-edge-computing-kit","title":"Optional Edge Computing Kit","text":"<ul> <li>NVIDIA Jetson Orin Nano/NX</li> <li>Intel RealSense D435i camera</li> <li>ReSpeaker USB Microphone</li> <li>Total cost: ~$700</li> </ul>"},{"location":"physical-ai/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"physical-ai/#1-setup-ros-2-environment","title":"1. Setup ROS 2 Environment","text":"<pre><code># Install ROS 2 Humble\nsudo apt-get update\nsudo curl -sSL https://raw.githubusercontent.com/ros/ros.key | sudo apt-key add -\nsudo apt-get install -y ros-humble-desktop-full\n\n# Add to bashrc\necho \"source /opt/ros/humble/setup.bash\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Create workspace\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws\ncolcon build\n</code></pre>"},{"location":"physical-ai/#2-install-required-packages","title":"2. Install Required Packages","text":"<pre><code># Core robotics\nsudo apt-get install -y \\\n  gazebo-harmonic \\\n  ros-humble-gazebo-ros2-control \\\n  ros-humble-isaac-ros-* \\\n  ros-humble-nav2-* \\\n  python3-pip\n\n# Python dependencies\npip install opencv-python torch transformers openai sounddevice scipy\n</code></pre>"},{"location":"physical-ai/#3-run-the-textbook","title":"3. Run the Textbook","text":"<pre><code># Build documentation\ncd RoboticsBook\nnpm install\nnpm run build\nnpm run serve  # Access at http://localhost:3000\n</code></pre>"},{"location":"physical-ai/#textbook-structure","title":"\ud83d\udcd6 Textbook Structure","text":"<pre><code>docs/physical-ai/\n\u251c\u2500\u2500 intro.md                          # Course introduction\n\u251c\u2500\u2500 chapter-1-intro.md               # Physical AI fundamentals\n\u251c\u2500\u2500 chapter-2-ros2-architecture.md   # ROS 2 concepts\n\u251c\u2500\u2500 chapter-3-first-node.md          # Your first ROS 2 node\n\u251c\u2500\u2500 chapter-4-urdf.md                # Robot description format\n\u251c\u2500\u2500 chapter-5-gazebo.md              # Physics simulation\n\u251c\u2500\u2500 chapter-6-isaac.md               # NVIDIA Isaac platform\n\u251c\u2500\u2500 chapter-7-vla.md                 # Vision-Language-Action\n\u2514\u2500\u2500 capstone.md                      # Autonomous humanoid project\n</code></pre>"},{"location":"physical-ai/#learning-path","title":"\ud83e\uddea Learning Path","text":"<p>Week 1-2: Introduction to Physical AI - Understand embodied intelligence - Learn why humanoid robots matter - Overview of the robotic stack</p> <p>Week 3-5: ROS 2 Fundamentals - Build your first node - Understand pub/sub communication - Create robot packages</p> <p>Week 6-7: Robot Simulation - Describe robots with URDF - Simulate physics in Gazebo - Add sensors to simulation</p> <p>Week 8-10: Advanced Perception - Visual perception with Isaac - Computer vision pipelines - Path planning and navigation</p> <p>Week 11-13: Autonomous Systems - Voice-to-action pipelines - Language understanding - Capstone project execution</p>"},{"location":"physical-ai/#chapter-highlights","title":"\ud83d\udcdd Chapter Highlights","text":""},{"location":"physical-ai/#chapter-1-introduction-to-physical-ai","title":"Chapter 1: Introduction to Physical AI","text":"<ul> <li>Digital-Physical Bridge concept</li> <li>Why humanoid robots matter</li> <li>The Robotic Stack layers</li> <li>Real-world applications</li> </ul>"},{"location":"physical-ai/#chapter-2-ros-2-architecture","title":"Chapter 2: ROS 2 Architecture","text":"<ul> <li>Core concepts: Nodes, Topics, Services</li> <li>Message types and coordinate frames</li> <li>Quality of Service (QoS)</li> <li>Multi-robot capabilities</li> </ul>"},{"location":"physical-ai/#chapter-3-your-first-ros-2-node","title":"Chapter 3: Your First ROS 2 Node","text":"<ul> <li>Setting up development environment</li> <li>Creating packages</li> <li>Publisher/Subscriber examples</li> <li>Services and debugging tools</li> </ul>"},{"location":"physical-ai/#chapter-4-urdf","title":"Chapter 4: URDF","text":"<ul> <li>Describing robot structure</li> <li>Link and joint definitions</li> <li>Visual vs collision geometry</li> <li>Complete humanoid examples</li> </ul>"},{"location":"physical-ai/#chapter-5-gazebo-physics","title":"Chapter 5: Gazebo Physics","text":"<ul> <li>World files and environments</li> <li>Sensor simulation</li> <li>Actuator control</li> <li>Performance optimization</li> </ul>"},{"location":"physical-ai/#chapter-6-nvidia-isaac","title":"Chapter 6: NVIDIA Isaac","text":"<ul> <li>Isaac Sim setup</li> <li>Computer vision pipelines</li> <li>V-SLAM implementation</li> <li>Reinforcement learning basics</li> </ul>"},{"location":"physical-ai/#chapter-7-vision-language-action","title":"Chapter 7: Vision-Language-Action","text":"<ul> <li>Voice capture with Whisper</li> <li>Language understanding with GPT</li> <li>Vision verification with CLIP</li> <li>Motor control execution</li> </ul>"},{"location":"physical-ai/#capstone-autonomous-humanoid","title":"Capstone: Autonomous Humanoid","text":"<ul> <li>Complete system integration</li> <li>Voice-to-action pipeline</li> <li>Real-time execution and feedback</li> <li>Deployment instructions</li> </ul>"},{"location":"physical-ai/#code-examples","title":"\ud83d\udd27 Code Examples","text":""},{"location":"physical-ai/#simple-publisher","title":"Simple Publisher","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass HelloPublisher(Node):\n    def __init__(self):\n        super().__init__('hello_publisher')\n        self.publisher = self.create_publisher(String, 'topic', 10)\n        self.timer = self.create_timer(1.0, self.timer_callback)\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello, Robot World!'\n        self.publisher.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HelloPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n</code></pre>"},{"location":"physical-ai/#urdf-example","title":"URDF Example","text":"<pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;robot name=\"my_robot\"&gt;\n  &lt;link name=\"base_link\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.1 0.1 0.1\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/visual&gt;\n  &lt;/link&gt;\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/#launch-file","title":"Launch File","text":"<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(package='my_package', executable='my_node')\n    ])\n</code></pre>"},{"location":"physical-ai/#learning-resources","title":"\ud83c\udf93 Learning Resources","text":""},{"location":"physical-ai/#official-documentation","title":"Official Documentation","text":"<ul> <li>ROS 2 Documentation</li> <li>Gazebo Simulation</li> <li>NVIDIA Isaac SDK</li> <li>OpenAI API</li> </ul>"},{"location":"physical-ai/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>\"A Gentle Introduction to ROS\" - Jason O'Kane</li> <li>\"Robotics, Vision and Control\" - Peter Corke</li> <li>\"Deep Reinforcement Learning\" - Richard Sutton &amp; Andrew Barto</li> <li>\"Introduction to Autonomous Mobile Robots\" - Roland Siegwart</li> </ul>"},{"location":"physical-ai/#communities","title":"Communities","text":"<ul> <li>ROS Answers</li> <li>ROS Discourse</li> <li>Gazebo Community</li> <li>NVIDIA Developer Forum</li> </ul>"},{"location":"physical-ai/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>This textbook is continuously evolving. Contributions are welcome!</p> <ul> <li>Report issues: Create a GitHub issue</li> <li>Suggest improvements: Submit a discussion</li> <li>Add content: Fork and create a pull request</li> </ul>"},{"location":"physical-ai/#exercises","title":"\ud83d\udccb Exercises","text":"<p>Each chapter includes hands-on exercises:</p> <ol> <li>Module 1 Exercises</li> <li>Build a simple ROS 2 node</li> <li>Create a pub/sub communication system</li> <li>Write launch files</li> <li> <p>Describe a 2-DOF robot in URDF</p> </li> <li> <p>Module 2 Exercises</p> </li> <li>Simulate physics in Gazebo</li> <li>Add sensors to simulation</li> <li>Control robots with ROS 2</li> <li> <p>Tune physics parameters</p> </li> <li> <p>Module 3 Exercises</p> </li> <li>Create Isaac Sim scenes</li> <li>Implement V-SLAM</li> <li>Train perception models</li> <li> <p>Deploy on Jetson</p> </li> <li> <p>Module 4 Exercises</p> </li> <li>Build voice pipeline</li> <li>Integrate language models</li> <li>Implement vision verification</li> <li>Create multi-step tasks</li> </ol>"},{"location":"physical-ai/#capstone-evaluation","title":"\ud83c\udfc5 Capstone Evaluation","text":"<p>Base Functionality: 100 points - Voice recognition (20 pts) - Language understanding (20 pts) - Action execution (30 pts) - Vision feedback (20 pts) - Code quality (10 pts)</p> <p>Bonus Points (up to 50 extra): - Advanced gait implementation - Obstacle avoidance - Multi-robot coordination - Sim-to-real transfer - Complex task sequences</p>"},{"location":"physical-ai/#next-steps-after-this-course","title":"\ud83d\ude80 Next Steps After This Course","text":"<ol> <li>Deploy to Real Robots</li> <li>Set up Jetson edge devices</li> <li>Connect physical sensors</li> <li> <p>Deploy trained models</p> </li> <li> <p>Advanced Topics</p> </li> <li>Sim-to-real transfer learning</li> <li>Multi-robot coordination</li> <li>Real-time embedded systems</li> <li> <p>Advanced AI/ML integration</p> </li> <li> <p>Build Your Own</p> </li> <li>Create custom robots</li> <li>Develop specialized applications</li> <li>Contribute to open-source robotics</li> </ol>"},{"location":"physical-ai/#license","title":"\ud83d\udcc4 License","text":"<p>This textbook and all accompanying code examples are provided as educational material.</p>"},{"location":"physical-ai/#about-this-course","title":"\ud83d\udc68\u200d\ud83c\udfeb About This Course","text":"<p>Developed for the Panaversity Physical AI &amp; Humanoid Robotics curriculum, this textbook bridges the gap between AI and robotics, preparing the next generation of roboticists and AI engineers.</p>"},{"location":"physical-ai/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Read the documentation</li> <li>\ud83d\udcac Ask in discussions</li> <li>\ud83d\udc1b Report issues</li> <li>\ud83d\udce7 Contact the course team</li> </ul> <p>Ready to build the future of robotics? Let's get started! \ud83d\ude80</p>"},{"location":"physical-ai/capstone/","title":"Capstone Project: Autonomous Humanoid with Voice Commands","text":""},{"location":"physical-ai/capstone/#project-overview","title":"Project Overview","text":"<p>Build an autonomous humanoid robot that can: 1. Listen to voice commands 2. Understand natural language instructions 3. Plan a sequence of actions 4. Execute those actions in simulation 5. Verify success using computer vision</p>"},{"location":"physical-ai/capstone/#final-deliverable","title":"Final Deliverable","text":"<p>A simulated humanoid robot that performs complex tasks based on voice commands, demonstrating the complete Physical AI stack.</p>"},{"location":"physical-ai/capstone/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input (Whisper)             \u2502 (speech \u2192 text)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Language Understanding (GPT-4)    \u2502 (text \u2192 action plan)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Vision-based Verification (CLIP)  \u2502 (verify feasibility)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Motor Control (ROS 2)             \u2502 (action plan \u2192 motor commands)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Gazebo Simulation                 \u2502 (execute in physics)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Vision Feedback                   \u2502 (verify task completion)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"physical-ai/capstone/#project-phases","title":"Project Phases","text":""},{"location":"physical-ai/capstone/#phase-1-setup-and-simulation-week-8-9","title":"Phase 1: Setup and Simulation (Week 8-9)","text":""},{"location":"physical-ai/capstone/#11-create-the-simulated-humanoid","title":"1.1: Create the Simulated Humanoid","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/humanoid_sim.py\n\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_folder\nimport numpy as np\n\nclass HumanoidSimulator:\n    def __init__(self):\n        self.simulation_app = SimulationApp({\"headless\": False})\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Load humanoid robot from NVIDIA's library\n        assets_root_path = get_assets_root_folder()\n        humanoid_usd = assets_root_path + \"/Isaac/2023.1.1/Isaac3rdParty/NVIDIA/Humanoid_Pro/H1.usd\"\n\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path=\"/World/Humanoid\",\n                usd_path=humanoid_usd,\n                position=np.array([0, 0, 0.8])\n            )\n        )\n\n    def step(self):\n        self.world.step(render=True)\n        return self.world.is_playing()\n\n    def close(self):\n        self.simulation_app.close()\n\ndef main():\n    simulator = HumanoidSimulator()\n\n    # Run simulation\n    for i in range(1000):\n        if not simulator.step():\n            break\n\n    simulator.close()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#12-connect-to-ros-2","title":"1.2: Connect to ROS 2","text":"<pre><code># Create bridge between Isaac Sim and ROS 2\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass HumanoidBridge(Node):\n    def __init__(self, simulator):\n        super().__init__('humanoid_bridge')\n        self.simulator = simulator\n\n        # Subscribe to control commands\n        self.cmd_vel_sub = self.create_subscription(\n            Twist,\n            '/cmd_vel',\n            self.velocity_callback,\n            10\n        )\n\n        self.joint_cmd_sub = self.create_subscription(\n            Float64MultiArray,\n            '/joint_commands',\n            self.joint_callback,\n            10\n        )\n\n    def velocity_callback(self, msg):\n        # Convert Twist to humanoid motion\n        linear_x = msg.linear.x\n        angular_z = msg.angular.z\n\n        # Compute walking pattern\n        joint_targets = self.compute_gait(linear_x, angular_z)\n        self.apply_joint_targets(joint_targets)\n\n    def joint_callback(self, msg):\n        self.apply_joint_targets(msg.data)\n\n    def compute_gait(self, forward_speed, turning_speed):\n        \"\"\"Compute bipedal walking gait\"\"\"\n        # Simplified gait computation\n        t = self.simulator.world.current_time\n\n        hip_angles = forward_speed * np.sin(2 * np.pi * t)\n        knee_angles = forward_speed * (np.cos(2 * np.pi * t) - 1)\n\n        return np.array([hip_angles, knee_angles, 0, 0, hip_angles, knee_angles])\n\n    def apply_joint_targets(self, targets):\n        # Apply to simulated robot\n        pass\n</code></pre>"},{"location":"physical-ai/capstone/#phase-2-voice-to-action-pipeline-week-10","title":"Phase 2: Voice to Action Pipeline (Week 10)","text":""},{"location":"physical-ai/capstone/#21-voice-capture","title":"2.1: Voice Capture","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/voice_interface.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport sounddevice as sd\nimport scipy.io.wavfile as wavfile\nimport numpy as np\nimport threading\n\nclass VoiceInterfaceNode(Node):\n    def __init__(self):\n        super().__init__('voice_interface')\n\n        openai.api_key = \"your-api-key-here\"\n\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.listening = True\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.listen_loop)\n        self.listen_thread.daemon = True\n        self.listen_thread.start()\n\n    def listen_loop(self):\n        while self.listening:\n            try:\n                # Record audio\n                self.get_logger().info('Listening...')\n                audio = sd.rec(\n                    int(16000 * 5),  # 5 seconds\n                    samplerate=16000,\n                    channels=1,\n                    dtype=np.int16\n                )\n                sd.wait()\n\n                # Save audio\n                wavfile.write('command.wav', 16000, audio)\n\n                # Transcribe with Whisper\n                with open('command.wav', 'rb') as f:\n                    transcript = openai.Audio.transcribe(\"whisper-1\", f)\n\n                command = transcript['text']\n                if command.strip():\n                    self.get_logger().info(f'Heard: {command}')\n\n                    # Publish command\n                    msg = String()\n                    msg.data = command\n                    self.command_pub.publish(msg)\n\n            except Exception as e:\n                self.get_logger().error(f'Error: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceInterfaceNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#22-language-understanding","title":"2.2: Language Understanding","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/language_planner.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport json\n\nclass LanguagePlannerNode(Node):\n    def __init__(self):\n        super().__init__('language_planner')\n\n        openai.api_key = \"your-api-key-here\"\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice/command',\n            self.command_callback,\n            10\n        )\n\n        # Publish action plan\n        self.plan_pub = self.create_publisher(String, '/action_plan', 10)\n\n    def command_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'Processing: {command}')\n\n        # Generate action plan\n        plan = self.generate_plan(command)\n\n        # Publish plan\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan)\n        self.plan_pub.publish(plan_msg)\n\n    def generate_plan(self, command):\n        \"\"\"Use GPT-4 to generate robot action plan\"\"\"\n\n        system_prompt = \"\"\"You are a robot planning system. Convert natural language commands into \nstructured robot action sequences.\n\nAvailable Actions:\n- move_to(x, y, z): Move arm to position\n- grasp(): Close gripper\n- release(): Open gripper  \n- navigate_to(x, y): Move robot base\n- look_at(x, y, z): Turn camera toward point\n- wait(seconds): Pause\n\nYou MUST respond with valid JSON array of actions only, no other text.\nExample response: [{\"action\": \"navigate_to\", \"params\": {\"x\": 5, \"y\": 0}}, {\"action\": \"grasp\", \"params\": {}}]\n\"\"\"\n\n        user_prompt = f'Command: \"{command}\"'\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.3\n        )\n\n        response_text = response['choices'][0]['message']['content'].strip()\n\n        try:\n            # Extract JSON if there's extra text\n            import re\n            json_match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n            if json_match:\n                plan = json.loads(json_match.group())\n            else:\n                plan = json.loads(response_text)\n\n            return plan\n        except:\n            self.get_logger().error(f'Failed to parse plan: {response_text}')\n            return []\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LanguagePlannerNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#phase-3-execution-and-feedback-week-11","title":"Phase 3: Execution and Feedback (Week 11)","text":""},{"location":"physical-ai/capstone/#31-action-executor","title":"3.1: Action Executor","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/action_executor.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float64MultiArray\nfrom geometry_msgs.msg import Twist\nimport json\nimport time\n\nclass ActionExecutorNode(Node):\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Subscribe to action plans\n        self.plan_sub = self.create_subscription(\n            String,\n            '/action_plan',\n            self.plan_callback,\n            10\n        )\n\n        # Control publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.arm_cmd_pub = self.create_publisher(Float64MultiArray, '/joint_commands', 10)\n\n        # Feedback publisher\n        self.feedback_pub = self.create_publisher(String, '/execution_feedback', 10)\n\n    def plan_callback(self, msg):\n        plan = json.loads(msg.data)\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        for i, action in enumerate(plan):\n            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action[\"action\"]}')\n\n            action_type = action.get('action')\n            params = action.get('params', {})\n\n            try:\n                if action_type == 'move_to':\n                    self.move_to_position(params['x'], params['y'], params['z'])\n                elif action_type == 'grasp':\n                    self.grasp()\n                elif action_type == 'release':\n                    self.release()\n                elif action_type == 'navigate_to':\n                    self.navigate_to(params['x'], params['y'])\n                elif action_type == 'look_at':\n                    self.look_at(params['x'], params['y'], params['z'])\n                elif action_type == 'wait':\n                    time.sleep(params.get('seconds', 1))\n\n                # Send feedback\n                feedback = String()\n                feedback.data = f\"\u2713 Completed: {action_type}\"\n                self.feedback_pub.publish(feedback)\n\n            except Exception as e:\n                self.get_logger().error(f'Error executing {action_type}: {e}')\n\n    def move_to_position(self, x, y, z):\n        \"\"\"Move arm to target position\"\"\"\n        # Compute IK and send joint commands\n        joint_angles = self.compute_inverse_kinematics(x, y, z)\n\n        msg = Float64MultiArray()\n        msg.data = joint_angles\n        self.arm_cmd_pub.publish(msg)\n\n        time.sleep(1)  # Wait for arm to reach\n\n    def grasp(self):\n        \"\"\"Close gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [0.5]  # Close\n        self.arm_cmd_pub.publish(msg)\n        time.sleep(0.5)\n\n    def release(self):\n        \"\"\"Open gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [0.0]  # Open\n        self.arm_cmd_pub.publish(msg)\n        time.sleep(0.5)\n\n    def navigate_to(self, x, y):\n        \"\"\"Navigate robot base to position\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.3  # Forward\n        msg.angular.z = 0.1  # Turning\n        self.cmd_vel_pub.publish(msg)\n\n        time.sleep(3)\n\n    def look_at(self, x, y, z):\n        \"\"\"Turn camera toward point\"\"\"\n        pass\n\n    def compute_inverse_kinematics(self, x, y, z):\n        \"\"\"Compute joint angles for target position\"\"\"\n        # Simplified IK\n        import numpy as np\n        r = np.sqrt(x**2 + y**2)\n\n        angles = [\n            np.arctan2(y, x),\n            np.arctan2(z, r),\n            np.arctan2(z, r) * 0.5,\n            0.0\n        ]\n\n        return angles\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionExecutorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#32-vision-feedback","title":"3.2: Vision Feedback","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/vision_feedback.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nimport json\n\nclass VisionFeedbackNode(Node):\n    def __init__(self):\n        super().__init__('vision_feedback')\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.feedback_pub = self.create_publisher(String, '/vision_feedback', 10)\n\n        # Load CLIP for understanding objects\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Analyze image\n        analysis = self.analyze_image(cv_image)\n\n        # Publish feedback\n        feedback = String()\n        feedback.data = json.dumps(analysis)\n        self.feedback_pub.publish(feedback)\n\n    def analyze_image(self, image):\n        \"\"\"Analyze image for task completion\"\"\"\n\n        # Prepare inputs\n        image_input = self.processor(\n            images=image,\n            return_tensors=\"pt\"\n        )['pixel_values'].to(self.device)\n\n        # Check for common objects\n        objects_to_check = [\"red cube\", \"box\", \"table\", \"gripper\", \"hand\", \"object\"]\n\n        with torch.no_grad():\n            image_features = self.model.get_image_features(image_input)\n\n            detections = {}\n            for obj in objects_to_check:\n                text_input = self.processor(\n                    text=[f\"a {obj}\"],\n                    return_tensors=\"pt\",\n                    padding=True\n                ).to(self.device)\n\n                text_features = self.model.get_text_features(**text_input)\n\n                # Normalize\n                image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n                text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n\n                # Compute similarity\n                similarity = (image_features_norm @ text_features_norm.T)[0][0].item()\n\n                if similarity &gt; 0.3:\n                    detections[obj] = float(similarity)\n\n        return {\n            \"timestamp\": float(rclpy.clock.Clock().now().nanoseconds),\n            \"objects_detected\": detections\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionFeedbackNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#phase-4-integration-and-testing-week-12","title":"Phase 4: Integration and Testing (Week 12)","text":""},{"location":"physical-ai/capstone/#41-main-orchestrator","title":"4.1: Main Orchestrator","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/main_orchestrator.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport time\n\nclass OrchestratorNode(Node):\n    def __init__(self):\n        super().__init__('orchestrator')\n\n        self.state = \"idle\"\n        self.current_task = None\n\n        # Publisher for starting execution\n        self.execute_pub = self.create_publisher(String, '/action_plan', 10)\n\n        # Subscriber for feedback\n        self.feedback_sub = self.create_subscription(\n            String,\n            '/execution_feedback',\n            self.feedback_callback,\n            10\n        )\n\n        self.voice_sub = self.create_subscription(\n            String,\n            '/voice/command',\n            self.voice_callback,\n            10\n        )\n\n    def voice_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'\\n\ud83c\udfa4 Command: \"{command}\"\\n')\n        self.current_task = command\n        self.state = \"processing\"\n\n    def feedback_callback(self, msg):\n        feedback = msg.data\n        if \"\u2713\" in feedback:\n            self.get_logger().info(f'  {feedback}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OrchestratorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/capstone/#42-launch-file","title":"4.2: Launch File","text":"<pre><code># ~/ros2_ws/src/autonomous_humanoid/launch/full_system.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Humanoid simulator\n        Node(\n            package='autonomous_humanoid',\n            executable='humanoid_sim',\n            name='humanoid_sim'\n        ),\n\n        # Voice interface\n        Node(\n            package='autonomous_humanoid',\n            executable='voice_interface',\n            name='voice_interface'\n        ),\n\n        # Language planner\n        Node(\n            package='autonomous_humanoid',\n            executable='language_planner',\n            name='language_planner'\n        ),\n\n        # Action executor\n        Node(\n            package='autonomous_humanoid',\n            executable='action_executor',\n            name='action_executor'\n        ),\n\n        # Vision feedback\n        Node(\n            package='autonomous_humanoid',\n            executable='vision_feedback',\n            name='vision_feedback'\n        ),\n\n        # Orchestrator\n        Node(\n            package='autonomous_humanoid',\n            executable='orchestrator',\n            name='orchestrator'\n        ),\n    ])\n</code></pre>"},{"location":"physical-ai/capstone/#running-the-capstone","title":"Running the Capstone","text":"<pre><code>cd ~/ros2_ws\ncolcon build --packages-select autonomous_humanoid\nsource install/setup.bash\n\n# Start the entire system\nros2 launch autonomous_humanoid full_system.launch.py\n</code></pre>"},{"location":"physical-ai/capstone/#example-commands-to-try","title":"Example Commands to Try","text":"<pre><code>\"Move your arm to the right\"\n\"Pick up the red cube\"\n\"Walk to the table\"\n\"Look at the camera\"\n\"Open your hand\"\n\"Pick up the blue sphere and place it on the table\"\n</code></pre>"},{"location":"physical-ai/capstone/#evaluation-criteria","title":"Evaluation Criteria","text":"<ol> <li>Voice Recognition (20 points)</li> <li>Accurately captures voice commands</li> <li> <p>Robust to background noise</p> </li> <li> <p>Language Understanding (20 points)</p> </li> <li>Correctly parses natural language</li> <li> <p>Handles complex multi-step tasks</p> </li> <li> <p>Execution (30 points)</p> </li> <li>Actions execute smoothly in simulation</li> <li>Proper sequencing of tasks</li> <li> <p>Error handling</p> </li> <li> <p>Vision Integration (20 points)</p> </li> <li>Provides real-time feedback</li> <li> <p>Verifies task completion</p> </li> <li> <p>Code Quality (10 points)</p> </li> <li>Well-documented</li> <li>Modular design</li> <li>Error handling</li> </ol>"},{"location":"physical-ai/capstone/#bonus-features","title":"Bonus Features","text":"<ul> <li>Advanced Gait: Implement realistic bipedal walking</li> <li>Obstacle Avoidance: Navigate around objects</li> <li>Multi-Robot Coordination: Control multiple robots</li> <li>Sim-to-Real Transfer: Deploy to real robot</li> <li>Complex Tasks: Multi-stage tasks with dependencies</li> </ul> <p>Congratulations! You've completed the Physical AI &amp; Humanoid Robotics textbook. \ud83d\ude80</p> <p>Next step: Deploy your project to GitHub and submit to the hackathon!</p>"},{"location":"physical-ai/chapter-1-intro/","title":"Chapter 1: Introduction to Physical AI &amp; Embodied Intelligence","text":""},{"location":"physical-ai/chapter-1-intro/#what-is-physical-ai","title":"What is Physical AI?","text":"<p>Physical AI represents a fundamental shift in artificial intelligence\u2014moving from purely digital systems to intelligent agents that perceive, reason, and act within the physical world.</p> <p>Definition: Physical AI is the science of building artificial intelligence systems that can operate autonomously in the real world, understand physical constraints, and interact naturally with physical environments and humans.</p>"},{"location":"physical-ai/chapter-1-intro/#the-digital-physical-bridge","title":"The Digital-Physical Bridge","text":"<p>Traditional AI systems operate entirely in the digital domain: - Text processing - Image classification - Game playing - Language understanding</p> <p>These systems excel at pattern recognition but lack physical grounding. They don't understand what it means to \"pick up\" an object or \"walk through\" a doorway because they've never experienced these actions.</p> <p>Physical AI adds embodiment: The AI system operates through a robotic body, giving it: - Perception - Cameras, LiDAR, tactile sensors - Action - Motors, actuators, grippers - Feedback - Real-time sensor data from physical interactions</p>"},{"location":"physical-ai/chapter-1-intro/#why-now-the-convergence","title":"Why Now? The Convergence","text":"<p>Three technological advances make Physical AI viable today:</p>"},{"location":"physical-ai/chapter-1-intro/#1-powerful-edge-computing","title":"1. Powerful Edge Computing","text":"<ul> <li>NVIDIA Jetson provides 40+ TOPS of AI performance on a single board</li> <li>Previous decade: impossible to run advanced models on robots</li> <li>Today: Deploy large neural networks directly to robots</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#2-generative-ai-foundation-models","title":"2. Generative AI &amp; Foundation Models","text":"<ul> <li>Large language models (GPT-4, Claude) can reason about physical actions</li> <li>Vision-language models understand images and descriptions</li> <li>These models can translate natural language into robot actions</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#3-affordable-robotics-hardware","title":"3. Affordable Robotics Hardware","text":"<ul> <li>Humanoid platforms now commercially available</li> <li>Drone technology mature and accessible</li> <li>Sensing hardware (cameras, LiDAR) commodity prices</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#from-digital-brain-to-physical-body","title":"From Digital Brain to Physical Body","text":"<p>Imagine an AI assistant trained on internet data. It can discuss robotics theory perfectly. But give it control of a robot arm, and it fails\u2014because it has no model of: - Physical gravity and momentum - Joint constraints and kinematics - Object persistence and collision - The difference between simulated physics and reality</p> <p>Physical AI requires:</p> <pre><code>Digital Intelligence + Physical Embodiment + Sensorimotor Learning = Embodied Intelligence\n</code></pre>"},{"location":"physical-ai/chapter-1-intro/#the-humanoid-form-factor","title":"The Humanoid Form Factor","text":"<p>Why focus on humanoid robots? Three reasons:</p>"},{"location":"physical-ai/chapter-1-intro/#1-data-abundance","title":"1. Data Abundance","text":"<p>Humans have collected trillion hours of human movement data: - YouTube videos - Motion capture datasets - Human demonstrations</p> <p>Robots shaped like humans can learn directly from human data through behavior cloning.</p>"},{"location":"physical-ai/chapter-1-intro/#2-environmental-compatibility","title":"2. Environmental Compatibility","text":"<p>Human environments are designed for human bodies: - Doorways are human-height - Stairs have human step dimensions - Tools are sized for human hands</p> <p>A humanoid robot can use existing infrastructure without modification.</p>"},{"location":"physical-ai/chapter-1-intro/#3-social-acceptance","title":"3. Social Acceptance","text":"<p>Humans naturally cooperate with humanoid agents: - More intuitive to command - Less uncanny valley effect than non-humanoid designs - Natural interpretation of gestures and expressions</p>"},{"location":"physical-ai/chapter-1-intro/#the-robotic-stack","title":"The Robotic Stack","text":"<p>Building a functioning robot requires integration across multiple layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application Layer (Task Planning)              \u2502\n\u2502  \"Clean this room\" \u2192 Sequence of actions        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  AI/ML Layer (Perception &amp; Decision)            \u2502\n\u2502  Computer vision, LLMs, reinforcement learning  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Middleware Layer (ROS 2)                       \u2502\n\u2502  Orchestrates communication between systems     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Hardware Abstraction Layer                     \u2502\n\u2502  Motors, sensors, actuators                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This course covers each layer:</p> Layer Module Tools Application Module 4: VLA LLMs, Prompt Engineering AI/ML Module 3: Isaac NVIDIA Isaac Sim, Computer Vision Middleware Module 1: ROS 2 ROS 2, Python, rclpy Hardware Module 2: Simulation Gazebo, URDF, Physics Engines"},{"location":"physical-ai/chapter-1-intro/#key-concepts","title":"Key Concepts","text":""},{"location":"physical-ai/chapter-1-intro/#embodiment","title":"Embodiment","text":"<p>The robot has a body that exists in the physical world and experiences consequences of its actions. This grounds learning in physical reality.</p>"},{"location":"physical-ai/chapter-1-intro/#sensorimotor-learning","title":"Sensorimotor Learning","text":"<p>Learning happens through the cycle: 1. Execute action \u2192 2. Observe result \u2192 3. Adjust model \u2192 4. Repeat</p> <p>This is fundamentally different from supervised learning on static datasets.</p>"},{"location":"physical-ai/chapter-1-intro/#sim-to-real-transfer","title":"Sim-to-Real Transfer","text":"<p>Train in simulation (fast, safe, cheap) then deploy to real robots. The challenge: simulation never perfectly matches reality (the \"reality gap\").</p>"},{"location":"physical-ai/chapter-1-intro/#embodied-intelligence","title":"Embodied Intelligence","text":"<p>Intelligence emerges from: - Physical embodiment - Continuous environmental interaction - Adaptation to real-world constraints</p> <p>It's not just a \"brain in a box\" receiving static inputs.</p>"},{"location":"physical-ai/chapter-1-intro/#the-course-journey","title":"The Course Journey","text":"<pre><code>Week 1-5:  Learn ROS 2 (how robots communicate)\n    \u2193\nWeek 6-7:  Simulate physics (how robots move)\n    \u2193\nWeek 8-10: Deploy AI perception (how robots see &amp; think)\n    \u2193\nWeek 13:   Integrate language (how robots understand us)\n    \u2193\nCapstone:  Build autonomous humanoid with voice commands\n</code></pre>"},{"location":"physical-ai/chapter-1-intro/#real-world-applications","title":"Real-World Applications","text":"<p>Physical AI is already transforming industries:</p>"},{"location":"physical-ai/chapter-1-intro/#manufacturing","title":"Manufacturing","text":"<ul> <li>Humanoid robot arms performing assembly tasks</li> <li>Collaborative robots (cobots) working alongside humans</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#logistics","title":"Logistics","text":"<ul> <li>Autonomous mobile robots sorting packages</li> <li>Humanoids handling irregular items</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#healthcare","title":"Healthcare","text":"<ul> <li>Robots assisting elderly care</li> <li>Surgical robots with AI perception</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#home-services","title":"Home Services","text":"<ul> <li>Household robots cleaning and organizing</li> <li>Humanoids performing dangerous tasks</li> </ul>"},{"location":"physical-ai/chapter-1-intro/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Physical AI bridges digital intelligence with robotic embodiment \u2705 Humanoid form factor optimizes for human environments \u2705 Three technological advances enable Physical AI now \u2705 The robotic stack has multiple interdependent layers \u2705 Embodied intelligence emerges through sensorimotor interaction </p>"},{"location":"physical-ai/chapter-1-intro/#next-chapter","title":"Next Chapter","text":"<p>We'll dive into ROS 2, the middleware that enables communication between all components of a robotic system.</p> <p>Next: Chapter 2: Foundations of ROS 2</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/","title":"Chapter 2: ROS 2 Architecture &amp; Core Concepts","text":""},{"location":"physical-ai/chapter-2-ros2-architecture/#what-is-ros-2","title":"What is ROS 2?","text":"<p>ROS 2 (Robot Operating System 2) is the industry-standard middleware for robotics development. It provides:</p> <ul> <li>Inter-process communication - Nodes exchange data</li> <li>Hardware abstraction - Control different robots with same code</li> <li>Message-passing system - Loosely coupled components</li> <li>Development tools - Visualization, simulation, debugging</li> </ul> <p>Think of ROS 2 as the \"nervous system\" of a robot\u2014enabling different components (motors, sensors, processors) to communicate efficiently.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#ros-2-vs-ros-1","title":"ROS 2 vs ROS 1","text":"Feature ROS 1 ROS 2 Communication TCP/IP only DDS (more flexible) Real-time Not guaranteed Real-time capable Multi-robot Complex Native support Security None Built-in Windows Support Poor Excellent Production Ready Mature but legacy Modern standard <p>ROS 2 is the industry standard for new projects.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#the-ros-2-ecosystem","title":"The ROS 2 Ecosystem","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Application Layer                   \u2502\n\u2502  (Your robot control code)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  ROS 2 Core Libraries                       \u2502\n\u2502  \u251c\u2500 rclpy (Python client)                   \u2502\n\u2502  \u251c\u2500 rclcpp (C++ client)                     \u2502\n\u2502  \u2514\u2500 rcl (middleware abstraction)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Middleware (DDS - Data Distribution Service) \u2502\n\u2502  (Handles all node communication)           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  OS Layer (Ubuntu 22.04)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"physical-ai/chapter-2-ros2-architecture/#core-concepts","title":"Core Concepts","text":""},{"location":"physical-ai/chapter-2-ros2-architecture/#1-nodes","title":"1. Nodes","text":"<p>A node is an executable process that performs a specific task.</p> <p>Example nodes in a robot system: - Motor controller node - Sends commands to motors - Camera driver node - Reads images from camera - Perception node - Processes images for object detection - Navigation node - Plans and executes robot movement - State machine node - Coordinates overall robot behavior</p> <p>Key principle: Each node is independent and can be started/stopped without affecting others.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#2-topics","title":"2. Topics","text":"<p>A topic is a named communication channel where nodes publish or subscribe to data.</p> <p>One-to-many communication pattern:</p> <pre><code>Publisher Node                 ROS 2 Topic              Subscriber Nodes\n(Camera)         \u2500\u2500\u2500publish\u2500\u2500\u2192 /camera/image \u2190\u2500subscribe\u2500 (Vision)\n                                                        (Display)\n                                                        (Tracking)\n</code></pre> <p>Multiple subscribers can listen to the same topic.</p> <p>Example topics: - <code>/camera/image</code> - RGB images - <code>/sensor/lidar</code> - LiDAR point clouds - <code>/motor/position</code> - Joint angles - <code>/navigation/goal</code> - Target positions</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#3-services","title":"3. Services","text":"<p>A service is a request-reply communication pattern\u2014useful for one-time computations.</p> <pre><code>Client Node              ROS 2 Service               Server Node\n(Main Control)  \u2500request\u2192 /compute/inverse_kinematics \u2500reply\u2192 (IK Solver)\n</code></pre> <p>Unlike topics (fire-and-forget), services wait for a response.</p> <p>Example services: - <code>/robot/grasp_object</code> - Request: object ID | Response: success - <code>/planner/path_plan</code> - Request: start, goal | Response: path - <code>/camera/take_snapshot</code> - Request: filename | Response: status</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#4-actions","title":"4. Actions","text":"<p>An action is a service with progress feedback\u2014used for long-running tasks.</p> <pre><code>Client                  ROS 2 Action                 Server\n(Commander) \u2500goal\u2192 /robot/move_to_location \u2190\u2500feedback\u2500 (Navigator)\n                                            \u2190\u2500result\u2500\n</code></pre> <p>Actions provide: - Initial goal - Periodic feedback (progress) - Final result</p> <p>Example actions: - <code>/robot/navigate</code> - Navigate to goal with continuous feedback - <code>/arm/pick_and_place</code> - Pick object from A, place at B - <code>/autonomous_humanoid/walk</code> - Walk to destination</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#message-types","title":"Message Types","text":"<p>Messages are the data packets sent through topics and services.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#standard-message-types","title":"Standard Message Types","text":"<pre><code># Geometry messages\nfrom geometry_msgs.msg import Point, Pose, Twist\n\n# Sensor messages\nfrom sensor_msgs.msg import Image, PointCloud2, Imu\n\n# Standard messages\nfrom std_msgs.msg import String, Float64, Bool\n</code></pre>"},{"location":"physical-ai/chapter-2-ros2-architecture/#example-motor-command-message","title":"Example: Motor Command Message","text":"<pre><code>from std_msgs.msg import Float64MultiArray\n\n# Send motor velocities\nmsg = Float64MultiArray()\nmsg.data = [0.5, 0.3, -0.2, 0.1]  # 4 joint velocities\n\npublisher.publish(msg)\n</code></pre>"},{"location":"physical-ai/chapter-2-ros2-architecture/#example-sensor-data-message","title":"Example: Sensor Data Message","text":"<pre><code>from sensor_msgs.msg import Imu\n\nmsg = Imu()\nmsg.header.frame_id = \"base_link\"\nmsg.linear_acceleration.x = 9.81  # m/s\u00b2\nmsg.angular_velocity.z = 0.5      # rad/s\n\npublisher.publish(msg)\n</code></pre>"},{"location":"physical-ai/chapter-2-ros2-architecture/#coordinate-frames-transforms","title":"Coordinate Frames &amp; Transforms","text":"<p>Every robot needs a frame of reference:</p> <pre><code>world\n  \u251c\u2500 base_link (robot origin)\n  \u2502   \u251c\u2500 camera_link (camera mounted on robot)\n  \u2502   \u2514\u2500 lidar_link (LiDAR sensor)\n  \u2514\u2500 base_footprint (ground projection)\n</code></pre> <p>tf2 library handles transformations between frames:</p> <pre><code>from tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\n\n# Convert point from camera frame to world frame\ntf_buffer = Buffer()\nlistener = TransformListener(tf_buffer)\n\n# Get transform from camera to world\ntransform = tf_buffer.lookup_transform(\"world\", \"camera_link\", Time())\n</code></pre>"},{"location":"physical-ai/chapter-2-ros2-architecture/#qos-quality-of-service","title":"QoS (Quality of Service)","text":"<p>ROS 2 allows fine-tuning how messages are delivered:</p> <pre><code>from rclpy.qos import QoSProfile, ReliabilityPolicy\n\n# Reliable, in-order delivery (slower)\nqos_profile = QoSProfile(\n    reliability=ReliabilityPolicy.RELIABLE,\n    history_depth=10\n)\n\n# Best-effort delivery (faster, for video streams)\nqos_profile = QoSProfile(\n    reliability=ReliabilityPolicy.BEST_EFFORT,\n    history_depth=1\n)\n\npublisher = node.create_publisher(String, \"topic\", qos_profile)\n</code></pre> <p>Use RELIABLE for critical commands, BEST_EFFORT for sensor streams.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#ros-2-distributions","title":"ROS 2 Distributions","text":"Distribution Release Year Support Humble 2022 LTS (until 2027) Iron 2023 Standard Jazzy 2024 Standard <p>We recommend ROS 2 Humble for stability and long-term support.</p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 ROS 2 provides middleware for robot communication \u2705 Nodes are independent processes with specific tasks \u2705 Topics enable one-to-many publish-subscribe communication \u2705 Services enable request-reply communication \u2705 Actions are for long-running tasks with feedback \u2705 Coordinate frames track spatial relationships </p>"},{"location":"physical-ai/chapter-2-ros2-architecture/#next-chapter","title":"Next Chapter","text":"<p>We'll implement these concepts in Python by building our first ROS 2 nodes.</p> <p>Next: Chapter 3: Your First ROS 2 Node</p>"},{"location":"physical-ai/chapter-3-first-node/","title":"Chapter 3: Your First ROS 2 Node","text":""},{"location":"physical-ai/chapter-3-first-node/#setting-up-your-environment","title":"Setting Up Your Environment","text":""},{"location":"physical-ai/chapter-3-first-node/#install-ros-2-humble","title":"Install ROS 2 Humble","text":"<pre><code># Add ROS 2 repository\nsudo apt-get update\nsudo curl -sSL https://raw.githubusercontent.com/ros/ros.key | sudo apt-key add -\nsudo apt-get install -y ros-humble-desktop-full\n\n# Add ROS 2 to bashrc\necho \"source /opt/ros/humble/setup.bash\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#create-a-ros-2-workspace","title":"Create a ROS 2 Workspace","text":"<pre><code># Create workspace\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws\n\n# Build workspace\ncolcon build\n\n# Source setup\nsource install/setup.bash\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#creating-your-first-package","title":"Creating Your First Package","text":"<p>A package is a collection of ROS 2 code and resources.</p> <pre><code>cd ~/ros2_ws/src\n\n# Create package with Python\nros2 pkg create my_first_package --build-type ament_python --dependencies rclpy std_msgs\n\n# Directory structure created:\n# my_first_package/\n# \u251c\u2500\u2500 package.xml\n# \u251c\u2500\u2500 setup.py\n# \u251c\u2500\u2500 my_first_package/\n# \u2502   \u2514\u2500\u2500 __init__.py\n# \u2514\u2500\u2500 resource/\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#building-a-simple-publisher","title":"Building a Simple Publisher","text":"<p>A publisher sends data to a topic.</p>"},{"location":"physical-ai/chapter-3-first-node/#example-motor-velocity-publisher","title":"Example: Motor Velocity Publisher","text":"<pre><code># ~/ros2_ws/src/my_first_package/my_first_package/motor_publisher.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float64MultiArray\nimport time\n\nclass MotorPublisher(Node):\n    def __init__(self):\n        super().__init__('motor_publisher')\n\n        # Create publisher\n        # Topic name: /motor/velocity\n        # Message type: Float64MultiArray\n        # Queue size: 10 messages\n        self.publisher = self.create_publisher(\n            Float64MultiArray,\n            '/motor/velocity',\n            10\n        )\n\n        # Create timer - publish every 100ms (10 Hz)\n        self.timer = self.create_timer(0.1, self.timer_callback)\n        self.counter = 0\n\n    def timer_callback(self):\n        msg = Float64MultiArray()\n        msg.data = [\n            0.5 * (self.counter % 20) / 10,  # Joint 1: 0 to 1\n            0.3,                               # Joint 2: constant\n            -0.2,                              # Joint 3: constant\n            0.1 * (-1)**(self.counter % 2)   # Joint 4: oscillating\n        ]\n\n        self.publisher.publish(msg)\n        self.get_logger().info(f'Publishing motor commands: {msg.data}')\n        self.counter += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MotorPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#building-a-simple-subscriber","title":"Building a Simple Subscriber","text":"<p>A subscriber listens to messages on a topic.</p>"},{"location":"physical-ai/chapter-3-first-node/#example-motor-feedback-subscriber","title":"Example: Motor Feedback Subscriber","text":"<pre><code># ~/ros2_ws/src/my_first_package/my_first_package/motor_subscriber.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float64MultiArray\n\nclass MotorSubscriber(Node):\n    def __init__(self):\n        super().__init__('motor_subscriber')\n\n        # Create subscriber\n        # Topic name: /motor/feedback\n        # Message type: Float64MultiArray\n        self.subscription = self.create_subscription(\n            Float64MultiArray,\n            '/motor/feedback',\n            self.feedback_callback,\n            10\n        )\n\n    def feedback_callback(self, msg):\n        # Called whenever message arrives on topic\n        self.get_logger().info(f'Received motor feedback: {msg.data}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MotorSubscriber()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#running-publisher-and-subscriber","title":"Running Publisher and Subscriber","text":""},{"location":"physical-ai/chapter-3-first-node/#terminal-1-start-publisher","title":"Terminal 1: Start publisher","text":"<pre><code>cd ~/ros2_ws\nsource install/setup.bash\nros2 run my_first_package motor_publisher\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#terminal-2-start-subscriber","title":"Terminal 2: Start subscriber","text":"<pre><code>cd ~/ros2_ws\nsource install/setup.bash\nros2 run my_first_package motor_subscriber\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#creating-a-service-request-reply","title":"Creating a Service (Request-Reply)","text":""},{"location":"physical-ai/chapter-3-first-node/#service-definition","title":"Service Definition","text":"<pre><code># ~/ros2_ws/src/my_first_package/srv/ComputeIK.srv\n\n# Request\ngeometry_msgs/Point end_effector_position\n---\n# Response\nfloat64[] joint_angles\nbool success\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#service-server","title":"Service Server","text":"<pre><code># ~/ros2_ws/src/my_first_package/my_first_package/ik_server.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\n# Need to create custom message - simulating here\nclass IKResponse:\n    def __init__(self):\n        self.joint_angles = []\n        self.success = False\n\nclass IKServer(Node):\n    def __init__(self):\n        super().__init__('ik_server')\n\n        # Create service\n        self.service = self.create_service(\n            self.compute_ik_srv,\n            '/robot/compute_ik',\n            self.compute_ik_callback\n        )\n        self.get_logger().info('IK Service ready')\n\n    def compute_ik_callback(self, request, response):\n        # Simple inverse kinematics (mock implementation)\n        x = request.end_effector_position.x\n        y = request.end_effector_position.y\n        z = request.end_effector_position.z\n\n        # Mock IK calculation\n        r = np.sqrt(x**2 + y**2)\n        joint_angles = [\n            np.arctan2(y, x),           # Base rotation\n            np.arctan2(z, r),           # Shoulder\n            np.arctan2(z, r) * 0.5,    # Elbow\n            0.0                         # Wrist\n        ]\n\n        response.joint_angles = joint_angles\n        response.success = True\n\n        self.get_logger().info(f'Computed IK: {joint_angles}')\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IKServer()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#service-client","title":"Service Client","text":"<pre><code># ~/ros2_ws/src/my_first_package/my_first_package/ik_client.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point\nimport asyncio\n\nclass IKClient(Node):\n    def __init__(self):\n        super().__init__('ik_client')\n\n    async def request_ik(self, x, y, z):\n        # Note: Simplified version - full implementation needs custom message\n        self.get_logger().info(f'Requesting IK for position: ({x}, {y}, {z})')\n\n        # In real implementation:\n        # future = client.call_async(request)\n        # await future\n        # result = future.result()\n\nasync def main(args=None):\n    rclpy.init(args=args)\n    node = IKClient()\n\n    # Request IK solution\n    await node.request_ik(0.3, 0.2, 0.5)\n\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#debugging-ros-2-applications","title":"Debugging ROS 2 Applications","text":""},{"location":"physical-ai/chapter-3-first-node/#list-active-nodes","title":"List Active Nodes","text":"<pre><code>ros2 node list\n# Output:\n# /motor_publisher\n# /motor_subscriber\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#inspect-topics","title":"Inspect Topics","text":"<pre><code># List all topics\nros2 topic list\n\n# Show topic information\nros2 topic info /motor/velocity\n\n# Echo topic data\nros2 topic echo /motor/velocity\n\n# Publish to topic manually\nros2 topic pub /motor/velocity std_msgs/msg/Float64MultiArray \"{data: [0.5, 0.3, -0.2, 0.1]}\"\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#inspect-services","title":"Inspect Services","text":"<pre><code># List all services\nros2 service list\n\n# Show service information\nros2 service info /robot/compute_ik\n\n# Call service\nros2 service call /robot/compute_ik ...\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#monitor-system","title":"Monitor System","text":"<pre><code># Real-time graph of node/topic connections\nrqt_graph\n\n# Console viewer\nrqt_console\n\n# Node dashboard\nrqt\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#ros-2-launch-files","title":"ROS 2 Launch Files","text":"<p>For complex systems, use launch files to start multiple nodes:</p> <pre><code># ~/ros2_ws/src/my_first_package/launch/motor_system.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='my_first_package',\n            executable='motor_publisher',\n            name='motor_pub'\n        ),\n        Node(\n            package='my_first_package',\n            executable='motor_subscriber',\n            name='motor_sub'\n        ),\n        Node(\n            package='my_first_package',\n            executable='ik_server',\n            name='ik_service'\n        ),\n    ])\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#run-launch-file","title":"Run launch file","text":"<pre><code>ros2 launch my_first_package motor_system.launch.py\n</code></pre>"},{"location":"physical-ai/chapter-3-first-node/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Nodes are executable processes with specific tasks \u2705 Publishers send data continuously to topics \u2705 Subscribers listen to topics and react to messages \u2705 Services provide request-reply communication \u2705 Launch files simplify starting complex systems \u2705 ROS 2 tools help debug and visualize systems </p>"},{"location":"physical-ai/chapter-3-first-node/#exercises","title":"Exercises","text":"<ol> <li>Modify the publisher to read motor velocities from a file</li> <li>Create a subscriber that saves received data to a CSV file</li> <li>Build a service that computes forward kinematics</li> <li>Write a launch file that starts 3 different subscriber nodes</li> </ol> <p>Next: Chapter 4: URDF - Describing Your Robot</p>"},{"location":"physical-ai/chapter-4-urdf/","title":"Chapter 4: URDF - Robot Description Format","text":""},{"location":"physical-ai/chapter-4-urdf/#what-is-urdf","title":"What is URDF?","text":"<p>URDF (Unified Robot Description Format) is an XML language that describes: - Robot structure (links and joints) - Physical properties (mass, dimensions, friction) - Visual appearance (for simulation) - Collision properties (for physics simulation) - Sensor placements</p> <p>Think of URDF as the \"blueprint\" of your robot.</p>"},{"location":"physical-ai/chapter-4-urdf/#why-urdf-matters","title":"Why URDF Matters","text":"<pre><code>URDF File \u2192 ROS 2 Parser \u2192 Robotic System Understanding\n              \u2193\n         Gazebo uses for simulation\n         NVIDIA Isaac uses for digital twin\n         Motion planners use for kinematics\n         Visualization tools use for display\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#basic-urdf-structure","title":"Basic URDF Structure","text":""},{"location":"physical-ai/chapter-4-urdf/#minimal-example-single-link","title":"Minimal Example: Single Link","text":"<pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;robot name=\"simple_robot\"&gt;\n\n  &lt;link name=\"base_link\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.1 0.1 0.1\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"&gt;\n        &lt;color rgba=\"0 0 1 1\"/&gt;\n      &lt;/material&gt;\n    &lt;/visual&gt;\n\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.1 0.1 0.1\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n\n    &lt;inertial&gt;\n      &lt;mass value=\"1.0\"/&gt;\n      &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#two-link-system-with-joint","title":"Two-Link System with Joint","text":"<pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;robot name=\"two_link_robot\"&gt;\n\n  &lt;!-- Base link --&gt;\n  &lt;link name=\"base_link\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.05\" length=\"0.1\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.05\" length=\"0.1\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.0\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.01\" iyz=\"0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Arm link --&gt;\n  &lt;link name=\"arm_link\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.05 0.05 0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.15\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.05 0.05 0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin rpy=\"0 0 0\" xyz=\"0 0 0.15\"/&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"0.5\"/&gt;\n      &lt;origin xyz=\"0 0 0.15\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.01\" iyz=\"0\" izz=\"0.001\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Joint connecting base to arm --&gt;\n  &lt;joint name=\"shoulder_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"base_link\"/&gt;\n    &lt;child link=\"arm_link\"/&gt;\n    &lt;origin xyz=\"0 0 0.05\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 0 1\"/&gt;\n    &lt;limit lower=\"0\" upper=\"3.14159\" effort=\"10\" velocity=\"0.5\"/&gt;\n  &lt;/joint&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#link-properties","title":"Link Properties","text":""},{"location":"physical-ai/chapter-4-urdf/#visual-geometry","title":"Visual Geometry","text":"<p>Shapes available in URDF:</p> <pre><code>&lt;!-- Box --&gt;\n&lt;box size=\"length width height\"/&gt;\n\n&lt;!-- Cylinder --&gt;\n&lt;cylinder radius=\"0.1\" length=\"0.5\"/&gt;\n\n&lt;!-- Sphere --&gt;\n&lt;sphere radius=\"0.1\"/&gt;\n\n&lt;!-- Mesh (from 3D model) --&gt;\n&lt;mesh filename=\"package://robot_description/meshes/arm.stl\" scale=\"1 1 1\"/&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#collision-geometry","title":"Collision Geometry","text":"<p>Collision shapes should be simpler than visual shapes for performance:</p> <pre><code>&lt;collision&gt;\n  &lt;!-- Use simplified shapes even if visual uses meshes --&gt;\n  &lt;geometry&gt;\n    &lt;box size=\"0.1 0.1 0.5\"/&gt;\n  &lt;/geometry&gt;\n&lt;/collision&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#inertial-properties","title":"Inertial Properties","text":"<p>For physics simulation, specify mass and inertia tensor:</p> <pre><code>&lt;inertial&gt;\n  &lt;mass value=\"2.5\"/&gt;  &lt;!-- kg --&gt;\n  &lt;origin xyz=\"0 0 0.1\"/&gt;  &lt;!-- Center of mass offset --&gt;\n\n  &lt;!-- Inertia tensor (kg\u22c5m\u00b2) --&gt;\n  &lt;inertia \n    ixx=\"0.02\"  iyy=\"0.02\"  izz=\"0.01\"\n    ixy=\"0\"     ixz=\"0\"     iyz=\"0\"/&gt;\n&lt;/inertial&gt;\n</code></pre> <p>Inertia calculation for simple shapes:</p> Shape Formula Box Ixx = m/12 \u00d7 (y\u00b2 + z\u00b2) Cylinder Ixx = m \u00d7 r\u00b2/2 Sphere Ixx = 2/5 \u00d7 m \u00d7 r\u00b2"},{"location":"physical-ai/chapter-4-urdf/#joint-types","title":"Joint Types","text":""},{"location":"physical-ai/chapter-4-urdf/#revolute-rotating-joint","title":"Revolute (Rotating) Joint","text":"<pre><code>&lt;joint name=\"shoulder\" type=\"revolute\"&gt;\n  &lt;parent link=\"base\"/&gt;\n  &lt;child link=\"upper_arm\"/&gt;\n  &lt;origin xyz=\"0 0 0.5\"/&gt;\n  &lt;axis xyz=\"0 0 1\"/&gt;  &lt;!-- Rotation around Z-axis --&gt;\n  &lt;limit lower=\"-1.57\" upper=\"1.57\" \n          effort=\"50\" velocity=\"1.0\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#prismatic-sliding-joint","title":"Prismatic (Sliding) Joint","text":"<pre><code>&lt;joint name=\"slide\" type=\"prismatic\"&gt;\n  &lt;parent link=\"base\"/&gt;\n  &lt;child link=\"carriage\"/&gt;\n  &lt;origin xyz=\"0 0 0\"/&gt;\n  &lt;axis xyz=\"1 0 0\"/&gt;  &lt;!-- Linear motion along X-axis --&gt;\n  &lt;limit lower=\"0\" upper=\"1.0\"\n          effort=\"100\" velocity=\"0.5\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#fixed-joint","title":"Fixed Joint","text":"<pre><code>&lt;joint name=\"camera_mount\" type=\"fixed\"&gt;\n  &lt;parent link=\"head\"/&gt;\n  &lt;child link=\"camera\"/&gt;\n  &lt;origin xyz=\"0.1 0 0.05\" rpy=\"0 0.3 0\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#other-joints","title":"Other Joints","text":"<ul> <li>Continuous: Rotates indefinitely (no limits)</li> <li>Planar: 2D movement in a plane</li> <li>Floating: 6 DOF (used for world root)</li> </ul>"},{"location":"physical-ai/chapter-4-urdf/#complete-humanoid-example","title":"Complete Humanoid Example","text":"<pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;robot name=\"simple_humanoid\"&gt;\n\n  &lt;!-- Base Link (Torso) --&gt;\n  &lt;link name=\"torso\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.2 0.5\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"torso_color\"&gt;\n        &lt;color rgba=\"0.8 0.8 0.8 1\"/&gt;\n      &lt;/material&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.2 0.5\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"10.0\"/&gt;\n      &lt;inertia ixx=\"0.5\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.5\" iyz=\"0\" izz=\"0.3\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Left Upper Arm --&gt;\n  &lt;link name=\"l_upper_arm\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.04\" length=\"0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.15\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.04\" length=\"0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.15\"/&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.01\" iyz=\"0\" izz=\"0.001\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Right Upper Arm --&gt;\n  &lt;link name=\"r_upper_arm\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.04\" length=\"0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.15\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder radius=\"0.04\" length=\"0.3\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.15\"/&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.01\" iyz=\"0\" izz=\"0.001\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Left Shoulder Joint --&gt;\n  &lt;joint name=\"l_shoulder\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"l_upper_arm\"/&gt;\n    &lt;origin xyz=\"0.15 0.1 0.2\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"50\" velocity=\"1.0\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Right Shoulder Joint --&gt;\n  &lt;joint name=\"r_shoulder\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"r_upper_arm\"/&gt;\n    &lt;origin xyz=\"-0.15 0.1 0.2\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"50\" velocity=\"1.0\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Left Leg --&gt;\n  &lt;link name=\"l_upper_leg\"&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.1 0.1 0.4\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.2\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.1 0.1 0.4\"/&gt;\n      &lt;/geometry&gt;\n      &lt;origin xyz=\"0 0 -0.2\"/&gt;\n    &lt;/collision&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"3.0\"/&gt;\n      &lt;inertia ixx=\"0.05\" ixy=\"0\" ixz=\"0\" \n               iyy=\"0.05\" iyz=\"0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Left Hip Joint --&gt;\n  &lt;joint name=\"l_hip\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"l_upper_leg\"/&gt;\n    &lt;origin xyz=\"0.1 0 -0.25\"/&gt;\n    &lt;axis xyz=\"1 0 0\"/&gt;\n    &lt;limit lower=\"-0.785\" upper=\"0.785\" effort=\"100\" velocity=\"1.0\"/&gt;\n  &lt;/joint&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#using-urdf-in-ros-2","title":"Using URDF in ROS 2","text":""},{"location":"physical-ai/chapter-4-urdf/#load-urdf-in-launch-file","title":"Load URDF in Launch File","text":"<pre><code># launch/display.launch.py\n\nimport os\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch.substitutions import Command\n\ndef generate_launch_description():\n    pkg_share = FindPackageShare('robot_description').find('robot_description')\n    urdf_file = os.path.join(pkg_share, 'urdf', 'robot.urdf')\n\n    # Load URDF\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        # Robot state publisher (broadcasts transforms)\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{\n                'robot_description': robot_desc,\n                'publish_frequency': 10.0\n            }]\n        ),\n\n        # RViz for visualization\n        Node(\n            package='rviz2',\n            executable='rviz2'\n        )\n    ])\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#verify-urdf","title":"Verify URDF","text":"<pre><code># Check for errors\ncheck_urdf robot.urdf\n\n# Convert to PDF graph\nurdf_to_graphviz robot.urdf &gt; robot.pdf\n\n# Test in RViz\nros2 launch my_package display.launch.py\n</code></pre>"},{"location":"physical-ai/chapter-4-urdf/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 URDF describes robot structure, properties, and appearance \u2705 Links represent rigid bodies, joints connect them \u2705 Collision shapes should be simpler than visual shapes \u2705 Inertial properties are critical for physics simulation \u2705 Different joint types enable different motions \u2705 URDF integrates with Gazebo, Isaac, and planning tools </p>"},{"location":"physical-ai/chapter-4-urdf/#exercises","title":"Exercises","text":"<ol> <li>Create a URDF for a 2-DOF robot arm</li> <li>Add visual meshes from STL files</li> <li>Verify inertial properties using check_urdf</li> <li>Visualize the robot in RViz with proper coordinate frames</li> </ol> <p>Next: Chapter 5: Physics Simulation with Gazebo</p>"},{"location":"physical-ai/chapter-5-gazebo/","title":"Chapter 5: Physics Simulation with Gazebo","text":""},{"location":"physical-ai/chapter-5-gazebo/#introduction-to-gazebo","title":"Introduction to Gazebo","text":"<p>Gazebo is the industry-standard physics simulator for robotics. It provides: - Accurate rigid-body physics simulation - Multi-robot simulation support - Sensor simulation (cameras, LiDAR, IMUs) - Plugin architecture for extending functionality - Integration with ROS 2</p>"},{"location":"physical-ai/chapter-5-gazebo/#why-simulate","title":"Why Simulate?","text":"<pre><code>Train in Simulation (Fast, Safe, Cheap)\n         \u2193\n         \u2193 (Sim-to-Real Transfer)\n         \u2193\nDeploy to Real Robots (Slow, Risky, Expensive)\n</code></pre> <p>Benefits of simulation: - 1000x faster than real-time (train in seconds) - No risk of hardware damage - Reproducible experiments - Easy parameter tuning - Cheap and accessible</p>"},{"location":"physical-ai/chapter-5-gazebo/#setting-up-gazebo","title":"Setting Up Gazebo","text":""},{"location":"physical-ai/chapter-5-gazebo/#installation","title":"Installation","text":"<pre><code># Install Gazebo and ROS 2 integration\nsudo apt-get install -y gazebo-harmonic\nsudo apt-get install -y ros-humble-gazebo-ros2-control\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#launch-gazebo","title":"Launch Gazebo","text":"<pre><code># Start Gazebo with empty world\ngazebo\n\n# Or via ROS 2\nros2 launch gazebo_ros gazebo.launch.py\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#gazebo-worlds","title":"Gazebo Worlds","text":"<p>A world file (<code>.world</code>) defines: - Physics engine settings - Environmental properties (gravity, friction) - Objects in the scene - Lighting</p>"},{"location":"physical-ai/chapter-5-gazebo/#basic-world-file","title":"Basic World File","text":"<pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;sdf version=\"1.10\"&gt;\n  &lt;world name=\"robot_world\"&gt;\n\n    &lt;!-- Physics engine --&gt;\n    &lt;physics name=\"default_physics\" type=\"ode\"&gt;\n      &lt;max_step_size&gt;0.001&lt;/max_step_size&gt;\n      &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt;\n      &lt;gravity&gt;0 0 -9.81&lt;/gravity&gt;\n    &lt;/physics&gt;\n\n    &lt;!-- Sun for lighting --&gt;\n    &lt;light name=\"sun\" type=\"directional\"&gt;\n      &lt;cast_shadows&gt;true&lt;/cast_shadows&gt;\n      &lt;pose&gt;0 0 10 0 0 0&lt;/pose&gt;\n      &lt;diffuse&gt;0.8 0.8 0.8 1&lt;/diffuse&gt;\n      &lt;specular&gt;0.2 0.2 0.2 1&lt;/specular&gt;\n      &lt;direction&gt;-0.5 0.1 -0.9&lt;/direction&gt;\n    &lt;/light&gt;\n\n    &lt;!-- Ground plane --&gt;\n    &lt;model name=\"ground_plane\"&gt;\n      &lt;static&gt;true&lt;/static&gt;\n      &lt;link name=\"link\"&gt;\n        &lt;collision name=\"collision\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n        &lt;/collision&gt;\n        &lt;visual name=\"visual\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n          &lt;material&gt;\n            &lt;ambient&gt;0.8 0.8 0.8 1&lt;/ambient&gt;\n          &lt;/material&gt;\n        &lt;/visual&gt;\n      &lt;/link&gt;\n    &lt;/model&gt;\n\n    &lt;!-- A box obstacle --&gt;\n    &lt;model name=\"obstacle\"&gt;\n      &lt;pose&gt;5 5 0.25 0 0 0&lt;/pose&gt;\n      &lt;link name=\"link\"&gt;\n        &lt;collision name=\"collision\"&gt;\n          &lt;geometry&gt;\n            &lt;box&gt;\n              &lt;size&gt;0.5 0.5 0.5&lt;/size&gt;\n            &lt;/box&gt;\n          &lt;/geometry&gt;\n        &lt;/collision&gt;\n        &lt;visual name=\"visual\"&gt;\n          &lt;geometry&gt;\n            &lt;box&gt;\n              &lt;size&gt;0.5 0.5 0.5&lt;/size&gt;\n            &lt;/box&gt;\n          &lt;/geometry&gt;\n          &lt;material&gt;\n            &lt;ambient&gt;0.5 0.2 0.2 1&lt;/ambient&gt;\n          &lt;/material&gt;\n        &lt;/visual&gt;\n      &lt;/link&gt;\n    &lt;/model&gt;\n\n  &lt;/world&gt;\n&lt;/sdf&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#sensor-simulation","title":"Sensor Simulation","text":""},{"location":"physical-ai/chapter-5-gazebo/#camera-simulation","title":"Camera Simulation","text":"<pre><code>&lt;link name=\"camera_link\"&gt;\n  &lt;sensor name=\"camera\" type=\"camera\"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;update_rate&gt;30&lt;/update_rate&gt;\n\n    &lt;camera&gt;\n      &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt;\n      &lt;image&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n        &lt;format&gt;R8G8B8&lt;/format&gt;\n      &lt;/image&gt;\n      &lt;clip&gt;\n        &lt;near&gt;0.01&lt;/near&gt;\n        &lt;far&gt;100&lt;/far&gt;\n      &lt;/clip&gt;\n    &lt;/camera&gt;\n\n    &lt;plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;camera&lt;/namespace&gt;\n        &lt;remapping&gt;~/image_raw:=image_raw&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;camera_name&gt;camera&lt;/camera_name&gt;\n      &lt;frame_name&gt;camera_optical_frame&lt;/frame_name&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/link&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#lidar-simulation","title":"LiDAR Simulation","text":"<pre><code>&lt;link name=\"lidar_link\"&gt;\n  &lt;sensor name=\"lidar\" type=\"ray\"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;visualize&gt;true&lt;/visualize&gt;\n    &lt;update_rate&gt;10&lt;/update_rate&gt;\n\n    &lt;ray&gt;\n      &lt;scan&gt;\n        &lt;horizontal&gt;\n          &lt;samples&gt;360&lt;/samples&gt;\n          &lt;resolution&gt;1.0&lt;/resolution&gt;\n          &lt;min_angle&gt;-3.14159&lt;/min_angle&gt;\n          &lt;max_angle&gt;3.14159&lt;/max_angle&gt;\n        &lt;/horizontal&gt;\n      &lt;/scan&gt;\n      &lt;range&gt;\n        &lt;min&gt;0.1&lt;/min&gt;\n        &lt;max&gt;30&lt;/max&gt;\n        &lt;resolution&gt;0.1&lt;/resolution&gt;\n      &lt;/range&gt;\n    &lt;/ray&gt;\n\n    &lt;plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\"&gt;\n      &lt;ros&gt;\n        &lt;remapping&gt;~/out:=scan&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;output_type&gt;sensor_msgs/LaserScan&lt;/output_type&gt;\n      &lt;frame_name&gt;lidar_frame&lt;/frame_name&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/link&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#imu-simulation","title":"IMU Simulation","text":"<pre><code>&lt;link name=\"imu_link\"&gt;\n  &lt;sensor name=\"imu\" type=\"imu\"&gt;\n    &lt;always_on&gt;true&lt;/always_on&gt;\n    &lt;update_rate&gt;100&lt;/update_rate&gt;\n\n    &lt;imu&gt;\n      &lt;angular_velocity&gt;\n        &lt;x&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.01&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/x&gt;\n        &lt;y&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.01&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/y&gt;\n        &lt;z&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.01&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/z&gt;\n      &lt;/angular_velocity&gt;\n      &lt;linear_acceleration&gt;\n        &lt;x&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.1&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/x&gt;\n        &lt;y&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.1&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/y&gt;\n        &lt;z&gt;\n          &lt;noise type=\"gaussian\"&gt;\n            &lt;mean&gt;0&lt;/mean&gt;\n            &lt;stddev&gt;0.1&lt;/stddev&gt;\n          &lt;/noise&gt;\n        &lt;/z&gt;\n      &lt;/linear_acceleration&gt;\n    &lt;/imu&gt;\n\n    &lt;plugin name=\"imu_controller\" filename=\"libgazebo_ros_imu_sensor.so\"&gt;\n      &lt;ros&gt;\n        &lt;namespace&gt;imu&lt;/namespace&gt;\n        &lt;remapping&gt;~/out:=data&lt;/remapping&gt;\n      &lt;/ros&gt;\n      &lt;initial_orientation_as_reference&gt;false&lt;/initial_orientation_as_reference&gt;\n    &lt;/plugin&gt;\n  &lt;/sensor&gt;\n&lt;/link&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#actuator-control","title":"Actuator Control","text":""},{"location":"physical-ai/chapter-5-gazebo/#motor-controller-plugin","title":"Motor Controller Plugin","text":"<pre><code>&lt;plugin name=\"gazebo_ros2_control\" filename=\"libgazebo_ros2_control.so\"&gt;\n  &lt;ros&gt;\n    &lt;namespace&gt;/&lt;/namespace&gt;\n  &lt;/ros&gt;\n  &lt;robot_param&gt;robot_description&lt;/robot_param&gt;\n  &lt;robot_sim_type&gt;gazebo_ros2_control/GazeboSystem&lt;/robot_sim_type&gt;\n&lt;/plugin&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#running-gazebo-with-ros-2","title":"Running Gazebo with ROS 2","text":""},{"location":"physical-ai/chapter-5-gazebo/#launch-file","title":"Launch File","text":"<pre><code># launch/gazebo_sim.launch.py\n\nimport os\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch.substitutions import Command\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\n\ndef generate_launch_description():\n    pkg_share = FindPackageShare('robot_description').find('robot_description')\n\n    urdf_file = os.path.join(pkg_share, 'urdf', 'robot.urdf')\n    world_file = os.path.join(pkg_share, 'worlds', 'robot_world.world')\n\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        # Start Gazebo\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                FindPackageShare('gazebo_ros'), '/launch/gazebo.launch.py'\n            ]),\n            launch_arguments={'world': world_file}.items(),\n        ),\n\n        # Robot state publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{\n                'robot_description': robot_desc\n            }]\n        ),\n\n        # Spawn robot in Gazebo\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=['-topic', 'robot_description', '-entity', 'robot'],\n            output='screen'\n        ),\n    ])\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#run-simulation","title":"Run Simulation","text":"<pre><code>cd ~/ros2_ws\nsource install/setup.bash\nros2 launch robot_description gazebo_sim.launch.py\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#controlling-the-simulated-robot","title":"Controlling the Simulated Robot","text":""},{"location":"physical-ai/chapter-5-gazebo/#send-motor-commands","title":"Send Motor Commands","text":"<pre><code># control_robot.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass RobotController(Node):\n    def __init__(self):\n        super().__init__('robot_controller')\n\n        # Publisher for motor commands\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Timer for control loop\n        self.timer = self.create_timer(0.1, self.control_callback)\n        self.time_counter = 0\n\n    def control_callback(self):\n        msg = Twist()\n\n        # Make robot move in a circle\n        msg.linear.x = 0.3   # Forward velocity\n        msg.angular.z = 0.5  # Angular velocity\n\n        self.cmd_vel_pub.publish(msg)\n        self.time_counter += 1\n\n        if self.time_counter % 10 == 0:\n            self.get_logger().info('Sending motor commands...')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RobotController()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#performance-tuning","title":"Performance Tuning","text":""},{"location":"physical-ai/chapter-5-gazebo/#physics-engine-optimization","title":"Physics Engine Optimization","text":"<pre><code>&lt;physics name=\"default_physics\" type=\"ode\"&gt;\n  &lt;!-- Smaller timestep = more accurate but slower --&gt;\n  &lt;max_step_size&gt;0.001&lt;/max_step_size&gt;\n\n  &lt;!-- Real time factor: &gt;1 = faster than real time --&gt;\n  &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt;\n\n  &lt;!-- Iterations for solver accuracy --&gt;\n  &lt;ode&gt;\n    &lt;solver&gt;\n      &lt;type&gt;quick&lt;/type&gt;\n      &lt;iters&gt;50&lt;/iters&gt;\n      &lt;precon_iters&gt;0&lt;/precon_iters&gt;\n      &lt;sor&gt;1.3&lt;/sor&gt;\n      &lt;use_dynamic_moi_rescaling&gt;false&lt;/use_dynamic_moi_rescaling&gt;\n    &lt;/solver&gt;\n  &lt;/ode&gt;\n&lt;/physics&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#multi-threading","title":"Multi-threading","text":"<p>Enable multi-threaded physics:</p> <pre><code>&lt;physics name=\"default_physics\" type=\"ode\"&gt;\n  &lt;ode&gt;\n    &lt;solver&gt;\n      &lt;parallel_method&gt;OpenMP&lt;/parallel_method&gt;\n      &lt;parallel_threads&gt;4&lt;/parallel_threads&gt;\n    &lt;/solver&gt;\n  &lt;/ode&gt;\n&lt;/physics&gt;\n</code></pre>"},{"location":"physical-ai/chapter-5-gazebo/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Gazebo provides physics simulation for robotics \u2705 Worlds define environments with physics and objects \u2705 Sensors (camera, LiDAR, IMU) can be simulated \u2705 Control robots through ROS 2 topics \u2705 Simulation enables safe and fast prototyping \u2705 Tuning physics parameters balances accuracy vs speed </p>"},{"location":"physical-ai/chapter-5-gazebo/#exercises","title":"Exercises","text":"<ol> <li>Create a complex world with obstacles and multiple objects</li> <li>Add a simulated camera and process its output</li> <li>Implement LiDAR-based obstacle detection</li> <li>Build a simple autonomous navigation node</li> </ol> <p>Next: Chapter 6: NVIDIA Isaac - Advanced Perception</p>"},{"location":"physical-ai/chapter-6-isaac/","title":"Chapter 6: NVIDIA Isaac - The AI Robot Brain","text":""},{"location":"physical-ai/chapter-6-isaac/#what-is-nvidia-isaac","title":"What is NVIDIA Isaac?","text":"<p>NVIDIA Isaac is a comprehensive platform for developing robotics applications with AI acceleration. It consists of:</p> <ul> <li>Isaac Sim - Photorealistic physics simulation and synthetic data generation</li> <li>Isaac ROS - Hardware-accelerated computer vision and robotics algorithms</li> <li>Isaac SDK - High-performance AI runtime for robots</li> </ul>"},{"location":"physical-ai/chapter-6-isaac/#why-isaac-over-gazebo","title":"Why Isaac Over Gazebo?","text":"Feature Gazebo Isaac Sim Physics Accurate but basic Photorealistic Graphics Simplified Real-time ray tracing AI Perception None Integrated Synthetic Data Limited Full pipeline Deployment ROS only Edge devices Cost Free Free (cloud) <p>Isaac Sim is designed for production AI robotics.</p>"},{"location":"physical-ai/chapter-6-isaac/#isaac-sim-setup","title":"Isaac Sim Setup","text":""},{"location":"physical-ai/chapter-6-isaac/#system-requirements","title":"System Requirements","text":"<ul> <li>NVIDIA RTX GPU (RTX 4070 Ti or higher recommended)</li> <li>Ubuntu 22.04 LTS</li> <li>64GB RAM (minimum 32GB)</li> <li>NVIDIA driver 535+</li> </ul>"},{"location":"physical-ai/chapter-6-isaac/#installation","title":"Installation","text":"<pre><code># Install NVIDIA Omniverse\n# Download from: https://www.nvidia.com/en-us/omniverse/\n\n# Install Isaac Sim extension in Omniverse\n# Launch Omniverse, go to Library, search for Isaac Sim\n\n# Install Isaac ROS\nsudo apt-get install -y ros-humble-isaac-ros-*\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#isaac-sim-concepts","title":"Isaac Sim Concepts","text":""},{"location":"physical-ai/chapter-6-isaac/#usd-files","title":"USD Files","text":"<p>USD (Universal Scene Description) is the format for Isaac Sim scenes:</p> <pre><code># Create a simple scene programmatically\n\nfrom omni.isaac.kit import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicSphere\n\nworld = World(stage_units_in_meters=1.0)\n\nsphere = DynamicSphere(\n    prim_path=\"/World/Sphere\",\n    radius=0.1,\n    mass=1.0\n)\n\nworld.reset()\n\n# Run simulation step\nfor i in range(100):\n    world.step(render=True)\n\nsimulation_app.close()\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#isaac-ros-computer-vision","title":"Isaac ROS Computer Vision","text":""},{"location":"physical-ai/chapter-6-isaac/#visual-slam-v-slam","title":"Visual SLAM (V-SLAM)","text":"<p>Visual SLAM estimates robot position using camera images:</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import Image, CameraInfo\nimport numpy as np\n\nclass VSLAMNode(Node):\n    def __init__(self):\n        super().__init__('vslam_node')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publish odometry\n        self.odom_pub = self.create_publisher(Odometry, '/odom', 10)\n\n        # State\n        self.position = np.array([0.0, 0.0, 0.0])\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n\n    def image_callback(self, msg):\n        # Process image for visual features\n        # Extract features, match with previous frame\n        # Estimate camera motion\n\n        # Update pose estimate\n        self.publish_odometry()\n\n    def camera_info_callback(self, msg):\n        # Camera intrinsics\n        pass\n\n    def publish_odometry(self):\n        msg = Odometry()\n        msg.header.frame_id = \"odom\"\n        msg.child_frame_id = \"base_link\"\n\n        msg.pose.pose.position.x = self.position[0]\n        msg.pose.pose.position.y = self.position[1]\n        msg.pose.pose.position.z = self.position[2]\n\n        msg.pose.pose.orientation.x = self.orientation[0]\n        msg.pose.pose.orientation.y = self.orientation[1]\n        msg.pose.pose.orientation.z = self.orientation[2]\n        msg.pose.pose.orientation.w = self.orientation[3]\n\n        self.odom_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VSLAMNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#object-detection-with-yolo","title":"Object Detection with YOLO","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithScore\nfrom cv_bridge import CvBridge\nimport cv2\nimport torch\nfrom yolov5 import YOLOv5\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/detections',\n            10\n        )\n\n        # Load YOLO model on GPU\n        self.model = YOLOv5('yolov5s')\n        self.model.to('cuda')\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Run YOLO inference\n        results = self.model(cv_image)\n\n        # Create detection message\n        detections_msg = Detection2DArray()\n        detections_msg.header = msg.header\n\n        for *box, conf, cls in results.xyxy[0]:\n            detection = Detection2D()\n            detection.bbox.center.x = float((box[0] + box[2]) / 2)\n            detection.bbox.center.y = float((box[1] + box[3]) / 2)\n            detection.bbox.size_x = float(box[2] - box[0])\n            detection.bbox.size_y = float(box[3] - box[1])\n\n            hypothesis = ObjectHypothesisWithScore()\n            hypothesis.class_name = str(int(cls))\n            hypothesis.score = float(conf)\n\n            detection.results.append(hypothesis)\n            detections_msg.detections.append(detection)\n\n        self.detection_pub.publish(detections_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetectionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#path-planning-with-nav2","title":"Path Planning with Nav2","text":""},{"location":"physical-ai/chapter-6-isaac/#nav2-architecture","title":"Nav2 Architecture","text":"<p>Nav2 provides autonomous navigation with: - Path planning (global planner) - Obstacle avoidance (local planner) - Behavior trees for complex behaviors</p> <pre><code># Basic Nav2 navigation\n\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\n\nclass NavigationClient(Node):\n    def __init__(self):\n        super().__init__('nav_client')\n        self._action_client = ActionClient(\n            self,\n            NavigateToPose,\n            'navigate_to_pose'\n        )\n\n    def send_goal(self, x, y, theta):\n        goal = NavigateToPose.Goal()\n        goal.pose.header.frame_id = 'map'\n        goal.pose.pose.position.x = x\n        goal.pose.pose.position.y = y\n\n        # Quaternion from Euler angles\n        goal.pose.pose.orientation.w = 1.0\n        goal.pose.pose.orientation.z = theta\n\n        self._action_client.wait_for_server()\n        self._send_goal_future = self._action_client.send_goal_async(goal)\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Goal rejected')\n            return\n        self.get_logger().info('Goal accepted!')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    client = NavigationClient()\n\n    # Navigate to goal\n    client.send_goal(x=5.0, y=5.0, theta=0.0)\n\n    rclpy.spin(client)\n    client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#reinforcement-learning-for-robot-control","title":"Reinforcement Learning for Robot Control","text":""},{"location":"physical-ai/chapter-6-isaac/#dqn-deep-q-network-example","title":"DQN (Deep Q-Network) Example","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(QNetwork, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim)\n        )\n\n    def forward(self, state):\n        return self.net(state)\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Q-networks\n        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-3)\n        self.loss_fn = nn.MSELoss()\n\n        # Replay buffer\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0\n        self.gamma = 0.99\n\n    def select_action(self, state):\n        if random.random() &lt; self.epsilon:\n            # Exploration\n            return random.randint(0, self.action_dim - 1)\n        else:\n            # Exploitation\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).to(self.device)\n                q_values = self.q_network(state_tensor)\n                return q_values.argmax().item()\n\n    def train(self, batch_size=32):\n        if len(self.memory) &lt; batch_size:\n            return\n\n        batch = random.sample(self.memory, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n\n        # Compute Q values\n        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n\n        # Compute target Q values\n        next_q_values = self.target_network(next_states).max(1)[0]\n        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n\n        # Train\n        loss = self.loss_fn(q_values, target_q_values.detach())\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.epsilon = max(0.01, self.epsilon * 0.995)\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#sim-to-real-transfer","title":"Sim-to-Real Transfer","text":""},{"location":"physical-ai/chapter-6-isaac/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Isaac Sim generates synthetic datasets for training:</p> <pre><code># Generate synthetic training data\n\ndef generate_training_data(num_images=1000):\n    from omni.isaac.kit import SimulationApp\n    from omni.isaac.core import World\n    import numpy as np\n\n    simulation_app = SimulationApp()\n    world = World()\n\n    training_data = []\n\n    for i in range(num_images):\n        # Randomize environment\n        # - Object positions\n        # - Lighting\n        # - Camera angles\n\n        world.step()\n\n        # Capture RGB image and depth\n        # rgb_image, depth_image = capture_images()\n\n        # Get object labels\n        # bboxes = get_bounding_boxes()\n\n        # training_data.append({\n        #     'rgb': rgb_image,\n        #     'depth': depth_image,\n        #     'labels': bboxes\n        # })\n\n    simulation_app.close()\n    return training_data\n\n# Train model on synthetic data\n# Then fine-tune on real data\n</code></pre>"},{"location":"physical-ai/chapter-6-isaac/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Isaac Sim provides photorealistic simulation with AI \u2705 Isaac ROS offers hardware-accelerated perception \u2705 V-SLAM enables robot localization from vision \u2705 Deep learning models can run on Jetson edge devices \u2705 Reinforcement learning trains robot policies \u2705 Synthetic data generation bridges sim-to-real gap </p>"},{"location":"physical-ai/chapter-6-isaac/#exercises","title":"Exercises","text":"<ol> <li>Create an Isaac Sim scene with a humanoid robot</li> <li>Implement a VSLAM node and visualize odometry</li> <li>Train a YOLO model on synthetic object detection data</li> <li>Build a DQN agent for robot arm control</li> </ol> <p>Next: Chapter 7: Vision-Language-Action Models</p>"},{"location":"physical-ai/chapter-7-vla/","title":"Chapter 7: Vision-Language-Action Models","text":""},{"location":"physical-ai/chapter-7-vla/#the-vla-paradigm","title":"The VLA Paradigm","text":"<p>Vision-Language-Action (VLA) models represent the frontier of robotics\u2014where language understanding, visual perception, and motor control converge.</p>"},{"location":"physical-ai/chapter-7-vla/#the-idea","title":"The Idea","text":"<pre><code>User Command (Language)\n        \u2193\nLLM Understands Task\n        \u2193\nOutputs Action Sequence\n        \u2193\nVision Model Verifies\n        \u2193\nRobot Executes\n        \u2193\nFeedback Loop\n</code></pre> <p>Example: - Input: \"Pick up the red cube and place it in the box\" - LLM Processing: Breaks into subtasks - Action Sequence: [locate_object, approach, grasp, lift, place, release] - Vision Check: Confirms object detected and reached - Execution: Motor commands sent to robot</p>"},{"location":"physical-ai/chapter-7-vla/#voice-to-action-pipeline","title":"Voice-to-Action Pipeline","text":""},{"location":"physical-ai/chapter-7-vla/#step-1-speech-to-text-with-whisper","title":"Step 1: Speech-to-Text with Whisper","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport numpy as np\nimport sounddevice as sd\nimport scipy.io.wavfile as wavfile\n\nclass SpeechToTextNode(Node):\n    def __init__(self):\n        super().__init__('speech_to_text')\n\n        # Setup OpenAI API\n        openai.api_key = \"your-api-key\"\n\n        self.text_pub = self.create_publisher(String, '/voice/text', 10)\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.duration = 5  # Record for 5 seconds\n\n    def record_audio(self):\n        # Record from microphone\n        audio_data = sd.rec(\n            int(self.sample_rate * self.duration),\n            samplerate=self.sample_rate,\n            channels=1,\n            dtype=np.int16\n        )\n        sd.wait()\n        return audio_data\n\n    def transcribe(self, audio_file):\n        # Use OpenAI Whisper\n        with open(audio_file, 'rb') as f:\n            transcript = openai.Audio.transcribe(\"whisper-1\", f)\n        return transcript['text']\n\n    def main_loop(self):\n        self.get_logger().info('Listening for voice commands...')\n\n        while True:\n            # Record audio\n            audio_data = self.record_audio()\n\n            # Save to file\n            wavfile.write('speech.wav', self.sample_rate, audio_data)\n\n            # Transcribe\n            text = self.transcribe('speech.wav')\n            self.get_logger().info(f'Recognized: {text}')\n\n            # Publish\n            msg = String()\n            msg.data = text\n            self.text_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SpeechToTextNode()\n    node.main_loop()\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#step-2-language-understanding-with-gpt","title":"Step 2: Language Understanding with GPT","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport json\n\nclass LanguageUnderstandingNode(Node):\n    def __init__(self):\n        super().__init__('language_understanding')\n\n        openai.api_key = \"your-api-key\"\n\n        # Subscribe to voice text\n        self.text_sub = self.create_subscription(\n            String,\n            '/voice/text',\n            self.text_callback,\n            10\n        )\n\n        # Publish action plan\n        self.action_pub = self.create_publisher(String, '/robot/action_plan', 10)\n\n    def text_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'Processing: {command}')\n\n        # Call GPT to understand the command\n        action_plan = self.plan_actions(command)\n\n        # Publish plan\n        plan_msg = String()\n        plan_msg.data = json.dumps(action_plan)\n        self.action_pub.publish(plan_msg)\n\n    def plan_actions(self, command):\n        \"\"\"Use GPT to convert natural language to robot actions\"\"\"\n\n        prompt = f\"\"\"You are a robot control system. Given a natural language command, \nbreak it into a sequence of robot actions.\n\nCommand: \"{command}\"\n\nAvailable actions:\n- move_to(x, y, z): Move end effector to position\n- grasp(): Close gripper\n- release(): Open gripper\n- rotate_gripper(angle): Rotate end effector\n- wait(seconds): Wait\n\nRespond as JSON array of actions:\n[{{\"action\": \"move_to\", \"params\": {{\"x\": 0.3, \"y\": 0.2, \"z\": 0.5}}}}, ...]\n\"\"\"\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a robot control planner.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.7\n        )\n\n        # Parse response\n        action_text = response['choices'][0]['message']['content']\n        try:\n            actions = json.loads(action_text)\n            return actions\n        except:\n            self.get_logger().error(f\"Failed to parse actions: {action_text}\")\n            return []\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LanguageUnderstandingNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#step-3-vision-based-action-verification","title":"Step 3: Vision-based Action Verification","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom cv_bridge import CvBridge\nimport json\nimport cv2\n\nclass ActionVerificationNode(Node):\n    def __init__(self):\n        super().__init__('action_verification')\n\n        # Subscribe to action plan and camera\n        self.action_sub = self.create_subscription(\n            String,\n            '/robot/action_plan',\n            self.action_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            1\n        )\n\n        # Publish execution feedback\n        self.feedback_pub = self.create_publisher(String, '/robot/feedback', 10)\n\n        # Load CLIP model for vision-language understanding\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.current_plan = None\n\n    def image_callback(self, msg):\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n    def action_callback(self, msg):\n        self.current_plan = json.loads(msg.data)\n        self.verify_and_execute_plan()\n\n    def verify_and_execute_plan(self):\n        \"\"\"Verify each action against current scene\"\"\"\n\n        for action in self.current_plan:\n            # Get current image\n            if self.current_image is None:\n                continue\n\n            # Verify object exists if action requires it\n            if 'object' in action.get('params', {}):\n                object_name = action['params']['object']\n\n                if self.check_object_in_scene(object_name):\n                    self.get_logger().info(f'\u2713 Object \"{object_name}\" detected')\n                    feedback = f\"Verified: {object_name} is present\"\n                else:\n                    self.get_logger().warn(f'\u2717 Object \"{object_name}\" NOT detected')\n                    feedback = f\"Warning: {object_name} not found in current view\"\n\n                # Publish feedback\n                msg = String()\n                msg.data = feedback\n                self.feedback_pub.publish(msg)\n\n    def check_object_in_scene(self, object_name):\n        \"\"\"Use CLIP to check if object is in scene\"\"\"\n\n        # Prepare text and image inputs\n        text_inputs = self.processor(\n            text=[f\"a {object_name}\"],\n            return_tensors=\"pt\",\n            padding=True\n        ).to(self.device)\n\n        image_input = self.processor(\n            images=self.current_image,\n            return_tensors=\"pt\"\n        )['pixel_values'].to(self.device)\n\n        # Compute embeddings\n        with torch.no_grad():\n            image_features = self.model.get_image_features(image_input)\n            text_features = self.model.get_text_features(**text_inputs)\n\n        # Normalize\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # Similarity score\n        similarity = (image_features @ text_features.T)[0][0].item()\n\n        return similarity &gt; 0.3  # Threshold\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionVerificationNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#step-4-motor-control-execution","title":"Step 4: Motor Control Execution","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float64MultiArray, String\nfrom geometry_msgs.msg import Twist\nimport json\nimport time\n\nclass MotorControlNode(Node):\n    def __init__(self):\n        super().__init__('motor_control')\n\n        # Subscribe to action plan\n        self.action_sub = self.create_subscription(\n            String,\n            '/robot/action_plan',\n            self.execute_action,\n            10\n        )\n\n        # Publishers for different actuators\n        self.arm_pub = self.create_publisher(Float64MultiArray, '/arm/velocity', 10)\n        self.gripper_pub = self.create_publisher(Float64MultiArray, '/gripper/velocity', 10)\n        self.base_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n    def execute_action(self, msg):\n        plan = json.loads(msg.data)\n\n        for action in plan:\n            action_type = action.get('action')\n            params = action.get('params', {})\n\n            if action_type == 'move_to':\n                self.move_to(params['x'], params['y'], params['z'])\n            elif action_type == 'grasp':\n                self.grasp()\n            elif action_type == 'release':\n                self.release()\n            elif action_type == 'rotate_gripper':\n                self.rotate_gripper(params['angle'])\n            elif action_type == 'wait':\n                time.sleep(params.get('seconds', 1))\n\n    def move_to(self, x, y, z):\n        \"\"\"Move end effector to target position\"\"\"\n        # Use inverse kinematics to compute joint angles\n        joint_angles = self.compute_ik(x, y, z)\n\n        msg = Float64MultiArray()\n        msg.data = joint_angles\n        self.arm_pub.publish(msg)\n\n        self.get_logger().info(f'Moving to ({x}, {y}, {z})')\n\n    def grasp(self):\n        \"\"\"Close gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [1.0]  # Close gripper\n        self.gripper_pub.publish(msg)\n        self.get_logger().info('Grasping')\n\n    def release(self):\n        \"\"\"Open gripper\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [0.0]  # Open gripper\n        self.gripper_pub.publish(msg)\n        self.get_logger().info('Releasing')\n\n    def rotate_gripper(self, angle):\n        \"\"\"Rotate end effector\"\"\"\n        msg = Float64MultiArray()\n        msg.data = [angle]\n        self.arm_pub.publish(msg)\n\n    def compute_ik(self, x, y, z):\n        \"\"\"Compute inverse kinematics\"\"\"\n        # Mock implementation\n        return [0.0, 0.0, 0.0, 0.0]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MotorControlNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#full-integration-autonomous-humanoid","title":"Full Integration: Autonomous Humanoid","text":"<pre><code># Complete autonomous system\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass AutonomousHumanoidNode(Node):\n    def __init__(self):\n        super().__init__('autonomous_humanoid')\n\n        # System state\n        self.state = \"idle\"\n        self.current_task = None\n\n    def voice_command_received(self, command):\n        \"\"\"Main entry point for voice commands\"\"\"\n\n        self.get_logger().info(f'\ud83c\udfa4 Received: {command}')\n\n        # Step 1: Parse with LLM\n        actions = self.parse_command(command)\n        self.get_logger().info(f'\ud83d\udccb Parsed {len(actions)} actions')\n\n        # Step 2: Verify with vision\n        verified_actions = self.verify_actions(actions)\n        self.get_logger().info(f'\u2713 Verified {len(verified_actions)} actions')\n\n        # Step 3: Execute actions\n        for action in verified_actions:\n            self.execute_action(action)\n\n        self.get_logger().info('\u2713 Task completed')\n\n    def parse_command(self, command):\n        \"\"\"Use LLM to parse natural language\"\"\"\n        # Implementation from LanguageUnderstandingNode\n        pass\n\n    def verify_actions(self, actions):\n        \"\"\"Use vision to verify feasibility\"\"\"\n        # Implementation from ActionVerificationNode\n        return actions\n\n    def execute_action(self, action):\n        \"\"\"Execute single action\"\"\"\n        # Implementation from MotorControlNode\n        pass\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#key-applications","title":"Key Applications","text":""},{"location":"physical-ai/chapter-7-vla/#1-home-assistance","title":"1. Home Assistance","text":"<pre><code>Voice: \"Clean the living room\"\n\u2192 Navigate to room\n\u2192 Search for trash\n\u2192 Pick up items\n\u2192 Place in trash bin\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#2-manufacturing","title":"2. Manufacturing","text":"<pre><code>Voice: \"Assemble part A and part B\"\n\u2192 Locate parts\n\u2192 Grasp part A\n\u2192 Align with part B\n\u2192 Apply force\n\u2192 Verify assembly\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#3-search-rescue","title":"3. Search &amp; Rescue","text":"<pre><code>Voice: \"Find and retrieve the medical kit\"\n\u2192 Navigate through debris\n\u2192 Search for kit\n\u2192 Identify location\n\u2192 Grasp and carry\n\u2192 Return to base\n</code></pre>"},{"location":"physical-ai/chapter-7-vla/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 VLA models combine language, vision, and action \u2705 Whisper enables accurate speech-to-text \u2705 LLMs break down natural language into actions \u2705 Vision models verify actions are feasible \u2705 Motor controllers execute verified actions \u2705 Feedback loops enable error recovery </p>"},{"location":"physical-ai/chapter-7-vla/#exercises","title":"Exercises","text":"<ol> <li>Implement a complete voice-to-action pipeline</li> <li>Add error handling and retry logic</li> <li>Create multi-step task sequences</li> <li>Build a demo with real-time visualization</li> </ol> <p>Next: Chapter 8: Capstone Project - Autonomous Humanoid</p>"},{"location":"physical-ai/intro/","title":"Physical AI &amp; Humanoid Robotics","text":""},{"location":"physical-ai/intro/#course-overview","title":"Course Overview","text":"<p>Welcome to the Physical AI &amp; Humanoid Robotics course. This comprehensive textbook introduces you to the convergence of artificial intelligence and physical embodiment\u2014where digital brains meet robotic bodies.</p>"},{"location":"physical-ai/intro/#the-future-is-physical","title":"The Future Is Physical","text":"<p>The future of AI extends beyond digital spaces into the physical world. This course introduces Physical AI\u2014artificial intelligence systems that function in reality and comprehend physical laws. You will learn to design, simulate, and deploy humanoid robots capable of natural human interactions using industry-standard tools.</p> <p>This transition from AI models confined to digital environments to embodied intelligence that operates in physical space is one of the most significant technological shifts of our generation.</p>"},{"location":"physical-ai/intro/#focus-theme","title":"Focus &amp; Theme","text":"<p>AI Systems in the Physical World. Embodied Intelligence.</p> <p>The goal of this course: Bridging the gap between the digital brain and the physical body. Students apply their AI knowledge to control humanoid robots in simulated and real-world environments.</p>"},{"location":"physical-ai/intro/#why-humanoid-robots-matter","title":"Why Humanoid Robots Matter","text":"<p>Humanoid robots are uniquely suited for our human-centered world because they: - Share our physical form and can interact naturally with human environments - Are trained with abundant data from human interactions - Represent the frontier of embodied intelligence and physical reasoning</p>"},{"location":"physical-ai/intro/#what-you-will-learn","title":"What You Will Learn","text":"<p>By completing this course, you will:</p> <p>\u2705 Understand Physical AI principles and embodied intelligence \u2705 Master ROS 2 (Robot Operating System) - the robotic nervous system \u2705 Simulate robots with Gazebo - the physics engine \u2705 Create high-fidelity environments with Unity \u2705 Develop with NVIDIA Isaac - advanced perception and training \u2705 Build Vision-Language-Action systems - voice-to-action control \u2705 Deploy the capstone: autonomous humanoid with voice commands  </p>"},{"location":"physical-ai/intro/#quarter-overview","title":"Quarter Overview","text":"<p>The four modules cover:</p> <ol> <li>Module 1: The Robotic Nervous System (ROS 2)</li> <li>ROS 2 middleware for robot control</li> <li>Nodes, Topics, and Services architecture</li> <li>Bridging Python agents to ROS controllers</li> <li> <p>URDF for humanoid robot description</p> </li> <li> <p>Module 2: The Digital Twin (Gazebo &amp; Unity)</p> </li> <li>Physics simulation and environment building with Gazebo</li> <li>High-fidelity rendering and human-robot interaction in Unity</li> <li>Simulating sensors: LiDAR, Depth Cameras, IMUs</li> <li> <p>Testing algorithms before hardware deployment</p> </li> <li> <p>Module 3: The AI-Robot Brain (NVIDIA Isaac)</p> </li> <li>NVIDIA Isaac Sim for photorealistic simulation</li> <li>Synthetic data generation for AI training</li> <li>Isaac ROS hardware-accelerated perception</li> <li> <p>Nav2 path planning for bipedal humanoid movement</p> </li> <li> <p>Module 4: Vision-Language-Action (VLA)</p> </li> <li>Voice-to-Action using OpenAI Whisper</li> <li>Cognitive planning with LLMs for natural language understanding</li> <li>Translating \"Clean the room\" into ROS 2 action sequences</li> <li>Real-time vision-language reasoning</li> </ol>"},{"location":"physical-ai/intro/#capstone-project","title":"Capstone Project","text":"<p>The Autonomous Humanoid: A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.</p> <p>Pipeline: 1. Human speaks: \"Pick up the red ball\" 2. Whisper transcribes to text 3. Vision system locates the red ball 4. LLM plans: move \u2192 reach \u2192 grasp \u2192 pickup 5. ROS 2 controllers execute the plan 6. Robot completes the task</p>"},{"location":"physical-ai/intro/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Strong Python fundamentals</li> <li>Mathematics: Linear algebra, calculus, and probability</li> <li>Robotics: Basic understanding of kinematics and dynamics is helpful</li> <li>Hardware: Access to a high-performance workstation (RTX 4070 Ti or higher recommended)</li> </ul>"},{"location":"physical-ai/intro/#hardware-requirements","title":"Hardware Requirements","text":"<p>This course is computationally intensive, requiring:</p> <p>The Digital Twin Workstation (Required) - GPU: NVIDIA RTX 4070 Ti (12GB VRAM) or higher - CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9 - RAM: 64GB DDR5 (32GB minimum) - OS: Ubuntu 22.04 LTS</p> <p>The Physical AI Edge Kit (Recommended) - NVIDIA Jetson Orin Nano (8GB) or Orin NX (16GB) - Intel RealSense D435i or D455 camera - Generic USB IMU (BNO055) - USB microphone/speaker array (ReSpeaker)</p> <p>The Robot Lab (Optional) - Unitree Go2 Edu (~$1,800-$3,000) for budget option - Unitree G1 Humanoid (~$16k) for premium option</p>"},{"location":"physical-ai/intro/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>Each chapter includes: - Conceptual Foundations - Theory and principles - Code Examples - Practical Python implementations - Hands-On Exercises - Build your skills step by step - Lab Projects - Integrate learning into complete systems - Key Takeaways - Summary of critical concepts</p>"},{"location":"physical-ai/intro/#join-the-robotics-revolution","title":"Join the Robotics Revolution","text":"<p>The convergence of AI and robotics is happening now. By mastering Physical AI, you're positioning yourself at the forefront of one of the most transformative technological shifts. Let's begin!</p> <p>Next: Chapter 1: Introduction to Physical AI</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/","title":"Chapter 1: ROS 2 Architecture &amp; Core Concepts","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#what-is-ros-2","title":"What is ROS 2?","text":"<p>ROS 2 (Robot Operating System 2) is the middleware that serves as the \"nervous system\" of your humanoid robot. It manages communication between all robot components: sensors, processors, and actuators.</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#the-nervous-system-analogy","title":"The Nervous System Analogy","text":"<p>Just like your biological nervous system coordinates your brain with your body: - Brain = AI algorithms running on edge computers - Spinal cord = ROS 2 (message passing, coordination) - Sensors = Cameras, LiDAR, IMUs, depth cameras - Muscles = Motors, actuators, grippers</p> <p>Without ROS 2, these components can't communicate. With ROS 2, they work as one integrated system.</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#ros-2-core-concepts","title":"ROS 2 Core Concepts","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#1-nodes","title":"1. Nodes","text":"<p>A node is an independent process that performs a specific task.</p> <p>Examples in your humanoid robot: - Vision processing node (detects objects) - Motor control node (moves joints) - Navigation node (plans paths) - AI inference node (runs LLM)</p> <p>Key principle: Each node is independent. If one crashes, others keep running.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\n\nclass MyRobotNode(Node):\n    def __init__(self):\n        super().__init__('my_robot_node')\n        self.get_logger().info('Robot node started!')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MyRobotNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#2-topics","title":"2. Topics","text":"<p>Topics are data streams. One node publishes data, others subscribe.</p> <p>Publisher example (motor sends joint angles): <pre><code>from geometry_msgs.msg import Twist\n\nself.publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n\nmsg = Twist()\nmsg.linear.x = 0.5  # Move forward\nself.publisher.publish(msg)\n</code></pre></p> <p>Subscriber example (sensor reads data): <pre><code>from sensor_msgs.msg import Image\n\nself.subscription = self.create_subscription(\n    Image,\n    'camera/image_raw',\n    self.image_callback,\n    10\n)\n\ndef image_callback(self, msg):\n    # Process camera image\n    pass\n</code></pre></p>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#3-services","title":"3. Services","text":"<p>Services are request-response patterns. One node asks, another replies.</p> <p>Service definition: <pre><code># GetRobotState.srv\n---\nbool is_moving\nfloat32 battery_level\n</code></pre></p> <p>Service server: <pre><code>srv = self.create_service(GetRobotState, 'get_state', self.handle_get_state)\n\ndef handle_get_state(self, request, response):\n    response.is_moving = True\n    response.battery_level = 0.85\n    return response\n</code></pre></p> <p>Service client: <pre><code>client = self.create_client(GetRobotState, 'get_state')\nfuture = client.call_async(GetRobotState.Request())\nresponse = future.result()\nprint(f\"Battery: {response.battery_level}\")\n</code></pre></p>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#ros-2-executor-pattern","title":"ROS 2 Executor Pattern","text":"<p>The <code>rclpy.spin()</code> function creates an event loop that: 1. Listens for messages on subscribed topics 2. Calls callbacks when data arrives 3. Processes service requests 4. Allows your robot to react in real-time</p> <pre><code>def main():\n    rclpy.init()\n    node = MyRobotNode()\n\n    # This loop runs forever, processing callbacks\n    rclpy.spin(node)\n\n    rclpy.shutdown()\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<p>ROS 2 allows tuning message delivery guarantees:</p> <pre><code>from rclpy.qos import QoSProfile, ReliabilityPolicy\n\n# Reliable delivery (for critical commands)\nqos = QoSProfile(reliability=ReliabilityPolicy.RELIABLE)\nself.create_subscription(Twist, 'cmd_vel', callback, qos)\n\n# Best-effort (for sensor data that's always updating)\nqos = QoSProfile(reliability=ReliabilityPolicy.BEST_EFFORT)\nself.create_subscription(Image, 'camera/image', callback, qos)\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/1-ros2-fundamentals/#summary","title":"Summary","text":"<p>ROS 2 gives your humanoid robot: - \u2705 Distributed computing - Many processes working together - \u2705 Real-time communication - Sensors \u2192 decisions \u2192 actions in milliseconds - \u2705 Fault isolation - One failed component doesn't crash everything - \u2705 Scalability - Add new nodes without modifying existing ones - \u2705 Language agnostic - Mix Python, C++, Rust nodes seamlessly</p> <p>In the next chapters, you'll build actual ROS 2 applications and create URDF descriptions for humanoid robots.</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/","title":"Chapter 2: Your First ROS 2 Node","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#building-your-first-node","title":"Building Your First Node","text":"<p>Let's create a simple robot controller that publishes velocity commands to move a humanoid robot forward.</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#setup","title":"Setup","text":"<pre><code># Install ROS 2 (if not already installed)\nsudo apt install ros-humble-desktop\n\n# Create a workspace\nmkdir -p ~/robot_ws/src\ncd ~/robot_ws\n\n# Create a package\nros2 pkg create --build-type ament_python robot_controller\ncd robot_controller\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#create-your-first-node","title":"Create Your First Node","text":"<p>Create <code>robot_controller/robot_controller/motor_controller.py</code>:</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass MotorController(Node):\n    \"\"\"Controls the humanoid robot's movement\"\"\"\n\n    def __init__(self):\n        super().__init__('motor_controller')\n\n        # Create a publisher for velocity commands\n        self.publisher = self.create_publisher(\n            Twist,\n            'cmd_vel',\n            10\n        )\n\n        self.get_logger().info('Motor Controller initialized')\n        self.get_logger().info('Publishing to /cmd_vel topic')\n\n        # Timer to send commands every 0.5 seconds\n        self.timer = self.create_timer(0.5, self.publish_velocity)\n        self.counter = 0\n\n    def publish_velocity(self):\n        \"\"\"Send movement commands to the robot\"\"\"\n\n        # Create a Twist message (standard ROS 2 velocity command)\n        msg = Twist()\n\n        # Linear velocity (move forward 0.5 m/s)\n        msg.linear.x = 0.5\n        msg.linear.y = 0.0\n        msg.linear.z = 0.0\n\n        # Angular velocity (rotate slowly)\n        msg.angular.x = 0.0\n        msg.angular.y = 0.0\n        msg.angular.z = 0.1  # Rotate 0.1 rad/s\n\n        # Publish the message\n        self.publisher.publish(msg)\n\n        self.counter += 1\n        self.get_logger().info(\n            f'Published velocity command #{self.counter}: '\n            f'forward={msg.linear.x} m/s, rotate={msg.angular.z} rad/s'\n        )\n\ndef main(args=None):\n    # Initialize ROS 2\n    rclpy.init(args=args)\n\n    # Create the node\n    node = MotorController()\n\n    # Keep the node running\n    rclpy.spin(node)\n\n    # Shutdown when done\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#update-setuppy","title":"Update setup.py","text":"<p>Edit <code>setup.py</code> to include your node:</p> <pre><code>entry_points={\n    'console_scripts': [\n        'motor_controller = robot_controller.motor_controller:main',\n    ],\n},\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#build-and-run","title":"Build and Run","text":"<pre><code># Go to workspace root\ncd ~/robot_ws\n\n# Build the package\ncolcon build\n\n# Source the setup script\nsource install/setup.bash\n\n# Run your node\nros2 run robot_controller motor_controller\n</code></pre> <p>Output: <pre><code>[INFO] Motor Controller initialized\n[INFO] Publishing to /cmd_vel topic\n[INFO] Published velocity command #1: forward=0.5 m/s, rotate=0.1 rad/s\n[INFO] Published velocity command #2: forward=0.5 m/s, rotate=0.1 rad/s\n</code></pre></p>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#monitoring-topics-with-ros2-cli","title":"Monitoring Topics with ros2 CLI","text":"<p>In another terminal, watch the published messages:</p> <pre><code># See all topics\nros2 topic list\n\n# See message frequency and data\nros2 topic hz /cmd_vel\nros2 topic echo /cmd_vel\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#advanced-subscribing-to-sensor-data","title":"Advanced: Subscribing to Sensor Data","text":"<p>Create a sensor subscriber node:</p> <pre><code>from sensor_msgs.msg import Image, Imu\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\n\nclass SensorMonitor(Node):\n    def __init__(self):\n        super().__init__('sensor_monitor')\n\n        # Subscribe to camera images\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            QoSProfile(reliability=ReliabilityPolicy.BEST_EFFORT)\n        )\n\n        # Subscribe to IMU data (Inertial Measurement Unit)\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n    def image_callback(self, msg):\n        self.get_logger().info(\n            f'Received image: {msg.width}x{msg.height}'\n        )\n\n    def imu_callback(self, msg):\n        self.get_logger().info(\n            f'Accel: x={msg.linear_acceleration.x:.2f} m/s\u00b2'\n        )\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/2-first-ros2-node/#summary","title":"Summary","text":"<p>You now understand: - \u2705 How to create a ROS 2 Node - \u2705 How to publish messages to topics - \u2705 How to subscribe to topics - \u2705 The ROS 2 execution model with <code>rclpy.spin()</code> - \u2705 Message types (Twist for velocities, Image for cameras, Imu for sensors)</p> <p>Next: Learn URDF to describe humanoid robot structure!</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/","title":"Chapter 3: URDF - Describing Your Humanoid Robot","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#what-is-urdf","title":"What is URDF?","text":"<p>URDF (Unified Robot Description Format) is an XML standard that describes: - Robot structure (links and joints) - Physical properties (mass, size, materials) - Sensor locations (cameras, LiDAR, IMU) - Actuator constraints (joint limits, friction)</p> <p>Think of URDF as a blueprint that tells Gazebo and ROS 2 how your robot is built.</p>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#urdf-structure","title":"URDF Structure","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#basic-components","title":"Basic Components","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot name=\"humanoid_v1\"&gt;\n\n  &lt;!-- Links: Physical bodies --&gt;\n  &lt;link name=\"base_link\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"10.0\"/&gt;\n      &lt;inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.1\" iyz=\"0.0\" izz=\"0.1\"/&gt;\n    &lt;/inertial&gt;\n\n    &lt;!-- Visual appearance --&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.3 0.8\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"white\"/&gt;\n    &lt;/visual&gt;\n\n    &lt;!-- Collision geometry --&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.3 0.8\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Joints: Connections between links --&gt;\n  &lt;joint name=\"torso_to_left_shoulder\" type=\"revolute\"&gt;\n    &lt;parent link=\"base_link\"/&gt;\n    &lt;child link=\"left_shoulder\"/&gt;\n    &lt;origin xyz=\"0.15 0.2 0.0\" rpy=\"0 0 0\"/&gt;\n\n    &lt;!-- Range of motion --&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/&gt;\n\n    &lt;!-- Friction properties --&gt;\n    &lt;dynamics damping=\"0.1\" friction=\"0.0\"/&gt;\n  &lt;/joint&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#a-complete-humanoid-example","title":"A Complete Humanoid Example","text":"<p>Here's a simplified humanoid with torso, arms, and head:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot name=\"humanoid_simple\"&gt;\n\n  &lt;!-- Torso (main body) --&gt;\n  &lt;link name=\"torso\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"15.0\"/&gt;\n      &lt;inertia ixx=\"0.5\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.5\" iyz=\"0.0\" izz=\"0.3\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.4 1.2\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;box size=\"0.3 0.4 1.2\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Head --&gt;\n  &lt;link name=\"head\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.02\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.15\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"skin\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.15\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Neck joint connecting head to torso --&gt;\n  &lt;joint name=\"neck_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"head\"/&gt;\n    &lt;origin xyz=\"0 0 0.7\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 0 1\"/&gt;  &lt;!-- Rotate around Z axis --&gt;\n    &lt;limit lower=\"-1.57\" upper=\"1.57\" effort=\"5\" velocity=\"2.0\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Left Arm --&gt;\n  &lt;link name=\"left_shoulder\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.01\" iyz=\"0.0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;link name=\"left_upper_arm\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.005\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;!-- Shoulder joint --&gt;\n  &lt;joint name=\"left_shoulder_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"left_shoulder\"/&gt;\n    &lt;origin xyz=\"0.2 0.25 0.4\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"1 0 0\"/&gt;  &lt;!-- Rotate around X axis --&gt;\n    &lt;limit lower=\"-3.14\" upper=\"3.14\" effort=\"20\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.1\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Upper arm joint --&gt;\n  &lt;joint name=\"left_shoulder_to_upper_arm\" type=\"revolute\"&gt;\n    &lt;parent link=\"left_shoulder\"/&gt;\n    &lt;child link=\"left_upper_arm\"/&gt;\n    &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;  &lt;!-- Rotate around Y axis --&gt;\n    &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"15\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Right Arm (mirror of left) --&gt;\n  &lt;link name=\"right_shoulder\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"1.5\"/&gt;\n      &lt;inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.01\" iyz=\"0.0\" izz=\"0.01\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;sphere radius=\"0.08\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;link name=\"right_upper_arm\"&gt;\n    &lt;inertial&gt;\n      &lt;mass value=\"2.0\"/&gt;\n      &lt;inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" \n               iyy=\"0.02\" iyz=\"0.0\" izz=\"0.005\"/&gt;\n    &lt;/inertial&gt;\n    &lt;visual&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n      &lt;material name=\"blue\"/&gt;\n    &lt;/visual&gt;\n    &lt;collision&gt;\n      &lt;geometry&gt;\n        &lt;cylinder length=\"0.3\" radius=\"0.05\"/&gt;\n      &lt;/geometry&gt;\n    &lt;/collision&gt;\n  &lt;/link&gt;\n\n  &lt;joint name=\"right_shoulder_joint\" type=\"revolute\"&gt;\n    &lt;parent link=\"torso\"/&gt;\n    &lt;child link=\"right_shoulder\"/&gt;\n    &lt;origin xyz=\"-0.2 0.25 0.4\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"1 0 0\"/&gt;\n    &lt;limit lower=\"-3.14\" upper=\"3.14\" effort=\"20\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.1\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;joint name=\"right_shoulder_to_upper_arm\" type=\"revolute\"&gt;\n    &lt;parent link=\"right_shoulder\"/&gt;\n    &lt;child link=\"right_upper_arm\"/&gt;\n    &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n    &lt;axis xyz=\"0 1 0\"/&gt;\n    &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"15\" velocity=\"1.5\"/&gt;\n    &lt;dynamics damping=\"0.05\"/&gt;\n  &lt;/joint&gt;\n\n  &lt;!-- Materials definition --&gt;\n  &lt;material name=\"blue\"&gt;\n    &lt;color rgba=\"0.0 0.5 1.0 1.0\"/&gt;\n  &lt;/material&gt;\n\n  &lt;material name=\"skin\"&gt;\n    &lt;color rgba=\"0.9 0.8 0.7 1.0\"/&gt;\n  &lt;/material&gt;\n\n&lt;/robot&gt;\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#understanding-urdf-concepts","title":"Understanding URDF Concepts","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#links","title":"Links","text":"<pre><code>&lt;link name=\"left_hand\"&gt;\n  &lt;!-- Inertial: mass and moment of inertia --&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.5\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n\n  &lt;!-- Visual: how it looks in simulation --&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;mesh filename=\"package://robot_description/meshes/hand.stl\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n\n  &lt;!-- Collision: how it interacts physically --&gt;\n  &lt;collision&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.1 0.05 0.2\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/collision&gt;\n&lt;/link&gt;\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#joint-types","title":"Joint Types","text":"<pre><code>&lt;!-- Revolute (hinge joint) - rotates around one axis --&gt;\n&lt;joint name=\"elbow\" type=\"revolute\"&gt;\n  &lt;axis xyz=\"0 1 0\"/&gt;\n  &lt;limit lower=\"-2.0\" upper=\"2.0\" effort=\"10\" velocity=\"1.0\"/&gt;\n&lt;/joint&gt;\n\n&lt;!-- Prismatic (sliding joint) - moves along one axis --&gt;\n&lt;joint name=\"gripper\" type=\"prismatic\"&gt;\n  &lt;axis xyz=\"1 0 0\"/&gt;\n  &lt;limit lower=\"0\" upper=\"0.1\" effort=\"50\" velocity=\"0.5\"/&gt;\n&lt;/joint&gt;\n\n&lt;!-- Fixed (no movement) --&gt;\n&lt;joint name=\"camera_mount\" type=\"fixed\"&gt;\n  &lt;parent link=\"head\"/&gt;\n  &lt;child link=\"camera\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#using-urdf-in-ros-2","title":"Using URDF in ROS 2","text":""},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#load-urdf-in-a-launch-file","title":"Load URDF in a Launch File","text":"<pre><code># launch/display.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    urdf_file = get_package_share_directory('robot_description')\n    urdf_file += '/urdf/humanoid.urdf'\n\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{'robot_description': robot_desc}]\n        ),\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            arguments=['-d', get_package_share_directory('robot_description') + '/rviz/config.rviz']\n        )\n    ])\n</code></pre>"},{"location":"physical-ai/module-1-robotic-nervous-system/3-urdf-robot-description/#summary","title":"Summary","text":"<p>URDF enables: - \u2705 Simulation - Test robots in Gazebo before building - \u2705 Visualization - See robot structure in RViz - \u2705 Physics - Realistic mass, inertia, collisions - \u2705 Joint Control - Define what can move - \u2705 Sensor Mounting - Place cameras, LiDAR, IMU on robot</p> <p>Next: Simulate your humanoid in Gazebo!</p>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/","title":"Chapter 4: Physics Simulation with Gazebo","text":""},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#what-is-gazebo","title":"What is Gazebo?","text":"<p>Gazebo is a powerful physics simulator that: - Simulates gravity, friction, and collisions - Renders 3D environments in real-time - Integrates with ROS 2 for robot control - Lets you test algorithms before using real robots</p> <p>Think of Gazebo as a virtual test lab where you can crash test your code without breaking expensive hardware.</p>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#gazebo-fundamentals","title":"Gazebo Fundamentals","text":""},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#starting-gazebo","title":"Starting Gazebo","text":"<pre><code># Install Gazebo Harmonic\nsudo apt-get install ignition-gazebo\n\n# Launch Gazebo\ngazebo\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#creating-a-gazebo-world","title":"Creating a Gazebo World","text":"<p>Gazebo uses SDF (Simulation Description Format) files. Here's a simple world:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;sdf version=\"1.10\"&gt;\n  &lt;world name=\"robot_world\"&gt;\n\n    &lt;!-- Physics engine --&gt;\n    &lt;physics name=\"default_physics\" type=\"bullet\"&gt;\n      &lt;max_step_size&gt;0.001&lt;/max_step_size&gt;\n      &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt;\n    &lt;/physics&gt;\n\n    &lt;!-- Ground plane --&gt;\n    &lt;model name=\"ground_plane\"&gt;\n      &lt;static&gt;true&lt;/static&gt;\n      &lt;link name=\"link\"&gt;\n        &lt;collision name=\"collision\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n        &lt;/collision&gt;\n        &lt;visual name=\"visual\"&gt;\n          &lt;geometry&gt;\n            &lt;plane&gt;\n              &lt;normal&gt;0 0 1&lt;/normal&gt;\n              &lt;size&gt;100 100&lt;/size&gt;\n            &lt;/plane&gt;\n          &lt;/geometry&gt;\n          &lt;material&gt;\n            &lt;ambient&gt;0.8 0.8 0.8 1&lt;/ambient&gt;\n            &lt;diffuse&gt;0.8 0.8 0.8 1&lt;/diffuse&gt;\n          &lt;/material&gt;\n        &lt;/visual&gt;\n      &lt;/link&gt;\n    &lt;/model&gt;\n\n    &lt;!-- Lighting --&gt;\n    &lt;light type=\"directional\" name=\"sun\"&gt;\n      &lt;cast_shadows&gt;true&lt;/cast_shadows&gt;\n      &lt;pose&gt;0 0 10 0 0 0&lt;/pose&gt;\n      &lt;diffuse&gt;1 1 1 1&lt;/diffuse&gt;\n      &lt;specular&gt;0.5 0.5 0.5 1&lt;/specular&gt;\n      &lt;direction&gt;-1 -1 -1&lt;/direction&gt;\n    &lt;/light&gt;\n\n    &lt;!-- Your robot will be inserted here --&gt;\n    &lt;include&gt;\n      &lt;uri&gt;model://humanoid&lt;/uri&gt;\n      &lt;pose&gt;0 0 1 0 0 0&lt;/pose&gt;\n    &lt;/include&gt;\n\n  &lt;/world&gt;\n&lt;/sdf&gt;\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#simulating-your-humanoid-robot","title":"Simulating Your Humanoid Robot","text":""},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#launch-with-ros-2","title":"Launch with ROS 2","text":"<p>Create <code>launch/gazebo_humanoid.launch.py</code>:</p> <pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import ExecuteProcess\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Get paths\n    robot_desc_dir = get_package_share_directory('robot_description')\n    world_file = robot_desc_dir + '/worlds/robot_world.sdf'\n    urdf_file = robot_desc_dir + '/urdf/humanoid.urdf'\n\n    # Read URDF\n    with open(urdf_file, 'r') as f:\n        robot_desc = f.read()\n\n    return LaunchDescription([\n        # Launch Gazebo\n        ExecuteProcess(\n            cmd=['gazebo', '--verbose', world_file],\n            output='screen'\n        ),\n\n        # Publish robot description\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            parameters=[{'robot_description': robot_desc}]\n        ),\n\n        # Joint state broadcaster\n        Node(\n            package='controller_manager',\n            executable='spawner',\n            arguments=['joint_state_broadcaster'],\n        ),\n\n        # Diff drive controller for movement\n        Node(\n            package='controller_manager',\n            executable='spawner',\n            arguments=['diff_drive_controller'],\n        ),\n\n        # Your custom controller node\n        Node(\n            package='robot_controller',\n            executable='motor_controller',\n            output='screen'\n        ),\n    ])\n</code></pre> <p>Run it: <pre><code>ros2 launch robot_description gazebo_humanoid.launch.py\n</code></pre></p>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#sensor-simulation","title":"Sensor Simulation","text":""},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#simulating-a-camera","title":"Simulating a Camera","text":"<p>Add to your URDF:</p> <pre><code>&lt;!-- Camera link --&gt;\n&lt;link name=\"camera\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.1\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.05 0.05 0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n  &lt;collision&gt;\n    &lt;geometry&gt;\n      &lt;box size=\"0.05 0.05 0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/collision&gt;\n&lt;/link&gt;\n\n&lt;!-- Mount camera on head --&gt;\n&lt;joint name=\"head_to_camera\" type=\"fixed\"&gt;\n  &lt;parent link=\"head\"/&gt;\n  &lt;child link=\"camera\"/&gt;\n  &lt;origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#gazebo-camera-plugin","title":"Gazebo Camera Plugin","text":"<p>Add to SDF world file:</p> <pre><code>&lt;plugin\n    filename=\"gz-sim-camera-system\"\n    name=\"gz::sim::systems::Camera\"&gt;\n&lt;/plugin&gt;\n\n&lt;sensor name=\"camera\" type=\"camera\"&gt;\n  &lt;camera&gt;\n    &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt;\n    &lt;image&gt;\n      &lt;width&gt;640&lt;/width&gt;\n      &lt;height&gt;480&lt;/height&gt;\n    &lt;/image&gt;\n    &lt;clip&gt;\n      &lt;near&gt;0.1&lt;/near&gt;\n      &lt;far&gt;100&lt;/far&gt;\n    &lt;/clip&gt;\n  &lt;/camera&gt;\n  &lt;always_on&gt;1&lt;/always_on&gt;\n  &lt;update_rate&gt;30&lt;/update_rate&gt;\n  &lt;visualize&gt;true&lt;/visualize&gt;\n  &lt;topic&gt;camera&lt;/topic&gt;\n&lt;/sensor&gt;\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#subscribe-to-camera-in-ros-2","title":"Subscribe to Camera in ROS 2","text":"<pre><code>from sensor_msgs.msg import Image\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Process image (detect objects, etc.)\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        self.get_logger().info(f'Processed frame: {gray.shape}')\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#simulating-lidar","title":"Simulating LiDAR","text":"<pre><code>&lt;!-- LiDAR link --&gt;\n&lt;link name=\"lidar\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.2\"/&gt;\n    &lt;inertia ixx=\"0.001\" ixy=\"0\" ixz=\"0\" iyy=\"0.001\" iyz=\"0\" izz=\"0.001\"/&gt;\n  &lt;/inertial&gt;\n  &lt;visual&gt;\n    &lt;geometry&gt;\n      &lt;cylinder length=\"0.08\" radius=\"0.05\"/&gt;\n    &lt;/geometry&gt;\n  &lt;/visual&gt;\n&lt;/link&gt;\n\n&lt;joint name=\"torso_to_lidar\" type=\"fixed\"&gt;\n  &lt;parent link=\"torso\"/&gt;\n  &lt;child link=\"lidar\"/&gt;\n  &lt;origin xyz=\"0 0 0.6\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre> <p>LiDAR plugin in Gazebo:</p> <pre><code>&lt;sensor name=\"gpu_lidar\" type=\"gpu_lidar\"&gt;\n  &lt;pose&gt;0 0 0.6 0 0 0&lt;/pose&gt;\n  &lt;topic&gt;lidar&lt;/topic&gt;\n  &lt;update_rate&gt;10&lt;/update_rate&gt;\n  &lt;ray&gt;\n    &lt;scan&gt;\n      &lt;horizontal&gt;\n        &lt;samples&gt;360&lt;/samples&gt;\n        &lt;resolution&gt;1&lt;/resolution&gt;\n        &lt;min_angle&gt;0&lt;/min_angle&gt;\n        &lt;max_angle&gt;6.28&lt;/max_angle&gt;\n      &lt;/horizontal&gt;\n      &lt;vertical&gt;\n        &lt;samples&gt;128&lt;/samples&gt;\n        &lt;resolution&gt;1&lt;/resolution&gt;\n        &lt;min_angle&gt;-1.047&lt;/min_angle&gt;\n        &lt;max_angle&gt;1.047&lt;/max_angle&gt;\n      &lt;/vertical&gt;\n    &lt;/scan&gt;\n    &lt;range&gt;\n      &lt;min&gt;0.1&lt;/min&gt;\n      &lt;max&gt;30.0&lt;/max&gt;\n      &lt;resolution&gt;0.02&lt;/resolution&gt;\n    &lt;/range&gt;\n  &lt;/ray&gt;\n&lt;/sensor&gt;\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#simulating-imu-inertial-measurement-unit","title":"Simulating IMU (Inertial Measurement Unit)","text":"<pre><code>&lt;!-- IMU link --&gt;\n&lt;link name=\"imu\"&gt;\n  &lt;inertial&gt;\n    &lt;mass value=\"0.05\"/&gt;\n    &lt;inertia ixx=\"0.0001\" ixy=\"0\" ixz=\"0\" iyy=\"0.0001\" iyz=\"0\" izz=\"0.0001\"/&gt;\n  &lt;/inertial&gt;\n&lt;/link&gt;\n\n&lt;joint name=\"torso_to_imu\" type=\"fixed\"&gt;\n  &lt;parent link=\"torso\"/&gt;\n  &lt;child link=\"imu\"/&gt;\n  &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;\n&lt;/joint&gt;\n</code></pre> <p>IMU plugin:</p> <pre><code>&lt;sensor name=\"imu_sensor\" type=\"imu\"&gt;\n  &lt;always_on&gt;true&lt;/always_on&gt;\n  &lt;update_rate&gt;100&lt;/update_rate&gt;\n  &lt;visualize&gt;false&lt;/visualize&gt;\n  &lt;topic&gt;imu&lt;/topic&gt;\n&lt;/sensor&gt;\n</code></pre> <p>Subscribe in ROS 2:</p> <pre><code>from sensor_msgs.msg import Imu\n\ndef __init__(self):\n    self.imu_sub = self.create_subscription(\n        Imu,\n        '/imu',\n        self.imu_callback,\n        10\n    )\n\ndef imu_callback(self, msg):\n    accel = msg.linear_acceleration\n    angular_vel = msg.angular_velocity\n\n    self.get_logger().info(\n        f'Accel: {accel.x:.2f}, {accel.y:.2f}, {accel.z:.2f}'\n    )\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#collision-detection","title":"Collision Detection","text":"<p>Gazebo automatically detects collisions. Access them via ROS 2:</p> <pre><code>from gazebo_msgs.srv import GetContactsInfo\n\nclass CollisionDetector(Node):\n    def __init__(self):\n        super().__init__('collision_detector')\n        self.client = self.create_client(\n            GetContactsInfo,\n            '/gazebo/get_contacts'\n        )\n\n        self.timer = self.create_timer(0.1, self.check_collisions)\n\n    def check_collisions(self):\n        if not self.client.service_is_ready():\n            return\n\n        future = self.client.call_async(GetContactsInfo.Request())\n\n        def callback(future):\n            contacts = future.result().contacts\n            if contacts:\n                self.get_logger().warn(f'Collision detected! {len(contacts)} contacts')\n\n        future.add_done_callback(callback)\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/1-gazebo-simulation/#summary","title":"Summary","text":"<p>Gazebo provides: - \u2705 Realistic Physics - Gravity, friction, inertia - \u2705 Sensor Simulation - Cameras, LiDAR, IMU - \u2705 ROS 2 Integration - Direct topic communication - \u2705 Safe Testing - Test algorithms without hardware risk - \u2705 Reproducibility - Deterministic simulations for debugging</p> <p>Next: Create high-fidelity 3D environments with Unity!</p>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/","title":"Chapter 5: High-Fidelity Simulation with Unity","text":""},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#why-unity-for-robotics","title":"Why Unity for Robotics?","text":"<p>While Gazebo excels at physics accuracy, Unity provides: - Photorealistic graphics - Detailed 3D environments - Human-robot interaction - Test social navigation - Semantic understanding - AI can \"reason about\" scenes - Fast iteration - Visual feedback for development</p>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#setup-unity-robotics","title":"Setup: Unity Robotics","text":""},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#install-unity-hub","title":"Install Unity Hub","text":"<ol> <li>Download from <code>unity.com/download</code></li> <li>Create a Unity account</li> <li>Install Unity 2022 LTS or newer</li> </ol>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#install-ros-tcp-connector","title":"Install ROS-TCP Connector","text":"<p>In Unity, use the Package Manager: <pre><code>Window \u2192 TextMesh Pro \u2192 Import TMP Essentials\nWindow \u2192 Package Manager \u2192 Add package from git URL\nhttps://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.ros-tcp-connector\n</code></pre></p>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#configure-ros-connection","title":"Configure ROS Connection","text":"<ol> <li>Create an empty GameObject called \"RosConnector\"</li> <li>Add component: <code>ROS Connector</code></li> <li>Set:</li> <li>ROS IP Address: <code>127.0.0.1</code> (localhost)</li> <li>ROS Port: <code>5005</code></li> </ol>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#creating-a-humanoid-in-unity","title":"Creating a Humanoid in Unity","text":""},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#step-1-model-the-robot","title":"Step 1: Model the Robot","text":"<p>Create a hierarchy: <pre><code>Humanoid\n\u251c\u2500\u2500 Torso (Cube, scale 0.3 \u00d7 0.4 \u00d7 1.2)\n\u251c\u2500\u2500 Head (Sphere, radius 0.15)\n\u2502   \u2514\u2500\u2500 Camera (Camera component)\n\u251c\u2500\u2500 LeftArm\n\u2502   \u251c\u2500\u2500 LeftShoulder (Sphere)\n\u2502   \u2514\u2500\u2500 LeftForearm (Cylinder)\n\u2514\u2500\u2500 RightArm\n    \u251c\u2500\u2500 RightShoulder (Sphere)\n    \u2514\u2500\u2500 RightForearm (Cylinder)\n</code></pre></p>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#step-2-add-physics","title":"Step 2: Add Physics","text":"<p>For each body part: 1. Add <code>Rigidbody</code> component 2. Set mass (torso: 15kg, arms: 2kg each) 3. Add <code>Capsule Collider</code> for realistic collisions</p>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#step-3-add-joints","title":"Step 3: Add Joints","text":"<p>Connect body parts with <code>ConfigurableJoint</code>:</p> <pre><code>using UnityEngine;\n\npublic class RobotJointController : MonoBehaviour\n{\n    public ConfigurableJoint joint;\n    public float targetAngle = 0f;\n    public float forceLimit = 100f;\n\n    void FixedUpdate()\n    {\n        // Calculate torque needed to reach target angle\n        float currentAngle = transform.localEulerAngles.x;\n        float error = targetAngle - currentAngle;\n\n        // Apply torque proportional to error\n        Rigidbody rb = joint.GetComponent&lt;Rigidbody&gt;();\n        Vector3 torque = Vector3.right * error * forceLimit;\n        rb.AddTorque(torque);\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#subscribing-to-ros-topics","title":"Subscribing to ROS Topics","text":""},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#create-a-ros-subscriber-script","title":"Create a ROS Subscriber Script","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Std;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.ROSGeometry;\n\npublic class TwistSubscriber : MonoBehaviour\n{\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe&lt;TwistMsg&gt;(\"/cmd_vel\", OnTwistReceived);\n    }\n\n    void OnTwistReceived(TwistMsg twist)\n    {\n        // Extract linear and angular velocities\n        float forward = (float)twist.linear.x;\n        float rotate = (float)twist.angular.z;\n\n        // Apply to robot\n        Rigidbody rb = GetComponent&lt;Rigidbody&gt;();\n        rb.velocity = new Vector3(forward, 0, 0);\n        rb.angularVelocity = new Vector3(0, rotate, 0);\n\n        Debug.Log($\"Moving forward: {forward}, rotating: {rotate}\");\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#publishing-robot-state","title":"Publishing Robot State","text":"<pre><code>using RosMessageTypes.Sensor;\nusing RosMessageTypes.Nav;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class OdometryPublisher : MonoBehaviour\n{\n    private ROSConnection ros;\n    private string odometryTopic = \"/odom\";\n    private float publishRate = 30f;\n    private float lastPublishTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher&lt;OdometryMsg&gt;(odometryTopic);\n    }\n\n    void FixedUpdate()\n    {\n        if (Time.time - lastPublishTime &gt; 1f / publishRate)\n        {\n            PublishOdometry();\n            lastPublishTime = Time.time;\n        }\n    }\n\n    void PublishOdometry()\n    {\n        // Get current position and velocity\n        Rigidbody rb = GetComponent&lt;Rigidbody&gt;();\n\n        var odomMsg = new OdometryMsg\n        {\n            header = new HeaderMsg\n            {\n                stamp = new TimeMsg { sec = (uint)Time.time },\n                frame_id = \"odom\"\n            },\n            child_frame_id = \"base_link\",\n            pose = new PoseWithCovarianceMsg\n            {\n                pose = new PoseMsg\n                {\n                    position = new PointMsg\n                    {\n                        x = transform.position.x,\n                        y = transform.position.y,\n                        z = transform.position.z\n                    }\n                }\n            },\n            twist = new TwistWithCovarianceMsg\n            {\n                twist = new TwistMsg\n                {\n                    linear = new Vector3Msg\n                    {\n                        x = rb.velocity.x,\n                        y = rb.velocity.y,\n                        z = rb.velocity.z\n                    }\n                }\n            }\n        };\n\n        ros.Publish(odometryTopic, odomMsg);\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#unity-camera-integration","title":"Unity Camera Integration","text":""},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#subscribe-to-ros-image-and-render-in-unity","title":"Subscribe to ROS Image and Render in Unity","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class ImageDisplayer : MonoBehaviour\n{\n    public RawImage displayImage;\n    private Texture2D texture;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe&lt;ImageMsg&gt;(\"/camera/image_raw\", OnImageReceived);\n\n        texture = new Texture2D(640, 480, TextureFormat.RGB24, false);\n        displayImage.texture = texture;\n    }\n\n    void OnImageReceived(ImageMsg imageMsg)\n    {\n        // Convert ROS image to Unity texture\n        texture.LoadRawTextureData(imageMsg.data);\n        texture.Apply();\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#publish-camera-images-from-unity","title":"Publish Camera Images from Unity","text":"<pre><code>using UnityEngine;\nusing RosMessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class UnityCamera : MonoBehaviour\n{\n    public Camera captureCamera;\n    private ROSConnection ros;\n    private Texture2D texture;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher&lt;ImageMsg&gt;(\"/unity/camera/image\");\n\n        texture = new Texture2D(640, 480, TextureFormat.RGB24, false);\n    }\n\n    void Update()\n    {\n        // Render camera to texture\n        RenderTexture rt = new RenderTexture(640, 480, 24);\n        captureCamera.targetTexture = rt;\n        captureCamera.Render();\n\n        // Read texture\n        RenderTexture.active = rt;\n        texture.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);\n        texture.Apply();\n\n        // Publish to ROS\n        var imageMsg = new ImageMsg\n        {\n            header = new HeaderMsg\n            {\n                stamp = new TimeMsg { sec = (uint)Time.time },\n                frame_id = \"camera\"\n            },\n            height = 480,\n            width = 640,\n            encoding = \"rgb8\",\n            is_bigendian = false,\n            step = 640 * 3,\n            data = texture.GetRawTextureData()\n        };\n\n        ros.Publish(\"/unity/camera/image\", imageMsg);\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#advanced-physics-based-humanoid-locomotion","title":"Advanced: Physics-Based Humanoid Locomotion","text":"<pre><code>public class HumanoidLocomotion : MonoBehaviour\n{\n    [SerializeField] private Rigidbody torsoRb;\n    [SerializeField] private Rigidbody leftFootRb;\n    [SerializeField] private Rigidbody rightFootRb;\n    [SerializeField] private float walkSpeed = 1f;\n    [SerializeField] private float stepHeight = 0.3f;\n\n    private float stepCycle = 0f;\n    private const float StepDuration = 0.5f;\n\n    void FixedUpdate()\n    {\n        stepCycle += Time.fixedDeltaTime / StepDuration;\n        if (stepCycle &gt; 1f) stepCycle -= 1f;\n\n        // Alternating leg movement (bipedal gait)\n        float leftLift = Mathf.Sin(stepCycle * Mathf.PI) * stepHeight;\n        float rightLift = Mathf.Sin((stepCycle + 0.5f) * Mathf.PI) * stepHeight;\n\n        // Move torso forward\n        torsoRb.velocity = new Vector3(walkSpeed, torsoRb.velocity.y, 0);\n\n        // Lift legs in walking pattern\n        Vector3 leftFootPos = leftFootRb.position;\n        leftFootPos.y = Mathf.Max(leftFootPos.y + leftLift, 0);\n        leftFootRb.MovePosition(leftFootPos);\n\n        Vector3 rightFootPos = rightFootRb.position;\n        rightFootPos.y = Mathf.Max(rightFootPos.y + rightLift, 0);\n        rightFootRb.MovePosition(rightFootPos);\n    }\n}\n</code></pre>"},{"location":"physical-ai/module-2-digital-twin/2-unity-simulation/#summary","title":"Summary","text":"<p>Unity robotics provides: - \u2705 Photorealistic simulation - Better for visualizing human-robot interaction - \u2705 Advanced graphics - Test perception in realistic scenarios - \u2705 ROS integration - Full bidirectional communication - \u2705 Game engine power - Animators, shaders, particles - \u2705 Fast development - Visual editor for scene design</p> <p>Next: Deploy advanced AI perception with NVIDIA Isaac!</p>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/","title":"Chapter 6: NVIDIA Isaac - The AI Robot Brain","text":""},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#what-is-nvidia-isaac","title":"What is NVIDIA Isaac?","text":"<p>NVIDIA Isaac is a comprehensive robotics platform that provides: - Isaac Sim - Photorealistic simulation with synthetic data generation - Isaac ROS - Hardware-accelerated vision and perception algorithms - Isaac Manipulator - Pre-trained models for object manipulation - Nav2 Integration - Path planning for humanoid navigation</p>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#isaac-sim-photorealistic-simulation","title":"Isaac Sim: Photorealistic Simulation","text":""},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#installation","title":"Installation","text":"<pre><code># Download from NVIDIA (free with registration)\n# https://www.nvidia.com/en-us/isaac/\n\n# Or use Docker\ndocker pull nvcr.io/nvidia/isaac-sim:2024.1\n\n# Run Isaac Sim\ndocker run --gpus all -it --rm \\\n  -v ~/isaac_data:/home/user/isaac_data \\\n  nvcr.io/nvidia/isaac-sim:2024.1\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#creating-a-humanoid-world-in-isaac-sim","title":"Creating a Humanoid World in Isaac Sim","text":"<p>Isaac Sim uses USD (Universal Scene Description) format:</p> <pre><code># Create file: create_humanoid_world.py\nimport carb\nfrom isaacsim import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": False})\n\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.prims import XFormPrim\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_ground_plane()\n\n# Add robot (pre-built humanoid model)\nrobot_prim_path = \"/World/humanoid\"\nadd_reference_to_stage(\n    usd_path=\"omniverse://localhost/NVIDIA/Assets/Isaac/2024.1/Isaac/Robots/Humanoids/H1/h1.usd\",\n    prim_path=robot_prim_path\n)\n\n# Simulate for 1000 frames\nfor i in range(1000):\n    world.step(render=True)\n\nsimulation_app.close()\n</code></pre> <p>Run it: <pre><code>python create_humanoid_world.py\n</code></pre></p>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Isaac Sim generates unlimited labeled training data:</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom isaacsim import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": True})  # Headless for speed\n\nimport omni\nfrom omni.isaac.core.utils.rotations import euler_angles_to_quat\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera\n\nworld = World()\nworld.scene.add_ground_plane()\n\n# Add camera\ncamera = Camera(\n    prim_path=\"/World/camera\",\n    resolution=(640, 480),\n    translation=[1.0, 1.0, 1.0],\n    orientation=euler_angles_to_quat([0, 0.7, 0])\n)\n\n# Add some objects to detect\nobjects = []\nfor i in range(10):\n    # Random cube\n    pos = [i * 0.5, np.random.rand() * 2, 0.5]\n    add_reference_to_stage(\n        usd_path=\"omniverse://localhost/NVIDIA/Assets/Isaac/2024.1/Isaac/Props/Primitives/Cube.usd\",\n        prim_path=f\"/World/object_{i}\",\n        position=pos\n    )\n\n# Capture and save 1000 annotated frames\nfor frame in range(1000):\n    world.step(render=False)\n\n    # Get camera image\n    rgb_data = camera.get_rgb()\n    depth_data = camera.get_depth()\n\n    # Save image\n    Image.fromarray((rgb_data * 255).astype(np.uint8)).save(\n        f\"training_data/rgb_{frame:04d}.png\"\n    )\n\n    # Save depth\n    (depth_data * 1000).astype(np.uint16).tobytes() # 16-bit depth\n\n    # Save annotations (object positions, labels, etc.)\n    # ... annotation code ...\n\nprint(\"Generated 1000 training images with automatic labels!\")\n\nsimulation_app.close()\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#isaac-ros-hardware-accelerated-perception","title":"Isaac ROS: Hardware-Accelerated Perception","text":""},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#vslam-visual-simultaneous-localization-and-mapping","title":"VSLAM (Visual Simultaneous Localization and Mapping)","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Twist\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import Path, Odometry\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass IsaacVSLAMNode(Node):\n    \"\"\"\n    Visual SLAM using Isaac ROS accelerated kernels\n    GPU-accelerated visual odometry from camera images\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_vslam')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Subscribe to camera info (intrinsics)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publish odometry and map\n        self.odometry_pub = self.create_publisher(Odometry, '/odom', 10)\n        self.path_pub = self.create_publisher(Path, '/path', 10)\n\n        self.bridge = CvBridge()\n        self.prev_frame = None\n        self.path_poses = []\n\n        self.get_logger().info(\"Isaac VSLAM Node started\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera frames\"\"\"\n\n        # Convert ROS image to OpenCV\n        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')\n\n        if self.prev_frame is None:\n            self.prev_frame = frame\n            return\n\n        # Feature detection (ORB - fast, GPU-friendly)\n        orb = cv2.ORB_create(nfeatures=500)\n        kp1, des1 = orb.detectAndCompute(self.prev_frame, None)\n        kp2, des2 = orb.detectAndCompute(frame, None)\n\n        if des1 is None or des2 is None:\n            return\n\n        # Feature matching\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        matches = bf.match(des1, des2)\n        matches = sorted(matches, key=lambda x: x.distance)\n\n        # Extract matched points\n        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches[:20]])\n        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches[:20]])\n\n        # Compute Essential Matrix (camera motion)\n        E, mask = cv2.findEssentialMat(pts1, pts2)\n\n        _, R, t, mask = cv2.recoverPose(E, pts1, pts2)\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \"odom\"\n        odom_msg.child_frame_id = \"camera\"\n\n        # Position\n        odom_msg.pose.pose.position.x = float(t[0])\n        odom_msg.pose.pose.position.y = float(t[1])\n        odom_msg.pose.pose.position.z = float(t[2])\n\n        self.odometry_pub.publish(odom_msg)\n\n        self.prev_frame = frame\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera intrinsics for VSLAM\"\"\"\n        self.K = np.array(msg.K).reshape(3, 3)\n        self.D = np.array(msg.D)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacVSLAMNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#object-detection-with-isaac-ros-perception","title":"Object Detection with Isaac ROS Perception","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, BoundingBox2D\nfrom cv_bridge import CvBridge\nimport torch\nfrom torchvision import models, transforms\nimport cv2\n\nclass IsaacObjectDetector(Node):\n    \"\"\"\n    GPU-accelerated object detection using YOLOv8\n    Optimized with NVIDIA TensorRT\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_detector')\n\n        # Load YOLOv8 model (optimized for Jetson)\n        self.model = torch.hub.load(\n            'ultralytics/yolov8',\n            'custom',\n            path='yolov8n.pt',  # nano model for speed\n            force_reload=False\n        )\n\n        # Set to GPU\n        self.model.to('cuda')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.detect_callback,\n            10\n        )\n\n        # Publish detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/detections',\n            10\n        )\n\n        self.bridge = CvBridge()\n        self.get_logger().info(\"Object Detector initialized (GPU-accelerated)\")\n\n    def detect_callback(self, msg):\n        \"\"\"Detect objects in image\"\"\"\n\n        # Convert ROS image to OpenCV\n        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Run inference\n        with torch.no_grad():\n            results = self.model(frame)\n\n        # Create detection array\n        detections = Detection2DArray()\n        detections.header.stamp = msg.header\n        detections.header.frame_id = \"camera\"\n\n        # Extract bounding boxes\n        for *box, conf, cls in results.xyxy[0]:\n            det = Detection2D()\n            det.results[0].id = str(int(cls))\n            det.results[0].score = float(conf)\n\n            # Bounding box\n            x1, y1, x2, y2 = box\n            det.bbox.center.x = float((x1 + x2) / 2)\n            det.bbox.center.y = float((y1 + y2) / 2)\n            det.bbox.size_x = float(x2 - x1)\n            det.bbox.size_y = float(y2 - y1)\n\n            detections.detections.append(det)\n\n        # Publish\n        self.detection_pub.publish(detections)\n\n        self.get_logger().info(\n            f\"Detected {len(detections.detections)} objects\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacObjectDetector()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#path-planning-with-nav2","title":"Path Planning with Nav2","text":""},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#setup-nav2-for-humanoid-navigation","title":"Setup Nav2 for Humanoid Navigation","text":"<pre><code>sudo apt install ros-humble-nav2 ros-humble-nav2-bringup\n</code></pre> <p>Create <code>nav2_humanoid.launch.py</code>:</p> <pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    nav2_dir = get_package_share_directory('nav2_bringup')\n\n    return LaunchDescription([\n        # Nav2 Costmap Server\n        Node(\n            package='nav2_costmap_2d',\n            executable='costmap_2d_node',\n            name='global_costmap',\n            parameters=[{\n                'use_sim_time': True,\n                'global_frame': 'map',\n                'robot_base_frame': 'base_link',\n                'plugins': ['static_layer', 'obstacle_layer', 'inflation_layer'],\n                'inflation_layer.inflation_radius': 0.5,\n            }]\n        ),\n\n        # Planner Server\n        Node(\n            package='nav2_planner',\n            executable='planner_server',\n            parameters=[{'use_sim_time': True}]\n        ),\n\n        # Controller Server\n        Node(\n            package='nav2_controller',\n            executable='controller_server',\n            parameters=[{'use_sim_time': True}]\n        ),\n    ])\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#use-nav2-to-navigate","title":"Use Nav2 to Navigate","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav2_simple_commander.robot_navigator import BasicNavigator\nfrom nav_msgs.msg import Path\n\nclass HumanoidNavigator(Node):\n    \"\"\"Navigate humanoid robot using Nav2\"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_navigator')\n        self.navigator = BasicNavigator()\n\n    def navigate_to(self, x, y, theta):\n        \"\"\"Send robot to target position\"\"\"\n\n        # Create goal pose\n        goal_pose = PoseStamped()\n        goal_pose.header.frame_id = 'map'\n        goal_pose.pose.position.x = x\n        goal_pose.pose.position.y = y\n        goal_pose.pose.orientation.w = 1.0\n\n        self.navigator.setInitialPose(PoseStamped())\n        self.navigator.goToPose(goal_pose)\n\n        self.get_logger().info(f'Navigating to ({x}, {y})')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidNavigator()\n\n    # Navigate robot\n    node.navigate_to(x=5.0, y=3.0, theta=0.0)\n\n    # Wait for completion\n    while not node.navigator.isNavComplete():\n        rclpy.spin_once(node)\n\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-3-ai-robot-brain/1-isaac-sim/#summary","title":"Summary","text":"<p>NVIDIA Isaac provides: - \u2705 Photorealistic simulation - Isaac Sim for testing - \u2705 Synthetic data - Unlimited labeled training data - \u2705 GPU acceleration - Fast perception algorithms - \u2705 Ready models - Pre-trained detection, SLAM, control - \u2705 Path planning - Nav2 integration for navigation</p> <p>Next: Combine language understanding with robotics using Vision-Language-Action models!</p>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/","title":"Chapter 7: Vision-Language-Action Models","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#what-is-vla","title":"What is VLA?","text":"<p>Vision-Language-Action (VLA) systems combine three capabilities: - Vision - See and understand the world (cameras, object detection) - Language - Understand natural language commands (LLMs) - Action - Translate understanding into robot movements (ROS 2 control)</p> <p>This creates robots that respond to natural language: \"Clean the table\" \u2192 robot understands, plans, and executes.</p>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#the-vla-pipeline","title":"The VLA Pipeline","text":"<pre><code>Human: \"Pick up the red cup\"\n     \u2193\n[Speech-to-Text: Whisper]\n     \u2193\nText: \"Pick up the red cup\"\n     \u2193\n[Vision: CLIP + Object Detection]\n     \u2193\nScene Understanding: \"Red cup at (0.5, 0.3, 0.8)\"\n     \u2193\n[Language Model: GPT-4]\n     \u2193\nAction Plan: [\"move_to(0.5, 0.3, 1.0)\", \"lower_gripper()\", \"close_gripper()\"]\n     \u2193\n[ROS 2 Controllers]\n     \u2193\nRobot executes plan\n</code></pre>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#part-1-voice-input-with-whisper","title":"Part 1: Voice Input with Whisper","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#speech-to-text-with-openai-whisper","title":"Speech-to-Text with OpenAI Whisper","text":"<pre><code>pip install openai-whisper sounddevice numpy scipy\n</code></pre> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\n\nclass VoiceCommandNode(Node):\n    \"\"\"Listen to voice commands and transcribe with Whisper\"\"\"\n\n    def __init__(self):\n        super().__init__('voice_command')\n\n        # Load Whisper model\n        self.model = whisper.load_model(\"base\")  # Options: tiny, base, small, medium, large\n\n        # Publisher for transcribed text\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\n\n        # Timer to record audio\n        self.timer = self.create_timer(1.0, self.record_and_transcribe)\n\n        self.get_logger().info(\"Voice command node ready (listening...)\")\n\n    def record_and_transcribe(self):\n        \"\"\"Record 5 seconds of audio and transcribe\"\"\"\n\n        # Record 5 seconds at 16kHz\n        sample_rate = 16000\n        duration = 5\n\n        self.get_logger().info(\"Recording...\")\n        audio = sd.rec(\n            int(sample_rate * duration),\n            samplerate=sample_rate,\n            channels=1,\n            dtype=np.int16\n        )\n        sd.wait()\n\n        # Normalize audio\n        audio_float = audio.astype(np.float32) / 32768.0\n\n        # Transcribe\n        result = self.model.transcribe(audio_float)\n        text = result['text'].strip()\n\n        if text:\n            self.get_logger().info(f\"You said: {text}\")\n\n            # Publish command\n            msg = String()\n            msg.data = text\n            self.command_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#part-2-vision-understanding-with-clip","title":"Part 2: Vision Understanding with CLIP","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#object-detection-and-understanding","title":"Object Detection and Understanding","text":"<pre><code>import torch\nfrom PIL import Image\nimport clip\n\nclass VisionUnderstandingNode(Node):\n    \"\"\"Understand scenes using CLIP vision-language model\"\"\"\n\n    def __init__(self):\n        super().__init__('vision_understanding')\n\n        # Load CLIP (Vision-Language model)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.understand_image,\n            10\n        )\n\n        # Publish scene understanding\n        self.scene_pub = self.create_publisher(String, '/scene_description', 10)\n\n        self.bridge = CvBridge()\n        self.get_logger().info(\"Vision understanding node ready\")\n\n    def understand_image(self, msg):\n        \"\"\"Analyze image and describe objects\"\"\"\n\n        # Convert ROS image to PIL\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\n\n        # Objects to look for\n        objects = [\"red cup\", \"blue ball\", \"person\", \"table\", \"chair\", \"book\"]\n\n        with torch.no_grad():\n            # Prepare image\n            image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n            # Prepare text labels\n            text = clip.tokenize(objects).to(self.device)\n\n            # Get similarity scores\n            logits_per_image, _ = self.model(image, text)\n            probabilities = logits_per_image.softmax(dim=-1)\n\n        # Find highest probability\n        top_idx = probabilities[0].argmax().item()\n        top_object = objects[top_idx]\n        top_prob = probabilities[0][top_idx].item()\n\n        scene_desc = f\"Detected: {top_object} (confidence: {top_prob:.2%})\"\n\n        self.get_logger().info(scene_desc)\n\n        # Publish\n        msg = String()\n        msg.data = scene_desc\n        self.scene_pub.publish(msg)\n</code></pre>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#part-3-llm-planning-with-gpt-4","title":"Part 3: LLM Planning with GPT-4","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#translate-natural-language-to-robot-actions","title":"Translate Natural Language to Robot Actions","text":"<pre><code>import openai\n\nclass LLMPlannerNode(Node):\n    \"\"\"Use GPT-4 to plan robot actions from natural language\"\"\"\n\n    def __init__(self):\n        super().__init__('llm_planner')\n\n        # Set OpenAI API key\n        openai.api_key = \"your-api-key\"\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.plan_actions,\n            10\n        )\n\n        # Publish action sequence\n        self.action_pub = self.create_publisher(String, '/action_sequence', 10)\n\n        self.system_prompt = \"\"\"\n        You are a robot action planner. Convert natural language commands into \n        a sequence of ROS 2 action calls.\n\n        Available actions:\n        - move_forward(distance_m)\n        - turn(angle_degrees)\n        - pickup_object(object_name)\n        - place_object(location)\n        - open_gripper()\n        - close_gripper()\n\n        Example:\n        Command: \"Pick up the red cup from the table\"\n        Actions:\n        1. move_forward(1.0)\n        2. detect_object(\"red cup\")\n        3. move_to_object(\"red cup\")\n        4. open_gripper()\n        5. pickup_object(\"red cup\")\n\n        Respond with only the action sequence, one per line.\n        \"\"\"\n\n        self.get_logger().info(\"LLM Planner ready\")\n\n    def plan_actions(self, msg):\n        \"\"\"Plan robot actions using GPT-4\"\"\"\n\n        command = msg.data\n        self.get_logger().info(f\"Planning actions for: {command}\")\n\n        try:\n            # Call GPT-4\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": command}\n                ],\n                temperature=0.3,  # More deterministic\n                max_tokens=200\n            )\n\n            action_sequence = response['choices'][0]['message']['content']\n\n            self.get_logger().info(f\"Action sequence:\\n{action_sequence}\")\n\n            # Publish\n            msg = String()\n            msg.data = action_sequence\n            self.action_pub.publish(msg)\n\n        except Exception as e:\n            self.get_logger().error(f\"LLM planning failed: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#part-4-action-execution-with-ros-2","title":"Part 4: Action Execution with ROS 2","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#execute-planned-actions-on-robot","title":"Execute Planned Actions on Robot","text":"<pre><code>import re\nfrom geometry_msgs.msg import Twist\n\nclass ActionExecutor(Node):\n    \"\"\"Execute action sequence on real or simulated robot\"\"\"\n\n    def __init__(self):\n        super().__init__('action_executor')\n\n        # Subscribers\n        self.action_sub = self.create_subscription(\n            String,\n            '/action_sequence',\n            self.execute_actions,\n            10\n        )\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.gripper_pub = self.create_publisher(String, '/gripper_command', 10)\n\n        self.get_logger().info(\"Action executor ready\")\n\n    def execute_actions(self, msg):\n        \"\"\"Parse and execute action sequence\"\"\"\n\n        action_sequence = msg.data\n        actions = [line.strip() for line in action_sequence.split('\\n') if line.strip()]\n\n        for action in actions:\n            self.get_logger().info(f\"Executing: {action}\")\n            self.execute_single_action(action)\n\n    def execute_single_action(self, action):\n        \"\"\"Execute one action\"\"\"\n\n        # Parse action format: action_name(arg1, arg2, ...)\n        match = re.match(r'(\\w+)\\((.*)\\)', action)\n        if not match:\n            self.get_logger().warn(f\"Invalid action format: {action}\")\n            return\n\n        action_name = match.group(1)\n        args_str = match.group(2)\n        args = [arg.strip().strip('\"\\'') for arg in args_str.split(',')]\n\n        if action_name == 'move_forward':\n            distance = float(args[0])\n            self.move_forward(distance)\n\n        elif action_name == 'turn':\n            angle = float(args[0])\n            self.turn(angle)\n\n        elif action_name == 'open_gripper':\n            self.send_gripper_command('OPEN')\n\n        elif action_name == 'close_gripper':\n            self.send_gripper_command('CLOSE')\n\n        elif action_name == 'pickup_object':\n            obj = args[0]\n            self.pickup_object(obj)\n\n    def move_forward(self, distance):\n        \"\"\"Move robot forward\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.5  # 0.5 m/s\n\n        # Estimate time needed\n        duration = distance / 0.5  # seconds\n\n        for _ in range(int(duration * 10)):\n            self.cmd_vel_pub.publish(msg)\n\n        # Stop\n        msg.linear.x = 0.0\n        self.cmd_vel_pub.publish(msg)\n\n    def turn(self, angle_degrees):\n        \"\"\"Turn robot\"\"\"\n        msg = Twist()\n        angle_rad = angle_degrees * 3.14159 / 180\n        msg.angular.z = 0.5  # 0.5 rad/s\n\n        duration = angle_rad / 0.5\n\n        for _ in range(int(duration * 10)):\n            self.cmd_vel_pub.publish(msg)\n\n        msg.angular.z = 0.0\n        self.cmd_vel_pub.publish(msg)\n\n    def send_gripper_command(self, command):\n        \"\"\"Open or close gripper\"\"\"\n        msg = String()\n        msg.data = command\n        self.gripper_pub.publish(msg)\n\n    def pickup_object(self, object_name):\n        \"\"\"Sequence to pick up an object\"\"\"\n        self.open_gripper()\n        self.move_forward(0.3)\n        self.send_gripper_command('CLOSE')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionExecutor()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#complete-vla-system-integration","title":"Complete VLA System Integration","text":""},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#launch-everything-together","title":"Launch Everything Together","text":"<pre><code># launch/vla_system.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Voice input\n        Node(\n            package='robot_vla',\n            executable='voice_command_node',\n            output='screen'\n        ),\n\n        # Vision understanding\n        Node(\n            package='robot_vla',\n            executable='vision_understanding_node',\n            output='screen'\n        ),\n\n        # LLM planning\n        Node(\n            package='robot_vla',\n            executable='llm_planner_node',\n            output='screen'\n        ),\n\n        # Action execution\n        Node(\n            package='robot_vla',\n            executable='action_executor_node',\n            output='screen'\n        ),\n    ])\n</code></pre> <p>Run it: <pre><code>ros2 launch robot_vla vla_system.launch.py\n</code></pre></p>"},{"location":"physical-ai/module-4-vision-language-action/1-vla-systems/#summary","title":"Summary","text":"<p>Vision-Language-Action systems enable: - \u2705 Natural language control - Talk to your robot like a human - \u2705 Scene understanding - Vision-language models \"see\" like humans - \u2705 Intelligent planning - LLMs reason about complex tasks - \u2705 Real robot execution - Commands translate to actual movements - \u2705 Adaptive behavior - Can learn new tasks from examples</p> <p>Next: Build the capstone\u2014an autonomous humanoid robot!</p>"}]}